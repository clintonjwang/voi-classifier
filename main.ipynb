{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input, Add, Dense, Activation, Flatten, Dropout, Conv3D, MaxPooling3D, ZeroPadding3D, AveragePooling3D, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import config\n",
    "import importlib\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "nb_classes = C.nb_classes\n",
    "\n",
    "X_train = np.expand_dims(x_train,axis=3)\n",
    "X_train = np.expand_dims(X_train,axis=4)\n",
    "X_test = np.expand_dims(x_test,axis=3)\n",
    "X_test = np.expand_dims(X_test,axis=4)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\config.py'>"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C = config.Config()\n",
    "dims = C.dims\n",
    "voi_img = Input(shape=(dims[0], dims[1], dims[2], C.nb_channels))\n",
    "x = voi_img\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = GaussianNoise(1)(x)\n",
    "#x = ZeroPadding3D(padding=(3,3,2))(voi_img)\n",
    "#x = Conv3D(filters=128, kernel_size=(3,3,2), activation='relu', kernel_regularizer=l2(.01))(x)\n",
    "x = Conv3D(filters=32, kernel_size=(3,3,1), activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "x = MaxPooling3D((2, 2, 2))(x)\n",
    "x = Conv3D(filters=64, kernel_size=(3,3,1), activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "#x = Conv3D(filters=128, kernel_size=(2,2,2), activation='relu')(x)\n",
    "x = MaxPooling3D((2, 2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(164, activation='relu')(x)#, kernel_initializer='normal', kernel_regularizer=l1(.01), kernel_constraint=max_norm(3.))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "pred_class = Dense(C.nb_classes, activation='softmax')(x)#Dense(C.nb_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_18 (InputLayer)        (None, 24, 24, 12, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_36 (Conv3D)           (None, 22, 22, 12, 32)    320       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_18 (MaxPooling (None, 11, 11, 6, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_37 (Conv3D)           (None, 9, 9, 6, 64)       18496     \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 9, 9, 6, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_19 (MaxPooling (None, 4, 4, 3, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_18 (Flatten)         (None, 3072)              0         \n",
      "_________________________________________________________________\n",
      "dense_35 (Dense)             (None, 164)               503972    \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 164)               0         \n",
      "_________________________________________________________________\n",
      "dense_36 (Dense)             (None, 4)                 660       \n",
      "=================================================================\n",
      "Total params: 523,448\n",
      "Trainable params: 523,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(lr=0.001)#, decay=0.001)\n",
    "early_stopping = EarlyStopping(min_delta=0.001, patience=10)\n",
    "\n",
    "model = Model(voi_img, pred_class)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model2 = Model(voi_img, intermed)\n",
    "\n",
    "for l in range(2,len(model2.layers)):\n",
    "    model2.layers[l].set_weights(model.layers[l].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X_train.append(np.ones(dims + [C.nb_channels]))\n",
    "    Y_train.append([1,0])\n",
    "for _ in range(10):\n",
    "    X_train.append(np.ones(dims + [C.nb_channels]))\n",
    "    X_train[-1][5:15,5:15,5:7,0] = 2\n",
    "    Y_train.append([0,1])\n",
    "    \n",
    "X_train = np.array(X_train) # X[:total_size//2]\n",
    "#X_val = np.array(X_test)\n",
    "Y_train = np.array(Y_train) # Y[:total_size//2]\n",
    "#Y_val = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "max_samples = {}\n",
    "base_dir = \"..\\\\liver-mr-processor\\\\train_imgs\\\\\"\n",
    "\n",
    "least_in_one_class = np.Inf\n",
    "\n",
    "for class_name in os.listdir(base_dir):\n",
    "    x = np.empty((10000, dims[0], dims[1], dims[2], C.nb_channels))\n",
    "    z = []\n",
    "    \n",
    "    for index, img_fn in enumerate(os.listdir(base_dir+class_name)):\n",
    "        try:\n",
    "            x[index] = np.load(base_dir+class_name+\"\\\\\"+img_fn)\n",
    "            z.append(img_fn)\n",
    "        except Exception as e:\n",
    "            print(class_name+\"\\\\\"+img_fn, e)\n",
    "    \n",
    "    x.resize((index, dims[0], dims[1], dims[2], C.nb_channels))\n",
    "    data_dict[class_name] = [x,np.array(z)]\n",
    "    max_samples[class_name] = index\n",
    "    \n",
    "    least_in_one_class = min(index, least_in_one_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "Z_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "Z_test = []\n",
    "#train_frac = 0.75\n",
    "cls_mapping = []\n",
    "\n",
    "for cls_num, cls in enumerate(data_dict):\n",
    "    if cls == \"cyst\":\n",
    "        train_frac = 1\n",
    "    elif cls == \"fnh\":\n",
    "        train_frac = 1.6\n",
    "    else:\n",
    "        train_frac = 0.68\n",
    "    order = np.random.permutation(list(range(max_samples[cls])))\n",
    "    #order = list(range(max_samples[cls]))\n",
    "    X_train = X_train + list(data_dict[cls][0][order[:round(least_in_one_class*train_frac)]])\n",
    "    X_test = X_test + list(data_dict[cls][0][order[round(least_in_one_class*train_frac):]])\n",
    "    Z_train = Z_train + list(data_dict[cls][1][order[:round(least_in_one_class*train_frac)]])\n",
    "    Z_test = Z_test + list(data_dict[cls][1][order[round(least_in_one_class*train_frac):]])\n",
    "    Y_train = Y_train + [[0] * cls_num + [1] + [0] * (C.nb_classes - cls_num - 1)] * (round(least_in_one_class*train_frac))\n",
    "    Y_test = Y_test + [[0] * cls_num + [1] + [0] * (C.nb_classes - cls_num - 1)] * \\\n",
    "                        (max_samples[cls] - round(least_in_one_class*train_frac))\n",
    "        \n",
    "    cls_mapping.append(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Z_cyst = data_dict[\"cyst\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_[-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z_cyst[-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "X_cyst = data_dict[\"cyst\"][0]\n",
    "X_cyst = np.array(X_cyst) # X[:total_size//2]\n",
    "X_cyst /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "X_hcc = data_dict[\"hcc\"][0]\n",
    "X_hcc = np.array(X_hcc) # X[:total_size//2]\n",
    "X_hcc /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train) # X[:total_size//2]\n",
    "X_train /= 255\n",
    "X_val = np.array(X_test)\n",
    "X_val /= 255\n",
    "#X_val = X[total_size//2:total_size*3//4]\n",
    "#X_test = X[total_size*3//4:]\n",
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(Y_test)\n",
    "Z_train = np.array(Z_train)\n",
    "Z_val = np.array(Z_test)\n",
    "#Y_val = Y[total_size//2:total_size*3//4]\n",
    "#Y_test = Y[total_size*3//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100.16666666666667"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Z_train)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37.03333333333333"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Z_val)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = np.array([z for z in x])\n",
    "#Y = np.array(y)\n",
    "#Y = np.array([[0,1] if y[x] == 1 else [1,0] for x in range(len(y))])\n",
    "#Y = K.constant(y, dtype=tf.int32)\n",
    "#Y = K.one_hot(Y, C.nb_classes)\n",
    "\n",
    "#total_size = X.shape[0]\n",
    "\n",
    "#order = np.random.permutation(list(range(total_size)))\n",
    "#X = X[order]\n",
    "#Y = Y[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_generator(X, Y):\n",
    "    while True:\n",
    "        for i in range(len(X)):\n",
    "            yield np.expand_dims(X[i], axis=0), np.expand_dims(Y[i], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_epochs = 10\n",
    "epoch_length = 100\n",
    "best_loss = np.Inf\n",
    "losses = np.zeros(epoch_length)\n",
    "acc = np.zeros(epoch_length)\n",
    "\n",
    "data_gen_train = train_generator(X_train, Y_train)\n",
    "for epoch_num in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
    "    iter_num = 0\n",
    "    \n",
    "    while True:\n",
    "        X, Y = next(data_gen_train)\n",
    "        losses[iter_num], acc[iter_num] = model.train_on_batch(X, Y)\n",
    "\n",
    "        iter_num += 1\n",
    "        if iter_num == epoch_length:\n",
    "            curr_loss = np.mean(losses)\n",
    "            curr_acc = np.mean(acc)\n",
    "            print(\"Mean Loss:\", curr_loss, \"// Mean Accuracy:\", curr_acc)\n",
    "\n",
    "            if curr_loss < best_loss:\n",
    "                print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "                best_loss = curr_loss\n",
    "                model.save_weights(C.model_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3005 samples, validate on 1111 samples\n",
      "Epoch 1/200\n",
      "3005/3005 [==============================] - 1s - loss: 1.3169 - acc: 0.4000 - val_loss: 1.3387 - val_acc: 0.3825\n",
      "Epoch 2/200\n",
      "3005/3005 [==============================] - 1s - loss: 1.2156 - acc: 0.4230 - val_loss: 1.2284 - val_acc: 0.4113\n",
      "Epoch 3/200\n",
      "3005/3005 [==============================] - 1s - loss: 1.0945 - acc: 0.4862 - val_loss: 1.1424 - val_acc: 0.4176\n",
      "Epoch 4/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.9903 - acc: 0.5594 - val_loss: 1.0836 - val_acc: 0.5050\n",
      "Epoch 5/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.8986 - acc: 0.6020 - val_loss: 1.0067 - val_acc: 0.5941\n",
      "Epoch 6/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.8372 - acc: 0.6419 - val_loss: 0.9843 - val_acc: 0.6040\n",
      "Epoch 7/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.8062 - acc: 0.6629 - val_loss: 0.9144 - val_acc: 0.6625\n",
      "Epoch 8/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.7409 - acc: 0.6922 - val_loss: 0.9021 - val_acc: 0.6796\n",
      "Epoch 9/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.6814 - acc: 0.7208 - val_loss: 0.8745 - val_acc: 0.6787\n",
      "Epoch 10/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.6744 - acc: 0.7178 - val_loss: 0.7931 - val_acc: 0.7138\n",
      "Epoch 11/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.6302 - acc: 0.7464 - val_loss: 0.8016 - val_acc: 0.6643\n",
      "Epoch 12/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.6332 - acc: 0.7424 - val_loss: 0.7885 - val_acc: 0.6868\n",
      "Epoch 13/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.5904 - acc: 0.7637 - val_loss: 0.8141 - val_acc: 0.6670\n",
      "Epoch 14/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.5413 - acc: 0.7917 - val_loss: 0.6874 - val_acc: 0.7516\n",
      "Epoch 15/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.5362 - acc: 0.7810 - val_loss: 0.7064 - val_acc: 0.7237\n",
      "Epoch 16/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.5035 - acc: 0.8010 - val_loss: 0.6760 - val_acc: 0.7912\n",
      "Epoch 17/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.4716 - acc: 0.8200 - val_loss: 0.6389 - val_acc: 0.8164\n",
      "Epoch 18/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.4353 - acc: 0.8376 - val_loss: 0.6033 - val_acc: 0.8128\n",
      "Epoch 19/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.4361 - acc: 0.8393 - val_loss: 0.6256 - val_acc: 0.8011\n",
      "Epoch 20/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.4324 - acc: 0.8266 - val_loss: 0.6041 - val_acc: 0.8263\n",
      "Epoch 21/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.4378 - acc: 0.8290 - val_loss: 0.6002 - val_acc: 0.8020\n",
      "Epoch 22/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.3860 - acc: 0.8519 - val_loss: 0.5373 - val_acc: 0.8344\n",
      "Epoch 23/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.3736 - acc: 0.8622 - val_loss: 0.5728 - val_acc: 0.7858\n",
      "Epoch 24/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.3597 - acc: 0.8669 - val_loss: 0.5145 - val_acc: 0.8362\n",
      "Epoch 25/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.3529 - acc: 0.8636 - val_loss: 0.5194 - val_acc: 0.8452\n",
      "Epoch 26/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.3419 - acc: 0.8725 - val_loss: 0.4889 - val_acc: 0.8416\n",
      "Epoch 27/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.3350 - acc: 0.8649 - val_loss: 0.4911 - val_acc: 0.8443\n",
      "Epoch 28/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.3099 - acc: 0.8809 - val_loss: 0.4772 - val_acc: 0.8470\n",
      "Epoch 29/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2920 - acc: 0.8945 - val_loss: 0.4694 - val_acc: 0.8461\n",
      "Epoch 30/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2829 - acc: 0.8948 - val_loss: 0.4688 - val_acc: 0.8488\n",
      "Epoch 31/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2870 - acc: 0.8932 - val_loss: 0.4930 - val_acc: 0.8227\n",
      "Epoch 32/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2750 - acc: 0.8985 - val_loss: 0.4752 - val_acc: 0.8290\n",
      "Epoch 33/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2599 - acc: 0.8972 - val_loss: 0.4160 - val_acc: 0.8830\n",
      "Epoch 34/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2310 - acc: 0.9151 - val_loss: 0.3936 - val_acc: 0.8785\n",
      "Epoch 35/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2479 - acc: 0.9138 - val_loss: 0.4203 - val_acc: 0.8740\n",
      "Epoch 36/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2299 - acc: 0.9148 - val_loss: 0.4064 - val_acc: 0.8623\n",
      "Epoch 37/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2282 - acc: 0.9175 - val_loss: 0.3818 - val_acc: 0.9037\n",
      "Epoch 38/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2292 - acc: 0.9171 - val_loss: 0.4282 - val_acc: 0.8542\n",
      "Epoch 39/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2524 - acc: 0.8988 - val_loss: 0.4627 - val_acc: 0.8236\n",
      "Epoch 40/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2234 - acc: 0.9185 - val_loss: 0.3888 - val_acc: 0.8821\n",
      "Epoch 41/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.2079 - acc: 0.9221 - val_loss: 0.3677 - val_acc: 0.8884\n",
      "Epoch 42/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1815 - acc: 0.9271 - val_loss: 0.3538 - val_acc: 0.8893\n",
      "Epoch 43/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1777 - acc: 0.9381 - val_loss: 0.3900 - val_acc: 0.8731\n",
      "Epoch 44/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1815 - acc: 0.9334 - val_loss: 0.3678 - val_acc: 0.8812\n",
      "Epoch 45/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1647 - acc: 0.9418 - val_loss: 0.3546 - val_acc: 0.8830\n",
      "Epoch 46/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1713 - acc: 0.9374 - val_loss: 0.3734 - val_acc: 0.8740\n",
      "Epoch 47/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1706 - acc: 0.9371 - val_loss: 0.3205 - val_acc: 0.9127\n",
      "Epoch 48/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1595 - acc: 0.9401 - val_loss: 0.3544 - val_acc: 0.8884\n",
      "Epoch 49/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1428 - acc: 0.9527 - val_loss: 0.3255 - val_acc: 0.8965\n",
      "Epoch 50/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1648 - acc: 0.9438 - val_loss: 0.3281 - val_acc: 0.9001\n",
      "Epoch 51/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1574 - acc: 0.9428 - val_loss: 0.3047 - val_acc: 0.9181\n",
      "Epoch 52/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1577 - acc: 0.9511 - val_loss: 0.3677 - val_acc: 0.8758\n",
      "Epoch 53/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1544 - acc: 0.9411 - val_loss: 0.2990 - val_acc: 0.9145\n",
      "Epoch 54/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1539 - acc: 0.9431 - val_loss: 0.2919 - val_acc: 0.9127\n",
      "Epoch 55/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1407 - acc: 0.9491 - val_loss: 0.3511 - val_acc: 0.8794\n",
      "Epoch 56/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1337 - acc: 0.9524 - val_loss: 0.3004 - val_acc: 0.9010\n",
      "Epoch 57/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1460 - acc: 0.9474 - val_loss: 0.3107 - val_acc: 0.9028\n",
      "Epoch 58/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1288 - acc: 0.9547 - val_loss: 0.2824 - val_acc: 0.9127\n",
      "Epoch 59/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1268 - acc: 0.9544 - val_loss: 0.3084 - val_acc: 0.9091\n",
      "Epoch 60/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1298 - acc: 0.9571 - val_loss: 0.2850 - val_acc: 0.9244\n",
      "Epoch 61/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1270 - acc: 0.9564 - val_loss: 0.3208 - val_acc: 0.8857\n",
      "Epoch 62/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1212 - acc: 0.9577 - val_loss: 0.3167 - val_acc: 0.9028\n",
      "Epoch 63/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1400 - acc: 0.9484 - val_loss: 0.2905 - val_acc: 0.9127\n",
      "Epoch 64/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1465 - acc: 0.9524 - val_loss: 0.2943 - val_acc: 0.9064\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3005/3005 [==============================] - 1s - loss: 0.1139 - acc: 0.9584 - val_loss: 0.2641 - val_acc: 0.9217\n",
      "Epoch 66/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0984 - acc: 0.9677 - val_loss: 0.3107 - val_acc: 0.8911\n",
      "Epoch 67/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1175 - acc: 0.9574 - val_loss: 0.3179 - val_acc: 0.8920\n",
      "Epoch 68/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1183 - acc: 0.9584 - val_loss: 0.2966 - val_acc: 0.9055\n",
      "Epoch 69/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0967 - acc: 0.9671 - val_loss: 0.2875 - val_acc: 0.8983\n",
      "Epoch 70/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1026 - acc: 0.9621 - val_loss: 0.2767 - val_acc: 0.9127\n",
      "Epoch 71/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0991 - acc: 0.9674 - val_loss: 0.2866 - val_acc: 0.9163\n",
      "Epoch 72/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0809 - acc: 0.9734 - val_loss: 0.2793 - val_acc: 0.9100\n",
      "Epoch 73/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1039 - acc: 0.9617 - val_loss: 0.2758 - val_acc: 0.9154\n",
      "Epoch 74/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0956 - acc: 0.9687 - val_loss: 0.2635 - val_acc: 0.9190\n",
      "Epoch 75/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0895 - acc: 0.9674 - val_loss: 0.2823 - val_acc: 0.9163\n",
      "Epoch 76/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0969 - acc: 0.9661 - val_loss: 0.2507 - val_acc: 0.9271\n",
      "Epoch 77/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0841 - acc: 0.9687 - val_loss: 0.2754 - val_acc: 0.9118\n",
      "Epoch 78/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0911 - acc: 0.9697 - val_loss: 0.2857 - val_acc: 0.9199\n",
      "Epoch 79/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0866 - acc: 0.9704 - val_loss: 0.2361 - val_acc: 0.9307\n",
      "Epoch 80/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0840 - acc: 0.9714 - val_loss: 0.2931 - val_acc: 0.9019\n",
      "Epoch 81/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0797 - acc: 0.9717 - val_loss: 0.3041 - val_acc: 0.8929\n",
      "Epoch 82/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0909 - acc: 0.9687 - val_loss: 0.2606 - val_acc: 0.9163\n",
      "Epoch 83/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0816 - acc: 0.9707 - val_loss: 0.2842 - val_acc: 0.9109\n",
      "Epoch 84/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0960 - acc: 0.9654 - val_loss: 0.2492 - val_acc: 0.9244\n",
      "Epoch 85/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0989 - acc: 0.9671 - val_loss: 0.2620 - val_acc: 0.9163\n",
      "Epoch 86/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0906 - acc: 0.9684 - val_loss: 0.2509 - val_acc: 0.9226\n",
      "Epoch 87/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0854 - acc: 0.9737 - val_loss: 0.3150 - val_acc: 0.8875\n",
      "Epoch 88/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0753 - acc: 0.9700 - val_loss: 0.2305 - val_acc: 0.9280\n",
      "Epoch 89/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0704 - acc: 0.9784 - val_loss: 0.2350 - val_acc: 0.9280\n",
      "Epoch 90/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0834 - acc: 0.9681 - val_loss: 0.2631 - val_acc: 0.9271\n",
      "Epoch 91/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0844 - acc: 0.9697 - val_loss: 0.2586 - val_acc: 0.9226\n",
      "Epoch 92/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0805 - acc: 0.9720 - val_loss: 0.2660 - val_acc: 0.9181\n",
      "Epoch 93/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0630 - acc: 0.9780 - val_loss: 0.2745 - val_acc: 0.9172\n",
      "Epoch 94/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0776 - acc: 0.9724 - val_loss: 0.2757 - val_acc: 0.9181\n",
      "Epoch 95/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1107 - acc: 0.9594 - val_loss: 0.2900 - val_acc: 0.9100\n",
      "Epoch 96/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1191 - acc: 0.9571 - val_loss: 0.2713 - val_acc: 0.9118\n",
      "Epoch 97/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0854 - acc: 0.9700 - val_loss: 0.2684 - val_acc: 0.9199\n",
      "Epoch 98/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0774 - acc: 0.9707 - val_loss: 0.2579 - val_acc: 0.9262\n",
      "Epoch 99/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0727 - acc: 0.9737 - val_loss: 0.2349 - val_acc: 0.9289\n",
      "Epoch 100/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0743 - acc: 0.9747 - val_loss: 0.3628 - val_acc: 0.8821\n",
      "Epoch 101/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0673 - acc: 0.9774 - val_loss: 0.2561 - val_acc: 0.9190\n",
      "Epoch 102/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0626 - acc: 0.9787 - val_loss: 0.2640 - val_acc: 0.9154\n",
      "Epoch 103/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0697 - acc: 0.9744 - val_loss: 0.3279 - val_acc: 0.8857\n",
      "Epoch 104/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0816 - acc: 0.9684 - val_loss: 0.2133 - val_acc: 0.9388\n",
      "Epoch 105/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0633 - acc: 0.9777 - val_loss: 0.2957 - val_acc: 0.9001\n",
      "Epoch 106/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0705 - acc: 0.9730 - val_loss: 0.2794 - val_acc: 0.9091\n",
      "Epoch 107/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0867 - acc: 0.9681 - val_loss: 0.3002 - val_acc: 0.8965\n",
      "Epoch 108/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.1074 - acc: 0.9587 - val_loss: 0.3113 - val_acc: 0.9046\n",
      "Epoch 109/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0841 - acc: 0.9651 - val_loss: 0.2424 - val_acc: 0.9217\n",
      "Epoch 110/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0696 - acc: 0.9754 - val_loss: 0.3172 - val_acc: 0.8947\n",
      "Epoch 111/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0602 - acc: 0.9790 - val_loss: 0.2597 - val_acc: 0.9145\n",
      "Epoch 112/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0552 - acc: 0.9810 - val_loss: 0.2605 - val_acc: 0.9226\n",
      "Epoch 113/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0647 - acc: 0.9760 - val_loss: 0.2853 - val_acc: 0.9010\n",
      "Epoch 114/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0677 - acc: 0.9790 - val_loss: 0.2870 - val_acc: 0.9154\n",
      "Epoch 115/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0600 - acc: 0.9774 - val_loss: 0.2627 - val_acc: 0.9190\n",
      "Epoch 116/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0636 - acc: 0.9794 - val_loss: 0.2415 - val_acc: 0.9217\n",
      "Epoch 117/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0577 - acc: 0.9787 - val_loss: 0.2604 - val_acc: 0.9208\n",
      "Epoch 118/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0590 - acc: 0.9787 - val_loss: 0.2567 - val_acc: 0.9208\n",
      "Epoch 119/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0578 - acc: 0.9790 - val_loss: 0.2515 - val_acc: 0.9118\n",
      "Epoch 120/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0522 - acc: 0.9830 - val_loss: 0.2460 - val_acc: 0.9199\n",
      "Epoch 121/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0503 - acc: 0.9840 - val_loss: 0.2496 - val_acc: 0.9307\n",
      "Epoch 122/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0522 - acc: 0.9857 - val_loss: 0.2568 - val_acc: 0.9226\n",
      "Epoch 123/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0589 - acc: 0.9790 - val_loss: 0.2310 - val_acc: 0.9271\n",
      "Epoch 124/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0637 - acc: 0.9747 - val_loss: 0.3113 - val_acc: 0.9073\n",
      "Epoch 125/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0616 - acc: 0.9790 - val_loss: 0.2862 - val_acc: 0.9082\n",
      "Epoch 126/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0694 - acc: 0.9767 - val_loss: 0.2394 - val_acc: 0.9199\n",
      "Epoch 127/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0683 - acc: 0.9774 - val_loss: 0.3238 - val_acc: 0.9082\n",
      "Epoch 128/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0512 - acc: 0.9810 - val_loss: 0.2833 - val_acc: 0.9154\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3005/3005 [==============================] - 1s - loss: 0.0593 - acc: 0.9804 - val_loss: 0.3369 - val_acc: 0.8974\n",
      "Epoch 130/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0527 - acc: 0.9830 - val_loss: 0.2353 - val_acc: 0.9280\n",
      "Epoch 131/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0497 - acc: 0.9837 - val_loss: 0.2330 - val_acc: 0.9334\n",
      "Epoch 132/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0466 - acc: 0.9864 - val_loss: 0.2332 - val_acc: 0.9271\n",
      "Epoch 133/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0462 - acc: 0.9860 - val_loss: 0.2540 - val_acc: 0.9235\n",
      "Epoch 134/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0476 - acc: 0.9840 - val_loss: 0.2201 - val_acc: 0.9379\n",
      "Epoch 135/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0411 - acc: 0.9864 - val_loss: 0.2692 - val_acc: 0.9118\n",
      "Epoch 136/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0486 - acc: 0.9790 - val_loss: 0.2222 - val_acc: 0.9361\n",
      "Epoch 137/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0560 - acc: 0.9794 - val_loss: 0.2778 - val_acc: 0.9118\n",
      "Epoch 138/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0534 - acc: 0.9790 - val_loss: 0.2385 - val_acc: 0.9298\n",
      "Epoch 139/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0458 - acc: 0.9834 - val_loss: 0.2826 - val_acc: 0.9145\n",
      "Epoch 140/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0554 - acc: 0.9804 - val_loss: 0.2751 - val_acc: 0.9055\n",
      "Epoch 141/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0677 - acc: 0.9767 - val_loss: 0.2841 - val_acc: 0.9109\n",
      "Epoch 142/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0485 - acc: 0.9817 - val_loss: 0.2733 - val_acc: 0.9190\n",
      "Epoch 143/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0581 - acc: 0.9824 - val_loss: 0.3162 - val_acc: 0.9127\n",
      "Epoch 144/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0680 - acc: 0.9764 - val_loss: 0.2483 - val_acc: 0.9226\n",
      "Epoch 145/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0618 - acc: 0.9780 - val_loss: 0.2629 - val_acc: 0.9163\n",
      "Epoch 146/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0565 - acc: 0.9824 - val_loss: 0.2564 - val_acc: 0.9289\n",
      "Epoch 147/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0478 - acc: 0.9820 - val_loss: 0.2314 - val_acc: 0.9325\n",
      "Epoch 148/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0462 - acc: 0.9827 - val_loss: 0.3104 - val_acc: 0.9055\n",
      "Epoch 149/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0478 - acc: 0.9814 - val_loss: 0.2765 - val_acc: 0.9244\n",
      "Epoch 150/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0444 - acc: 0.9827 - val_loss: 0.2644 - val_acc: 0.9181\n",
      "Epoch 151/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0514 - acc: 0.9830 - val_loss: 0.2442 - val_acc: 0.9280\n",
      "Epoch 152/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0416 - acc: 0.9874 - val_loss: 0.2122 - val_acc: 0.9361\n",
      "Epoch 153/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0556 - acc: 0.9800 - val_loss: 0.2457 - val_acc: 0.9289\n",
      "Epoch 154/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0447 - acc: 0.9860 - val_loss: 0.2796 - val_acc: 0.9181\n",
      "Epoch 155/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0881 - acc: 0.9700 - val_loss: 0.2431 - val_acc: 0.9217\n",
      "Epoch 156/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0873 - acc: 0.9710 - val_loss: 0.2400 - val_acc: 0.9217\n",
      "Epoch 157/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0573 - acc: 0.9787 - val_loss: 0.2318 - val_acc: 0.9325\n",
      "Epoch 158/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0580 - acc: 0.9760 - val_loss: 0.2401 - val_acc: 0.9262\n",
      "Epoch 159/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0476 - acc: 0.9830 - val_loss: 0.2503 - val_acc: 0.9289\n",
      "Epoch 160/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0552 - acc: 0.9797 - val_loss: 0.2490 - val_acc: 0.9181\n",
      "Epoch 161/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0585 - acc: 0.9810 - val_loss: 0.2369 - val_acc: 0.9334\n",
      "Epoch 162/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0459 - acc: 0.9844 - val_loss: 0.2561 - val_acc: 0.9190\n",
      "Epoch 163/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0517 - acc: 0.9794 - val_loss: 0.2317 - val_acc: 0.9352\n",
      "Epoch 164/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0524 - acc: 0.9797 - val_loss: 0.2563 - val_acc: 0.9226\n",
      "Epoch 165/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0602 - acc: 0.9794 - val_loss: 0.2375 - val_acc: 0.9244\n",
      "Epoch 166/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0466 - acc: 0.9820 - val_loss: 0.2299 - val_acc: 0.9370\n",
      "Epoch 167/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0539 - acc: 0.9814 - val_loss: 0.2560 - val_acc: 0.9307\n",
      "Epoch 168/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0498 - acc: 0.9820 - val_loss: 0.3117 - val_acc: 0.9136\n",
      "Epoch 169/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0443 - acc: 0.9834 - val_loss: 0.2497 - val_acc: 0.9244\n",
      "Epoch 170/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0443 - acc: 0.9864 - val_loss: 0.2426 - val_acc: 0.9289\n",
      "Epoch 171/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0504 - acc: 0.9817 - val_loss: 0.2351 - val_acc: 0.9280\n",
      "Epoch 172/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0643 - acc: 0.9790 - val_loss: 0.3337 - val_acc: 0.8893\n",
      "Epoch 173/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0823 - acc: 0.9657 - val_loss: 0.2435 - val_acc: 0.9280\n",
      "Epoch 174/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0419 - acc: 0.9840 - val_loss: 0.2419 - val_acc: 0.9289\n",
      "Epoch 175/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0452 - acc: 0.9860 - val_loss: 0.3338 - val_acc: 0.9046\n",
      "Epoch 176/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0571 - acc: 0.9760 - val_loss: 0.2747 - val_acc: 0.9244\n",
      "Epoch 177/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0445 - acc: 0.9840 - val_loss: 0.2471 - val_acc: 0.9334\n",
      "Epoch 178/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0521 - acc: 0.9810 - val_loss: 0.2455 - val_acc: 0.9262\n",
      "Epoch 179/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0427 - acc: 0.9844 - val_loss: 0.2504 - val_acc: 0.9280\n",
      "Epoch 180/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0397 - acc: 0.9837 - val_loss: 0.2939 - val_acc: 0.9082\n",
      "Epoch 181/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0455 - acc: 0.9824 - val_loss: 0.2621 - val_acc: 0.9289\n",
      "Epoch 182/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0438 - acc: 0.9844 - val_loss: 0.2732 - val_acc: 0.9235\n",
      "Epoch 183/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0477 - acc: 0.9837 - val_loss: 0.2398 - val_acc: 0.9334\n",
      "Epoch 184/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0382 - acc: 0.9860 - val_loss: 0.2718 - val_acc: 0.9262\n",
      "Epoch 185/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0352 - acc: 0.9860 - val_loss: 0.2676 - val_acc: 0.9271\n",
      "Epoch 186/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0331 - acc: 0.9884 - val_loss: 0.3337 - val_acc: 0.9064\n",
      "Epoch 187/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0847 - acc: 0.9697 - val_loss: 0.2918 - val_acc: 0.9199\n",
      "Epoch 188/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0793 - acc: 0.9684 - val_loss: 0.2510 - val_acc: 0.9244\n",
      "Epoch 189/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0554 - acc: 0.9787 - val_loss: 0.2628 - val_acc: 0.9334\n",
      "Epoch 190/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0418 - acc: 0.9830 - val_loss: 0.2801 - val_acc: 0.9118\n",
      "Epoch 191/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0455 - acc: 0.9837 - val_loss: 0.2759 - val_acc: 0.9271\n",
      "Epoch 192/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0371 - acc: 0.9874 - val_loss: 0.2119 - val_acc: 0.9379\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3005/3005 [==============================] - 1s - loss: 0.0318 - acc: 0.9884 - val_loss: 0.2385 - val_acc: 0.9343\n",
      "Epoch 194/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0308 - acc: 0.9897 - val_loss: 0.3118 - val_acc: 0.9244\n",
      "Epoch 195/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0402 - acc: 0.9874 - val_loss: 0.2894 - val_acc: 0.9208\n",
      "Epoch 196/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0342 - acc: 0.9894 - val_loss: 0.3025 - val_acc: 0.9136\n",
      "Epoch 197/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0438 - acc: 0.9837 - val_loss: 0.3150 - val_acc: 0.9190\n",
      "Epoch 198/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0301 - acc: 0.9910 - val_loss: 0.2563 - val_acc: 0.9361\n",
      "Epoch 199/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0309 - acc: 0.9887 - val_loss: 0.2663 - val_acc: 0.9352\n",
      "Epoch 200/200\n",
      "3005/3005 [==============================] - 1s - loss: 0.0328 - acc: 0.9877 - val_loss: 0.2560 - val_acc: 0.9289\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=128, epochs=200, validation_data=(X_val, Y_val)) #callbacks=[early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import operator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[226,  15,  21,   6],\n",
       "       [  0, 183,  41,   4],\n",
       "       [  2,   6, 333,   8],\n",
       "       [  0,   7,  10, 143]], dtype=int64)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = model.predict(X_val)\n",
    "y_true = [max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_val]\n",
    "y_pred = [max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred]\n",
    "\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.70719121e-15,   1.00000000e+00],\n",
       "       [  5.68544777e-14,   1.00000000e+00],\n",
       "       [  1.99857548e-18,   1.00000000e+00],\n",
       "       [  1.62834822e-05,   9.99983668e-01],\n",
       "       [  2.65297940e-11,   1.00000000e+00],\n",
       "       [  3.26242852e-12,   1.00000000e+00],\n",
       "       [  4.11501969e-06,   9.99995828e-01],\n",
       "       [  1.92551225e-10,   1.00000000e+00],\n",
       "       [  6.02835673e-04,   9.99397159e-01],\n",
       "       [  2.27131158e-09,   1.00000000e+00],\n",
       "       [  4.50570951e-05,   9.99954939e-01],\n",
       "       [  5.96788668e-05,   9.99940276e-01],\n",
       "       [  5.31585003e-14,   1.00000000e+00],\n",
       "       [  6.96933782e-08,   9.99999881e-01],\n",
       "       [  1.58653242e-13,   1.00000000e+00],\n",
       "       [  1.53077719e-26,   1.00000000e+00],\n",
       "       [  2.25973595e-02,   9.77402627e-01],\n",
       "       [  4.77840245e-01,   5.22159815e-01],\n",
       "       [  2.87815639e-11,   1.00000000e+00],\n",
       "       [  1.31480746e-28,   1.00000000e+00],\n",
       "       [  1.57597730e-12,   1.00000000e+00],\n",
       "       [  1.88092380e-12,   1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_cyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activ = model2.predict(X_train)\n",
    "#activ = model2.predict(np.expand_dims(X_train[10],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c8e3907550>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvB8ZGkIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c8e38b92b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(activ[20][:,:,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,  121.71251678,   53.23372269,    0.        ,\n",
       "          0.        ,   24.43340492,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    7.88301706,    0.        ], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activ[0][5,5,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['12296182_23.npy', '13028374_2.npy', 'E104657225_14.npy',\n",
       "       '12972894_6.npy'],\n",
       "      dtype='<U17')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_val[::30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47784024,  0.52215981], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8881987577639752"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "143/(143+18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(15+6+7)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.99998% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.99858% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.99996% confidence)', 'cyst (99.99999% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.60698% confidence)', 'hcc (70.35562% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)']\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "#print(\"Ground truth:\", [cls_mapping[max(enumerate(x), key=operator.itemgetter(1))[0]] for x in Y_val[::30]])\n",
    "Y_ = model.predict(X_cyst)\n",
    "print(\"Predictions:\", [cls_mapping[max(enumerate(x), key=operator.itemgetter(1))[0]] + \" (%.5f%% confidence)\" % (max(x)*100) for x in Y_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x266df7b4da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "voi_df = pd.read_csv('..\\\\liver-mr-processor\\\\vois.csv')\n",
    "img_fn = \"12972894.npy\"\n",
    "img = np.load(\"..\\\\liver-mr-processor\\\\full_imgs\\\\\"+img_fn)\n",
    "plot_section(img, voi_df[voi_df[\"Filename\"] == img_fn].iloc[0], pad=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_section(img, df, pad=30):\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(np.transpose(img[df['x1']-pad:df['x2']+pad,\n",
    "                                df['y2']+pad:df['y1']-pad:-1,\n",
    "                                (df['z1']+df['z2'])//2, 0], (1,0)), cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(np.transpose(img[df['x1']-pad:df['x2']+pad,\n",
    "                                df['y2']+pad:df['y1']-pad:-1,\n",
    "                                (df['z1']+df['z2'])//2, 1], (1,0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.78843247e-04,   9.99721110e-01], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sum(y)/len(y), 1-sum(y)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20054101943969727\n"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "Y_ = model.predict(X_val)\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(X[650,:,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20b42b02b70>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20b424cdcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0,:,:,5,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
