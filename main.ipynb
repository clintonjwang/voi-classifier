{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input, Add, Dense, Activation, Flatten, Dropout, Conv3D, MaxPooling3D, ZeroPadding3D, AveragePooling3D, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import config\n",
    "import importlib\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "nb_classes = C.nb_classes\n",
    "\n",
    "X_train = np.expand_dims(x_train,axis=3)\n",
    "X_train = np.expand_dims(X_train,axis=4)\n",
    "X_test = np.expand_dims(x_test,axis=3)\n",
    "X_test = np.expand_dims(X_test,axis=4)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\config.py'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C = config.Config()\n",
    "dims = C.dims\n",
    "voi_img = Input(shape=(dims[0], dims[1], dims[2], C.nb_channels))\n",
    "#x = Dropout(0.2)(voi_img)\n",
    "#x = GaussianNoise(1)(x)\n",
    "#x = ZeroPadding3D(padding=(3,3,2))(voi_img)\n",
    "#x = Conv3D(filters=128, kernel_size=(3,3,2), activation='relu', kernel_regularizer=l2(.01))(x)\n",
    "x = Conv3D(filters=32, kernel_size=(3,3,1), activation='relu')(voi_img)\n",
    "x = Conv3D(filters=64, kernel_size=(3,3,1), activation='relu')(x)\n",
    "x = Dropout(0.2)(x)\n",
    "#x = Dropout(0.2)(x)\n",
    "intermed = MaxPooling3D((2, 2, 2))(x)\n",
    "x = Flatten()(intermed)\n",
    "x = Dense(128, activation='relu')(x)#, kernel_initializer='normal', kernel_regularizer=l1(.01), kernel_constraint=max_norm(3.))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "pred_class = Dense(C.nb_classes, activation='softmax')(x)#Dense(C.nb_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_11 (InputLayer)        (None, 20, 20, 10, 2)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_21 (Conv3D)           (None, 18, 18, 10, 32)    608       \n",
      "_________________________________________________________________\n",
      "conv3d_22 (Conv3D)           (None, 16, 16, 10, 64)    18496     \n",
      "_________________________________________________________________\n",
      "dropout_21 (Dropout)         (None, 16, 16, 10, 64)    0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_11 (MaxPooling (None, 8, 8, 5, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_11 (Flatten)         (None, 20480)             0         \n",
      "_________________________________________________________________\n",
      "dense_21 (Dense)             (None, 128)               2621568   \n",
      "_________________________________________________________________\n",
      "dropout_22 (Dropout)         (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_22 (Dense)             (None, 2)                 258       \n",
      "=================================================================\n",
      "Total params: 2,640,930\n",
      "Trainable params: 2,640,930\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(lr=0.01)#, decay=0.001)\n",
    "early_stopping = EarlyStopping(min_delta=0.01, patience=5)\n",
    "\n",
    "model = Model(voi_img, pred_class)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = Model(voi_img, intermed)\n",
    "\n",
    "for l in range(2,len(model2.layers)):\n",
    "    model2.layers[l].set_weights(model.layers[l].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X_train.append(np.ones(dims + [C.nb_channels]))\n",
    "    Y_train.append([1,0])\n",
    "for _ in range(10):\n",
    "    X_train.append(np.ones(dims + [C.nb_channels]))\n",
    "    X_train[-1][5:15,5:15,5:7,0] = 2\n",
    "    Y_train.append([0,1])\n",
    "    \n",
    "X_train = np.array(X_train) # X[:total_size//2]\n",
    "#X_val = np.array(X_test)\n",
    "Y_train = np.array(Y_train) # Y[:total_size//2]\n",
    "#Y_val = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "max_samples = {}\n",
    "base_dir = \"..\\\\liver-mr-processor\\\\train_imgs\\\\\"\n",
    "\n",
    "least_in_one_class = np.Inf\n",
    "\n",
    "for class_name in os.listdir(base_dir):\n",
    "    x = np.empty((10000, dims[0], dims[1], dims[2], C.nb_channels))\n",
    "    z = []\n",
    "    \n",
    "    for index, img_fn in enumerate(os.listdir(base_dir+class_name)):\n",
    "        x[index] = np.load(base_dir+class_name+\"\\\\\"+img_fn)\n",
    "        z.append(img_fn)\n",
    "    \n",
    "    x.resize((index, dims[0], dims[1], dims[2], C.nb_channels))\n",
    "    data_dict[class_name] = [x,np.array(z)]\n",
    "    max_samples[class_name] = index\n",
    "    \n",
    "    least_in_one_class = min(index, least_in_one_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_cyst = data_dict[\"cyst\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47784024,  0.52215981], dtype=float32)"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'E100529980_0.npy'"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_cyst[-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_cyst = data_dict[\"cyst\"][0]\n",
    "X_cyst = np.array(X_cyst) # X[:total_size//2]\n",
    "X_cyst /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_hcc = data_dict[\"hcc\"][0]\n",
    "X_hcc = np.array(X_hcc) # X[:total_size//2]\n",
    "X_hcc /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "Z_train = []\n",
    "X_test = []\n",
    "Y_test = []\n",
    "Z_test = []\n",
    "#train_frac = 0.75\n",
    "cls_mapping = []\n",
    "\n",
    "for cls_num, cls in enumerate(data_dict):\n",
    "    if cls == \"cyst\":\n",
    "        train_frac = 1.4\n",
    "    else:\n",
    "        train_frac = 0.8\n",
    "    #order = np.random.permutation(list(range(max_samples)))\n",
    "    order = list(range(max_samples[cls]))\n",
    "    X_train = X_train + list(data_dict[cls][0][order[:round(least_in_one_class*train_frac)]])\n",
    "    X_test = X_test + list(data_dict[cls][0][order[round(least_in_one_class*train_frac):]])\n",
    "    Z_train = Z_train + list(data_dict[cls][1][order[:round(least_in_one_class*train_frac)]])\n",
    "    Z_test = Z_test + list(data_dict[cls][1][order[round(least_in_one_class*train_frac):]])\n",
    "    Y_train = Y_train + [[0] * cls_num + [1] + [0] * (C.nb_classes - cls_num - 1)] * (round(least_in_one_class*train_frac))\n",
    "    Y_test = Y_test + [[0] * cls_num + [1] + [0] * (C.nb_classes - cls_num - 1)] * \\\n",
    "                        (max_samples[cls] - round(least_in_one_class*train_frac))\n",
    "        \n",
    "    cls_mapping.append(cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train) # X[:total_size//2]\n",
    "X_train /= 255\n",
    "X_val = np.array(X_test)\n",
    "X_val /= 255\n",
    "#X_val = X[total_size//2:total_size*3//4]\n",
    "#X_test = X[total_size*3//4:]\n",
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(Y_test)\n",
    "Z_train = np.array(Z_train)\n",
    "Z_val = np.array(Z_test)\n",
    "#Y_val = Y[total_size//2:total_size*3//4]\n",
    "#Y_test = Y[total_size*3//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21.933333333333334"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Z_train)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.0"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Z_val)/30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = np.array([z for z in x])\n",
    "#Y = np.array(y)\n",
    "#Y = np.array([[0,1] if y[x] == 1 else [1,0] for x in range(len(y))])\n",
    "#Y = K.constant(y, dtype=tf.int32)\n",
    "#Y = K.one_hot(Y, C.nb_classes)\n",
    "\n",
    "#total_size = X.shape[0]\n",
    "\n",
    "#order = np.random.permutation(list(range(total_size)))\n",
    "#X = X[order]\n",
    "#Y = Y[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_generator(X, Y):\n",
    "    while True:\n",
    "        for i in range(len(X)):\n",
    "            yield np.expand_dims(X[i], axis=0), np.expand_dims(Y[i], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_epochs = 10\n",
    "epoch_length = 100\n",
    "best_loss = np.Inf\n",
    "losses = np.zeros(epoch_length)\n",
    "acc = np.zeros(epoch_length)\n",
    "\n",
    "data_gen_train = train_generator(X_train, Y_train)\n",
    "for epoch_num in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
    "    iter_num = 0\n",
    "    \n",
    "    while True:\n",
    "        X, Y = next(data_gen_train)\n",
    "        losses[iter_num], acc[iter_num] = model.train_on_batch(X, Y)\n",
    "\n",
    "        iter_num += 1\n",
    "        if iter_num == epoch_length:\n",
    "            curr_loss = np.mean(losses)\n",
    "            curr_acc = np.mean(acc)\n",
    "            print(\"Mean Loss:\", curr_loss, \"// Mean Accuracy:\", curr_acc)\n",
    "\n",
    "            if curr_loss < best_loss:\n",
    "                print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "                best_loss = curr_loss\n",
    "                model.save_weights(C.model_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 658 samples, validate on 330 samples\n",
      "Epoch 1/200\n",
      "658/658 [==============================] - 1s - loss: 0.7848 - acc: 0.5350 - val_loss: 0.5602 - val_acc: 0.8182\n",
      "Epoch 2/200\n",
      "658/658 [==============================] - 0s - loss: 0.6343 - acc: 0.6581 - val_loss: 0.4769 - val_acc: 0.8182\n",
      "Epoch 3/200\n",
      "658/658 [==============================] - 0s - loss: 0.4798 - acc: 0.7401 - val_loss: 0.6404 - val_acc: 0.6727\n",
      "Epoch 4/200\n",
      "658/658 [==============================] - 0s - loss: 0.4194 - acc: 0.7994 - val_loss: 0.5685 - val_acc: 0.8212\n",
      "Epoch 5/200\n",
      "658/658 [==============================] - 0s - loss: 0.3721 - acc: 0.8116 - val_loss: 0.7230 - val_acc: 0.8212\n",
      "Epoch 6/200\n",
      "658/658 [==============================] - 0s - loss: 0.3474 - acc: 0.8435 - val_loss: 0.5105 - val_acc: 0.8667\n",
      "Epoch 7/200\n",
      "658/658 [==============================] - 0s - loss: 0.3031 - acc: 0.8480 - val_loss: 0.4781 - val_acc: 0.8212\n",
      "Epoch 8/200\n",
      "658/658 [==============================] - 0s - loss: 0.2345 - acc: 0.9088 - val_loss: 0.5565 - val_acc: 0.8212\n",
      "Epoch 9/200\n",
      "658/658 [==============================] - 0s - loss: 0.1940 - acc: 0.9164 - val_loss: 0.4622 - val_acc: 0.8212\n",
      "Epoch 10/200\n",
      "658/658 [==============================] - 0s - loss: 0.1993 - acc: 0.9179 - val_loss: 0.3448 - val_acc: 0.8727\n",
      "Epoch 11/200\n",
      "658/658 [==============================] - 0s - loss: 0.1615 - acc: 0.9407 - val_loss: 0.2849 - val_acc: 0.8879\n",
      "Epoch 12/200\n",
      "658/658 [==============================] - 0s - loss: 0.1353 - acc: 0.9635 - val_loss: 0.2275 - val_acc: 0.8848\n",
      "Epoch 13/200\n",
      "658/658 [==============================] - 0s - loss: 0.1186 - acc: 0.9666 - val_loss: 0.3393 - val_acc: 0.8303\n",
      "Epoch 14/200\n",
      "658/658 [==============================] - 0s - loss: 0.0937 - acc: 0.9635 - val_loss: 0.2353 - val_acc: 0.8909\n",
      "Epoch 15/200\n",
      "658/658 [==============================] - 0s - loss: 0.0759 - acc: 0.9742 - val_loss: 0.2025 - val_acc: 0.8909\n",
      "Epoch 16/200\n",
      "658/658 [==============================] - 0s - loss: 0.0644 - acc: 0.9802 - val_loss: 0.2069 - val_acc: 0.9000\n",
      "Epoch 17/200\n",
      "658/658 [==============================] - 0s - loss: 0.0505 - acc: 0.9924 - val_loss: 0.2133 - val_acc: 0.8667\n",
      "Epoch 18/200\n",
      "658/658 [==============================] - 0s - loss: 0.0434 - acc: 0.9909 - val_loss: 0.1767 - val_acc: 0.9061\n",
      "Epoch 19/200\n",
      "658/658 [==============================] - 0s - loss: 0.0392 - acc: 0.9939 - val_loss: 0.2139 - val_acc: 0.8727\n",
      "Epoch 20/200\n",
      "658/658 [==============================] - 0s - loss: 0.0410 - acc: 0.9909 - val_loss: 0.1891 - val_acc: 0.8788\n",
      "Epoch 21/200\n",
      "658/658 [==============================] - 0s - loss: 0.0493 - acc: 0.9848 - val_loss: 0.1915 - val_acc: 0.9061\n",
      "Epoch 22/200\n",
      "658/658 [==============================] - 0s - loss: 0.0523 - acc: 0.9787 - val_loss: 0.2545 - val_acc: 0.8939\n",
      "Epoch 23/200\n",
      "658/658 [==============================] - 0s - loss: 0.0476 - acc: 0.9924 - val_loss: 0.1915 - val_acc: 0.8970\n",
      "Epoch 24/200\n",
      "658/658 [==============================] - 0s - loss: 0.0370 - acc: 0.9909 - val_loss: 0.1601 - val_acc: 0.9273\n",
      "Epoch 25/200\n",
      "658/658 [==============================] - 0s - loss: 0.0341 - acc: 0.9954 - val_loss: 0.1647 - val_acc: 0.9030\n",
      "Epoch 26/200\n",
      "658/658 [==============================] - 0s - loss: 0.0309 - acc: 0.9954 - val_loss: 0.1433 - val_acc: 0.9364\n",
      "Epoch 27/200\n",
      "658/658 [==============================] - 0s - loss: 0.0231 - acc: 0.9939 - val_loss: 0.1425 - val_acc: 0.9485\n",
      "Epoch 28/200\n",
      "658/658 [==============================] - 0s - loss: 0.0216 - acc: 0.9970 - val_loss: 0.1546 - val_acc: 0.9394\n",
      "Epoch 29/200\n",
      "658/658 [==============================] - 0s - loss: 0.0281 - acc: 0.9924 - val_loss: 0.1321 - val_acc: 0.9364\n",
      "Epoch 30/200\n",
      "658/658 [==============================] - 0s - loss: 0.0215 - acc: 0.9970 - val_loss: 0.1122 - val_acc: 0.9576\n",
      "Epoch 31/200\n",
      "658/658 [==============================] - 0s - loss: 0.0234 - acc: 0.9939 - val_loss: 0.1337 - val_acc: 0.9636\n",
      "Epoch 32/200\n",
      "658/658 [==============================] - 0s - loss: 0.0207 - acc: 0.9939 - val_loss: 0.1319 - val_acc: 0.9606\n",
      "Epoch 33/200\n",
      "658/658 [==============================] - 0s - loss: 0.0159 - acc: 0.9970 - val_loss: 0.1100 - val_acc: 0.9667\n",
      "Epoch 34/200\n",
      "658/658 [==============================] - 0s - loss: 0.0164 - acc: 0.9985 - val_loss: 0.1196 - val_acc: 0.9697\n",
      "Epoch 35/200\n",
      "658/658 [==============================] - 0s - loss: 0.0112 - acc: 1.0000 - val_loss: 0.1160 - val_acc: 0.9667\n",
      "Epoch 36/200\n",
      "658/658 [==============================] - 0s - loss: 0.0262 - acc: 0.9954 - val_loss: 0.1719 - val_acc: 0.9152\n",
      "Epoch 37/200\n",
      "658/658 [==============================] - 0s - loss: 0.0165 - acc: 0.9954 - val_loss: 0.1173 - val_acc: 0.9606\n",
      "Epoch 38/200\n",
      "658/658 [==============================] - 0s - loss: 0.0154 - acc: 0.9970 - val_loss: 0.1301 - val_acc: 0.9485\n",
      "Epoch 39/200\n",
      "658/658 [==============================] - 0s - loss: 0.0145 - acc: 0.9970 - val_loss: 0.1372 - val_acc: 0.9545\n",
      "Epoch 40/200\n",
      "658/658 [==============================] - 0s - loss: 0.0140 - acc: 0.9970 - val_loss: 0.1314 - val_acc: 0.9424\n",
      "Epoch 41/200\n",
      "658/658 [==============================] - 0s - loss: 0.0115 - acc: 0.9970 - val_loss: 0.1078 - val_acc: 0.9455\n",
      "Epoch 42/200\n",
      "658/658 [==============================] - 0s - loss: 0.0160 - acc: 1.0000 - val_loss: 0.1208 - val_acc: 0.9606\n",
      "Epoch 43/200\n",
      "658/658 [==============================] - 0s - loss: 0.0165 - acc: 0.9985 - val_loss: 0.1279 - val_acc: 0.9485\n",
      "Epoch 44/200\n",
      "658/658 [==============================] - 0s - loss: 0.0092 - acc: 1.0000 - val_loss: 0.0912 - val_acc: 0.9788\n",
      "Epoch 45/200\n",
      "658/658 [==============================] - 0s - loss: 0.0121 - acc: 1.0000 - val_loss: 0.1288 - val_acc: 0.9485\n",
      "Epoch 46/200\n",
      "658/658 [==============================] - 0s - loss: 0.0147 - acc: 0.9985 - val_loss: 0.0865 - val_acc: 0.9788\n",
      "Epoch 47/200\n",
      "658/658 [==============================] - 0s - loss: 0.0126 - acc: 0.9985 - val_loss: 0.0891 - val_acc: 0.9848\n",
      "Epoch 48/200\n",
      "658/658 [==============================] - 0s - loss: 0.0092 - acc: 0.9985 - val_loss: 0.1429 - val_acc: 0.9485\n",
      "Epoch 49/200\n",
      "658/658 [==============================] - 0s - loss: 0.0058 - acc: 1.0000 - val_loss: 0.1248 - val_acc: 0.9515\n",
      "Epoch 50/200\n",
      "658/658 [==============================] - 0s - loss: 0.0064 - acc: 0.9985 - val_loss: 0.0985 - val_acc: 0.9727\n",
      "Epoch 51/200\n",
      "658/658 [==============================] - 0s - loss: 0.0110 - acc: 0.9954 - val_loss: 0.1277 - val_acc: 0.9545\n",
      "Epoch 52/200\n",
      "658/658 [==============================] - 0s - loss: 0.0137 - acc: 0.9970 - val_loss: 0.1381 - val_acc: 0.9061\n",
      "Epoch 53/200\n",
      "658/658 [==============================] - 0s - loss: 0.0129 - acc: 0.9924 - val_loss: 0.1229 - val_acc: 0.9515\n",
      "Epoch 54/200\n",
      "658/658 [==============================] - 0s - loss: 0.0117 - acc: 1.0000 - val_loss: 0.1367 - val_acc: 0.9303\n",
      "Epoch 55/200\n",
      "658/658 [==============================] - 0s - loss: 0.0089 - acc: 1.0000 - val_loss: 0.1081 - val_acc: 0.9545\n",
      "Epoch 56/200\n",
      "658/658 [==============================] - 0s - loss: 0.0104 - acc: 1.0000 - val_loss: 0.1224 - val_acc: 0.9424\n",
      "Epoch 57/200\n",
      "658/658 [==============================] - 0s - loss: 0.0129 - acc: 0.9954 - val_loss: 0.0775 - val_acc: 0.9727\n",
      "Epoch 58/200\n",
      "658/658 [==============================] - 0s - loss: 0.0164 - acc: 0.9954 - val_loss: 0.1048 - val_acc: 0.9515\n",
      "Epoch 59/200\n",
      "658/658 [==============================] - 0s - loss: 0.0106 - acc: 0.9985 - val_loss: 0.1904 - val_acc: 0.9121\n",
      "Epoch 60/200\n",
      "658/658 [==============================] - 0s - loss: 0.0077 - acc: 1.0000 - val_loss: 0.1685 - val_acc: 0.9152\n",
      "Epoch 61/200\n",
      "658/658 [==============================] - 0s - loss: 0.0113 - acc: 1.0000 - val_loss: 0.0826 - val_acc: 0.9727\n",
      "Epoch 62/200\n",
      "658/658 [==============================] - 0s - loss: 0.0066 - acc: 0.9985 - val_loss: 0.0653 - val_acc: 0.9818\n",
      "Epoch 63/200\n",
      "658/658 [==============================] - 0s - loss: 0.0083 - acc: 1.0000 - val_loss: 0.1121 - val_acc: 0.9606\n",
      "Epoch 64/200\n",
      "658/658 [==============================] - 0s - loss: 0.0102 - acc: 0.9985 - val_loss: 0.0743 - val_acc: 0.9788\n",
      "Epoch 65/200\n",
      "658/658 [==============================] - 0s - loss: 0.0090 - acc: 0.9985 - val_loss: 0.0952 - val_acc: 0.9545\n",
      "Epoch 66/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/658 [==============================] - 0s - loss: 0.0088 - acc: 1.0000 - val_loss: 0.1153 - val_acc: 0.9394\n",
      "Epoch 67/200\n",
      "658/658 [==============================] - 0s - loss: 0.0080 - acc: 1.0000 - val_loss: 0.0758 - val_acc: 0.9848\n",
      "Epoch 68/200\n",
      "658/658 [==============================] - 0s - loss: 0.0101 - acc: 1.0000 - val_loss: 0.0736 - val_acc: 0.9788\n",
      "Epoch 69/200\n",
      "658/658 [==============================] - 0s - loss: 0.0077 - acc: 0.9985 - val_loss: 0.0791 - val_acc: 0.9758\n",
      "Epoch 70/200\n",
      "658/658 [==============================] - 0s - loss: 0.0094 - acc: 1.0000 - val_loss: 0.0890 - val_acc: 0.9697\n",
      "Epoch 71/200\n",
      "658/658 [==============================] - 0s - loss: 0.0102 - acc: 1.0000 - val_loss: 0.0697 - val_acc: 0.9788\n",
      "Epoch 72/200\n",
      "658/658 [==============================] - 0s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0755 - val_acc: 0.9758\n",
      "Epoch 73/200\n",
      "658/658 [==============================] - 0s - loss: 0.0063 - acc: 1.0000 - val_loss: 0.0743 - val_acc: 0.9758\n",
      "Epoch 74/200\n",
      "658/658 [==============================] - 0s - loss: 0.0130 - acc: 0.9985 - val_loss: 0.1077 - val_acc: 0.9576\n",
      "Epoch 75/200\n",
      "658/658 [==============================] - 0s - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0867 - val_acc: 0.9667\n",
      "Epoch 76/200\n",
      "658/658 [==============================] - 0s - loss: 0.0091 - acc: 0.9985 - val_loss: 0.0970 - val_acc: 0.9606\n",
      "Epoch 77/200\n",
      "658/658 [==============================] - 0s - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0841 - val_acc: 0.9788\n",
      "Epoch 78/200\n",
      "658/658 [==============================] - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0680 - val_acc: 0.9818\n",
      "Epoch 79/200\n",
      "658/658 [==============================] - 0s - loss: 0.0147 - acc: 0.9954 - val_loss: 0.0620 - val_acc: 0.9788\n",
      "Epoch 80/200\n",
      "658/658 [==============================] - 0s - loss: 0.0051 - acc: 0.9985 - val_loss: 0.1375 - val_acc: 0.9303\n",
      "Epoch 81/200\n",
      "658/658 [==============================] - 0s - loss: 0.0101 - acc: 1.0000 - val_loss: 0.1152 - val_acc: 0.9576\n",
      "Epoch 82/200\n",
      "658/658 [==============================] - 0s - loss: 0.0091 - acc: 1.0000 - val_loss: 0.0931 - val_acc: 0.9697\n",
      "Epoch 83/200\n",
      "658/658 [==============================] - 0s - loss: 0.0071 - acc: 1.0000 - val_loss: 0.0985 - val_acc: 0.9636\n",
      "Epoch 84/200\n",
      "658/658 [==============================] - 0s - loss: 0.0076 - acc: 0.9985 - val_loss: 0.1285 - val_acc: 0.9455\n",
      "Epoch 85/200\n",
      "658/658 [==============================] - 0s - loss: 0.0172 - acc: 0.9985 - val_loss: 0.0831 - val_acc: 0.9727\n",
      "Epoch 86/200\n",
      "658/658 [==============================] - 0s - loss: 0.0143 - acc: 0.9985 - val_loss: 0.0940 - val_acc: 0.9758\n",
      "Epoch 87/200\n",
      "658/658 [==============================] - 0s - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0969 - val_acc: 0.9606\n",
      "Epoch 88/200\n",
      "658/658 [==============================] - 0s - loss: 0.0123 - acc: 1.0000 - val_loss: 0.0974 - val_acc: 0.9636\n",
      "Epoch 89/200\n",
      "658/658 [==============================] - 0s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.1765 - val_acc: 0.9152\n",
      "Epoch 90/200\n",
      "658/658 [==============================] - 0s - loss: 0.0073 - acc: 1.0000 - val_loss: 0.1814 - val_acc: 0.9152\n",
      "Epoch 91/200\n",
      "658/658 [==============================] - 0s - loss: 0.0083 - acc: 1.0000 - val_loss: 0.1748 - val_acc: 0.9152\n",
      "Epoch 92/200\n",
      "658/658 [==============================] - 0s - loss: 0.0058 - acc: 0.9985 - val_loss: 0.1169 - val_acc: 0.9394\n",
      "Epoch 93/200\n",
      "658/658 [==============================] - 0s - loss: 0.0102 - acc: 0.9985 - val_loss: 0.0651 - val_acc: 0.9818\n",
      "Epoch 94/200\n",
      "658/658 [==============================] - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 0.0522 - val_acc: 0.9848\n",
      "Epoch 95/200\n",
      "658/658 [==============================] - 0s - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0905 - val_acc: 0.9667\n",
      "Epoch 96/200\n",
      "658/658 [==============================] - 0s - loss: 0.0054 - acc: 1.0000 - val_loss: 0.1275 - val_acc: 0.9455\n",
      "Epoch 97/200\n",
      "658/658 [==============================] - 0s - loss: 0.0147 - acc: 0.9985 - val_loss: 0.0397 - val_acc: 0.9939\n",
      "Epoch 98/200\n",
      "658/658 [==============================] - 0s - loss: 0.0080 - acc: 0.9970 - val_loss: 0.1323 - val_acc: 0.9333\n",
      "Epoch 99/200\n",
      "658/658 [==============================] - 0s - loss: 0.0111 - acc: 1.0000 - val_loss: 0.2879 - val_acc: 0.8848\n",
      "Epoch 100/200\n",
      "658/658 [==============================] - 0s - loss: 0.0087 - acc: 1.0000 - val_loss: 0.2520 - val_acc: 0.8939\n",
      "Epoch 101/200\n",
      "658/658 [==============================] - 0s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.1836 - val_acc: 0.9182\n",
      "Epoch 102/200\n",
      "658/658 [==============================] - 0s - loss: 0.0050 - acc: 0.9985 - val_loss: 0.1073 - val_acc: 0.9515\n",
      "Epoch 103/200\n",
      "658/658 [==============================] - 0s - loss: 0.0082 - acc: 0.9985 - val_loss: 0.0981 - val_acc: 0.9485\n",
      "Epoch 104/200\n",
      "658/658 [==============================] - 0s - loss: 0.0087 - acc: 1.0000 - val_loss: 0.0659 - val_acc: 0.9727\n",
      "Epoch 105/200\n",
      "658/658 [==============================] - 0s - loss: 0.0161 - acc: 0.9939 - val_loss: 0.0624 - val_acc: 0.9788\n",
      "Epoch 106/200\n",
      "658/658 [==============================] - 0s - loss: 0.0084 - acc: 1.0000 - val_loss: 0.1428 - val_acc: 0.9455\n",
      "Epoch 107/200\n",
      "658/658 [==============================] - 0s - loss: 0.0107 - acc: 1.0000 - val_loss: 0.1017 - val_acc: 0.9364\n",
      "Epoch 108/200\n",
      "658/658 [==============================] - 0s - loss: 0.0121 - acc: 0.9985 - val_loss: 0.0815 - val_acc: 0.9636\n",
      "Epoch 109/200\n",
      "658/658 [==============================] - 0s - loss: 0.0108 - acc: 1.0000 - val_loss: 0.1919 - val_acc: 0.9242\n",
      "Epoch 110/200\n",
      "658/658 [==============================] - 0s - loss: 0.0112 - acc: 1.0000 - val_loss: 0.1972 - val_acc: 0.9273\n",
      "Epoch 111/200\n",
      "658/658 [==============================] - 0s - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0778 - val_acc: 0.9606\n",
      "Epoch 112/200\n",
      "658/658 [==============================] - 0s - loss: 0.0062 - acc: 0.9985 - val_loss: 0.0528 - val_acc: 0.9788\n",
      "Epoch 113/200\n",
      "658/658 [==============================] - 0s - loss: 0.0051 - acc: 1.0000 - val_loss: 0.1012 - val_acc: 0.9545\n",
      "Epoch 114/200\n",
      "658/658 [==============================] - 0s - loss: 0.0039 - acc: 1.0000 - val_loss: 0.1832 - val_acc: 0.9364\n",
      "Epoch 115/200\n",
      "658/658 [==============================] - 0s - loss: 0.0118 - acc: 1.0000 - val_loss: 0.2075 - val_acc: 0.9273\n",
      "Epoch 116/200\n",
      "658/658 [==============================] - 0s - loss: 0.0067 - acc: 1.0000 - val_loss: 0.1918 - val_acc: 0.9273\n",
      "Epoch 117/200\n",
      "658/658 [==============================] - 0s - loss: 0.0083 - acc: 0.9985 - val_loss: 0.1663 - val_acc: 0.9303\n",
      "Epoch 118/200\n",
      "658/658 [==============================] - 0s - loss: 0.0059 - acc: 1.0000 - val_loss: 0.1159 - val_acc: 0.9394\n",
      "Epoch 119/200\n",
      "658/658 [==============================] - 0s - loss: 0.0072 - acc: 1.0000 - val_loss: 0.0879 - val_acc: 0.9545\n",
      "Epoch 120/200\n",
      "658/658 [==============================] - 0s - loss: 0.0086 - acc: 1.0000 - val_loss: 0.0858 - val_acc: 0.9576\n",
      "Epoch 121/200\n",
      "658/658 [==============================] - 0s - loss: 0.0104 - acc: 1.0000 - val_loss: 0.0864 - val_acc: 0.9606\n",
      "Epoch 122/200\n",
      "658/658 [==============================] - 0s - loss: 0.0042 - acc: 1.0000 - val_loss: 0.0890 - val_acc: 0.9606\n",
      "Epoch 123/200\n",
      "658/658 [==============================] - 0s - loss: 0.0034 - acc: 1.0000 - val_loss: 0.0904 - val_acc: 0.9606\n",
      "Epoch 124/200\n",
      "658/658 [==============================] - 0s - loss: 0.0040 - acc: 1.0000 - val_loss: 0.0977 - val_acc: 0.9485\n",
      "Epoch 125/200\n",
      "658/658 [==============================] - 0s - loss: 0.0117 - acc: 1.0000 - val_loss: 0.0997 - val_acc: 0.9485\n",
      "Epoch 126/200\n",
      "658/658 [==============================] - 0s - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0773 - val_acc: 0.9697\n",
      "Epoch 127/200\n",
      "658/658 [==============================] - 0s - loss: 0.0062 - acc: 0.9985 - val_loss: 0.0903 - val_acc: 0.9545\n",
      "Epoch 128/200\n",
      "658/658 [==============================] - 0s - loss: 0.0111 - acc: 1.0000 - val_loss: 0.1498 - val_acc: 0.9364\n",
      "Epoch 129/200\n",
      "658/658 [==============================] - 0s - loss: 0.0064 - acc: 1.0000 - val_loss: 0.1710 - val_acc: 0.9303\n",
      "Epoch 130/200\n",
      "658/658 [==============================] - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 0.0588 - val_acc: 0.9758\n",
      "Epoch 131/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/658 [==============================] - 0s - loss: 0.0071 - acc: 0.9985 - val_loss: 0.0183 - val_acc: 1.0000\n",
      "Epoch 132/200\n",
      "658/658 [==============================] - 0s - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0337 - val_acc: 0.9879\n",
      "Epoch 133/200\n",
      "658/658 [==============================] - 0s - loss: 0.0118 - acc: 0.9985 - val_loss: 0.0703 - val_acc: 0.9727\n",
      "Epoch 134/200\n",
      "658/658 [==============================] - 0s - loss: 0.0060 - acc: 1.0000 - val_loss: 0.0740 - val_acc: 0.9727\n",
      "Epoch 135/200\n",
      "658/658 [==============================] - 0s - loss: 0.0074 - acc: 1.0000 - val_loss: 0.0676 - val_acc: 0.9758\n",
      "Epoch 136/200\n",
      "658/658 [==============================] - 0s - loss: 0.0103 - acc: 1.0000 - val_loss: 0.0922 - val_acc: 0.9545\n",
      "Epoch 137/200\n",
      "658/658 [==============================] - 0s - loss: 0.0049 - acc: 1.0000 - val_loss: 0.1114 - val_acc: 0.9455\n",
      "Epoch 138/200\n",
      "658/658 [==============================] - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0776 - val_acc: 0.9697\n",
      "Epoch 139/200\n",
      "658/658 [==============================] - 0s - loss: 0.0068 - acc: 1.0000 - val_loss: 0.0701 - val_acc: 0.9758\n",
      "Epoch 140/200\n",
      "658/658 [==============================] - 0s - loss: 0.0073 - acc: 1.0000 - val_loss: 0.0782 - val_acc: 0.9727\n",
      "Epoch 141/200\n",
      "658/658 [==============================] - 0s - loss: 0.0037 - acc: 1.0000 - val_loss: 0.0849 - val_acc: 0.9697\n",
      "Epoch 142/200\n",
      "658/658 [==============================] - 0s - loss: 0.0083 - acc: 0.9985 - val_loss: 0.0641 - val_acc: 0.9727\n",
      "Epoch 143/200\n",
      "658/658 [==============================] - 0s - loss: 0.0071 - acc: 0.9985 - val_loss: 0.0472 - val_acc: 0.9818\n",
      "Epoch 144/200\n",
      "658/658 [==============================] - 0s - loss: 0.0059 - acc: 1.0000 - val_loss: 0.0478 - val_acc: 0.9818\n",
      "Epoch 145/200\n",
      "658/658 [==============================] - 0s - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0655 - val_acc: 0.9758\n",
      "Epoch 146/200\n",
      "658/658 [==============================] - 0s - loss: 0.0078 - acc: 1.0000 - val_loss: 0.0577 - val_acc: 0.9758\n",
      "Epoch 147/200\n",
      "658/658 [==============================] - 0s - loss: 0.0105 - acc: 1.0000 - val_loss: 0.0382 - val_acc: 0.9848\n",
      "Epoch 148/200\n",
      "658/658 [==============================] - 0s - loss: 0.0067 - acc: 0.9985 - val_loss: 0.0495 - val_acc: 0.9818\n",
      "Epoch 149/200\n",
      "658/658 [==============================] - 0s - loss: 0.0016 - acc: 1.0000 - val_loss: 0.0530 - val_acc: 0.9788\n",
      "Epoch 150/200\n",
      "658/658 [==============================] - 0s - loss: 0.0128 - acc: 1.0000 - val_loss: 0.0472 - val_acc: 0.9818\n",
      "Epoch 151/200\n",
      "658/658 [==============================] - 0s - loss: 0.0098 - acc: 1.0000 - val_loss: 0.0601 - val_acc: 0.9758\n",
      "Epoch 152/200\n",
      "658/658 [==============================] - 0s - loss: 0.0038 - acc: 1.0000 - val_loss: 0.0634 - val_acc: 0.9727\n",
      "Epoch 153/200\n",
      "658/658 [==============================] - 0s - loss: 0.0021 - acc: 1.0000 - val_loss: 0.0653 - val_acc: 0.9727\n",
      "Epoch 154/200\n",
      "658/658 [==============================] - 0s - loss: 0.0069 - acc: 1.0000 - val_loss: 0.0657 - val_acc: 0.9727\n",
      "Epoch 155/200\n",
      "658/658 [==============================] - 0s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.0649 - val_acc: 0.9727\n",
      "Epoch 156/200\n",
      "658/658 [==============================] - 0s - loss: 0.0085 - acc: 1.0000 - val_loss: 0.0875 - val_acc: 0.9697\n",
      "Epoch 157/200\n",
      "658/658 [==============================] - 0s - loss: 0.0109 - acc: 1.0000 - val_loss: 0.1002 - val_acc: 0.9606\n",
      "Epoch 158/200\n",
      "658/658 [==============================] - 0s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.0866 - val_acc: 0.9697\n",
      "Epoch 159/200\n",
      "658/658 [==============================] - 0s - loss: 0.0122 - acc: 1.0000 - val_loss: 0.1295 - val_acc: 0.9424\n",
      "Epoch 160/200\n",
      "658/658 [==============================] - 0s - loss: 0.0071 - acc: 1.0000 - val_loss: 0.1526 - val_acc: 0.9364\n",
      "Epoch 161/200\n",
      "658/658 [==============================] - 0s - loss: 0.0093 - acc: 1.0000 - val_loss: 0.1217 - val_acc: 0.9424\n",
      "Epoch 162/200\n",
      "658/658 [==============================] - 0s - loss: 0.0037 - acc: 1.0000 - val_loss: 0.1029 - val_acc: 0.9485\n",
      "Epoch 163/200\n",
      "658/658 [==============================] - 0s - loss: 0.0061 - acc: 1.0000 - val_loss: 0.0913 - val_acc: 0.9667\n",
      "Epoch 164/200\n",
      "658/658 [==============================] - 0s - loss: 0.0077 - acc: 1.0000 - val_loss: 0.1063 - val_acc: 0.9485\n",
      "Epoch 165/200\n",
      "658/658 [==============================] - 0s - loss: 0.0066 - acc: 1.0000 - val_loss: 0.1646 - val_acc: 0.9273\n",
      "Epoch 166/200\n",
      "658/658 [==============================] - 0s - loss: 0.0036 - acc: 1.0000 - val_loss: 0.1845 - val_acc: 0.9242\n",
      "Epoch 167/200\n",
      "658/658 [==============================] - 0s - loss: 0.0035 - acc: 1.0000 - val_loss: 0.1853 - val_acc: 0.9242\n",
      "Epoch 168/200\n",
      "658/658 [==============================] - 0s - loss: 0.0066 - acc: 1.0000 - val_loss: 0.1784 - val_acc: 0.9242\n",
      "Epoch 169/200\n",
      "658/658 [==============================] - 0s - loss: 0.0057 - acc: 1.0000 - val_loss: 0.1038 - val_acc: 0.9515\n",
      "Epoch 170/200\n",
      "658/658 [==============================] - 0s - loss: 0.0045 - acc: 1.0000 - val_loss: 0.0664 - val_acc: 0.9758\n",
      "Epoch 171/200\n",
      "658/658 [==============================] - 0s - loss: 0.0064 - acc: 1.0000 - val_loss: 0.0708 - val_acc: 0.9758\n",
      "Epoch 172/200\n",
      "658/658 [==============================] - 0s - loss: 0.0090 - acc: 1.0000 - val_loss: 0.0918 - val_acc: 0.9636\n",
      "Epoch 173/200\n",
      "658/658 [==============================] - 0s - loss: 0.0067 - acc: 1.0000 - val_loss: 0.1343 - val_acc: 0.9424\n",
      "Epoch 174/200\n",
      "658/658 [==============================] - 0s - loss: 0.0044 - acc: 1.0000 - val_loss: 0.1544 - val_acc: 0.9303\n",
      "Epoch 175/200\n",
      "658/658 [==============================] - 0s - loss: 0.0067 - acc: 1.0000 - val_loss: 0.1209 - val_acc: 0.9515\n",
      "Epoch 176/200\n",
      "658/658 [==============================] - 0s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.1236 - val_acc: 0.9485\n",
      "Epoch 177/200\n",
      "658/658 [==============================] - 0s - loss: 0.0073 - acc: 1.0000 - val_loss: 0.1358 - val_acc: 0.9394\n",
      "Epoch 178/200\n",
      "658/658 [==============================] - 0s - loss: 0.0059 - acc: 1.0000 - val_loss: 0.1305 - val_acc: 0.9424\n",
      "Epoch 179/200\n",
      "658/658 [==============================] - 0s - loss: 0.0090 - acc: 1.0000 - val_loss: 0.1051 - val_acc: 0.9394\n",
      "Epoch 180/200\n",
      "658/658 [==============================] - 0s - loss: 0.0084 - acc: 0.9970 - val_loss: 0.0778 - val_acc: 0.9667\n",
      "Epoch 181/200\n",
      "658/658 [==============================] - 0s - loss: 0.0064 - acc: 1.0000 - val_loss: 0.2359 - val_acc: 0.9061\n",
      "Epoch 182/200\n",
      "658/658 [==============================] - 0s - loss: 0.0071 - acc: 1.0000 - val_loss: 0.2901 - val_acc: 0.8848\n",
      "Epoch 183/200\n",
      "658/658 [==============================] - 0s - loss: 0.0062 - acc: 1.0000 - val_loss: 0.2475 - val_acc: 0.9030\n",
      "Epoch 184/200\n",
      "658/658 [==============================] - 0s - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2011 - val_acc: 0.9242\n",
      "Epoch 185/200\n",
      "658/658 [==============================] - 0s - loss: 0.0037 - acc: 1.0000 - val_loss: 0.1832 - val_acc: 0.9273\n",
      "Epoch 186/200\n",
      "658/658 [==============================] - 0s - loss: 0.0062 - acc: 1.0000 - val_loss: 0.1967 - val_acc: 0.9212\n",
      "Epoch 187/200\n",
      "658/658 [==============================] - 0s - loss: 0.0027 - acc: 1.0000 - val_loss: 0.1599 - val_acc: 0.9273\n",
      "Epoch 188/200\n",
      "658/658 [==============================] - 0s - loss: 0.0057 - acc: 1.0000 - val_loss: 0.1204 - val_acc: 0.9394\n",
      "Epoch 189/200\n",
      "658/658 [==============================] - 0s - loss: 0.0037 - acc: 1.0000 - val_loss: 0.1025 - val_acc: 0.9576\n",
      "Epoch 190/200\n",
      "658/658 [==============================] - 0s - loss: 0.0056 - acc: 0.9985 - val_loss: 0.1333 - val_acc: 0.9424\n",
      "Epoch 191/200\n",
      "658/658 [==============================] - 0s - loss: 0.0089 - acc: 1.0000 - val_loss: 0.1648 - val_acc: 0.9364\n",
      "Epoch 192/200\n",
      "658/658 [==============================] - 0s - loss: 0.0065 - acc: 1.0000 - val_loss: 0.1985 - val_acc: 0.9333\n",
      "Epoch 193/200\n",
      "658/658 [==============================] - 0s - loss: 0.0066 - acc: 1.0000 - val_loss: 0.2092 - val_acc: 0.9333\n",
      "Epoch 194/200\n",
      "658/658 [==============================] - 0s - loss: 0.0056 - acc: 1.0000 - val_loss: 0.2047 - val_acc: 0.9333\n",
      "Epoch 195/200\n",
      "658/658 [==============================] - 0s - loss: 0.0092 - acc: 1.0000 - val_loss: 0.1848 - val_acc: 0.9364\n",
      "Epoch 196/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "658/658 [==============================] - 0s - loss: 0.0080 - acc: 1.0000 - val_loss: 0.1720 - val_acc: 0.9394\n",
      "Epoch 197/200\n",
      "658/658 [==============================] - 0s - loss: 0.0079 - acc: 1.0000 - val_loss: 0.1941 - val_acc: 0.9303\n",
      "Epoch 198/200\n",
      "658/658 [==============================] - 0s - loss: 0.0052 - acc: 1.0000 - val_loss: 0.1065 - val_acc: 0.9485\n",
      "Epoch 199/200\n",
      "658/658 [==============================] - 0s - loss: 0.0058 - acc: 1.0000 - val_loss: 0.0244 - val_acc: 0.9939\n",
      "Epoch 200/200\n",
      "658/658 [==============================] - 0s - loss: 0.0035 - acc: 0.9985 - val_loss: 0.0265 - val_acc: 0.9939\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=128, epochs=200, validation_data=(X_val, Y_val)) #callbacks=[early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.70719121e-15,   1.00000000e+00],\n",
       "       [  5.68544777e-14,   1.00000000e+00],\n",
       "       [  1.99857548e-18,   1.00000000e+00],\n",
       "       [  1.62834822e-05,   9.99983668e-01],\n",
       "       [  2.65297940e-11,   1.00000000e+00],\n",
       "       [  3.26242852e-12,   1.00000000e+00],\n",
       "       [  4.11501969e-06,   9.99995828e-01],\n",
       "       [  1.92551225e-10,   1.00000000e+00],\n",
       "       [  6.02835673e-04,   9.99397159e-01],\n",
       "       [  2.27131158e-09,   1.00000000e+00],\n",
       "       [  4.50570951e-05,   9.99954939e-01],\n",
       "       [  5.96788668e-05,   9.99940276e-01],\n",
       "       [  5.31585003e-14,   1.00000000e+00],\n",
       "       [  6.96933782e-08,   9.99999881e-01],\n",
       "       [  1.58653242e-13,   1.00000000e+00],\n",
       "       [  1.53077719e-26,   1.00000000e+00],\n",
       "       [  2.25973595e-02,   9.77402627e-01],\n",
       "       [  4.77840245e-01,   5.22159815e-01],\n",
       "       [  2.87815639e-11,   1.00000000e+00],\n",
       "       [  1.31480746e-28,   1.00000000e+00],\n",
       "       [  1.57597730e-12,   1.00000000e+00],\n",
       "       [  1.88092380e-12,   1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_cyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activ = model2.predict(X_train)\n",
    "#activ = model2.predict(np.expand_dims(X_train[10],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c8e3907550>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvB8ZGkIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c8e38b92b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(activ[20][:,:,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,  121.71251678,   53.23372269,    0.        ,\n",
       "          0.        ,   24.43340492,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    7.88301706,    0.        ], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activ[0][5,5,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['12296182_23.npy', '13028374_2.npy', 'E104657225_14.npy',\n",
       "       '12972894_6.npy'],\n",
       "      dtype='<U17')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_val[::30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47784024,  0.52215981], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.99998% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.99858% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.99996% confidence)', 'cyst (99.99999% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.60698% confidence)', 'hcc (70.35562% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)']\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "#print(\"Ground truth:\", [cls_mapping[max(enumerate(x), key=operator.itemgetter(1))[0]] for x in Y_val[::30]])\n",
    "Y_ = model.predict(X_cyst)\n",
    "print(\"Predictions:\", [cls_mapping[max(enumerate(x), key=operator.itemgetter(1))[0]] + \" (%.5f%% confidence)\" % (max(x)*100) for x in Y_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x266df7b4da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "voi_df = pd.read_csv('..\\\\liver-mr-processor\\\\vois.csv')\n",
    "img_fn = \"12972894.npy\"\n",
    "img = np.load(\"..\\\\liver-mr-processor\\\\full_imgs\\\\\"+img_fn)\n",
    "plot_section(img, voi_df[voi_df[\"Filename\"] == img_fn].iloc[0], pad=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_section(img, df, pad=30):\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(np.transpose(img[df['x1']-pad:df['x2']+pad,\n",
    "                                df['y2']+pad:df['y1']-pad:-1,\n",
    "                                (df['z1']+df['z2'])//2, 0], (1,0)), cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(np.transpose(img[df['x1']-pad:df['x2']+pad,\n",
    "                                df['y2']+pad:df['y1']-pad:-1,\n",
    "                                (df['z1']+df['z2'])//2, 1], (1,0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.78843247e-04,   9.99721110e-01], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sum(y)/len(y), 1-sum(y)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20054101943969727\n"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "Y_ = model.predict(X_val)\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(X[650,:,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20b42b02b70>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20b424cdcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0,:,:,5,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
