{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input, Add, Dense, Activation, Flatten, Dropout, Conv3D, MaxPooling3D, ZeroPadding3D, AveragePooling3D, TimeDistributed\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l1, l2\n",
    "from keras.optimizers import Adam, SGD\n",
    "from keras.constraints import max_norm\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "import operator\n",
    "import matplotlib.pyplot as plt\n",
    "import config\n",
    "import importlib\n",
    "import os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "nb_classes = C.nb_classes\n",
    "\n",
    "X_train = np.expand_dims(x_train,axis=3)\n",
    "X_train = np.expand_dims(X_train,axis=4)\n",
    "X_test = np.expand_dims(x_test,axis=3)\n",
    "X_test = np.expand_dims(X_test,axis=4)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'config' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\config.py'>"
      ]
     },
     "execution_count": 324,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C = config.Config()\n",
    "dims = C.dims\n",
    "voi_img = Input(shape=(dims[0], dims[1], dims[2], C.nb_channels))\n",
    "x = voi_img\n",
    "#x = Dropout(0.2)(x)\n",
    "#x = GaussianNoise(1)(x)\n",
    "#x = ZeroPadding3D(padding=(3,3,2))(voi_img)\n",
    "#x = Conv3D(filters=128, kernel_size=(3,3,2), activation='relu')(x)\n",
    "x = Conv3D(filters=32, kernel_size=(3,3,1), activation='relu')(x)\n",
    "#x = Dropout(0.5)(x)\n",
    "x = MaxPooling3D((2, 2, 2))(x)\n",
    "x = Conv3D(filters=64, kernel_size=(3,3,2), activation='relu')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "#x = Conv3D(filters=128, kernel_size=(2,2,2), activation='relu')(x)\n",
    "x = MaxPooling3D((2, 2, 2))(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(164, activation='relu')(x)#, kernel_initializer='normal', kernel_regularizer=l1(.01), kernel_constraint=max_norm(3.))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "pred_class = Dense(C.nb_classes, activation='softmax')(x)#Dense(C.nb_classes, activation='softmax')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_29 (InputLayer)        (None, 24, 24, 12, 1)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_58 (Conv3D)           (None, 22, 22, 12, 32)    320       \n",
      "_________________________________________________________________\n",
      "max_pooling3d_40 (MaxPooling (None, 11, 11, 6, 32)     0         \n",
      "_________________________________________________________________\n",
      "conv3d_59 (Conv3D)           (None, 9, 9, 5, 64)       36928     \n",
      "_________________________________________________________________\n",
      "dropout_73 (Dropout)         (None, 9, 9, 5, 64)       0         \n",
      "_________________________________________________________________\n",
      "max_pooling3d_41 (MaxPooling (None, 4, 4, 2, 64)       0         \n",
      "_________________________________________________________________\n",
      "flatten_29 (Flatten)         (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 164)               336036    \n",
      "_________________________________________________________________\n",
      "dropout_74 (Dropout)         (None, 164)               0         \n",
      "_________________________________________________________________\n",
      "dense_58 (Dense)             (None, 4)                 660       \n",
      "=================================================================\n",
      "Total params: 373,944\n",
      "Trainable params: 373,944\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optim = Adam(lr=0.001)#, decay=0.001)\n",
    "early_stopping = EarlyStopping(min_delta=0.001, patience=10)\n",
    "\n",
    "model = Model(voi_img, pred_class)\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "model2 = Model(voi_img, intermed)\n",
    "\n",
    "for l in range(2,len(model2.layers)):\n",
    "    model2.layers[l].set_weights(model.layers[l].get_weights())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "\n",
    "for _ in range(10):\n",
    "    X_train.append(np.ones(dims + [C.nb_channels]))\n",
    "    Y_train.append([1,0])\n",
    "for _ in range(10):\n",
    "    X_train.append(np.ones(dims + [C.nb_channels]))\n",
    "    X_train[-1][5:15,5:15,5:7,0] = 2\n",
    "    Y_train.append([0,1])\n",
    "    \n",
    "X_train = np.array(X_train) # X[:total_size//2]\n",
    "#X_val = np.array(X_test)\n",
    "Y_train = np.array(Y_train) # Y[:total_size//2]\n",
    "#Y_val = np.array(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '..\\\\liver-mr-processor\\\\aug_imgs\\\\'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-327-9e8c60f25647>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mnum_samples\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mclass_name\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maug_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m10000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdims\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnb_channels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mz\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: '..\\\\liver-mr-processor\\\\aug_imgs\\\\'"
     ]
    }
   ],
   "source": [
    "aug_data_dict = {}\n",
    "orig_data_dict = {}\n",
    "num_samples = {}\n",
    "\n",
    "for class_name in os.listdir(C.aug_dir):\n",
    "    x = np.empty((10000, dims[0], dims[1], dims[2], C.nb_channels))\n",
    "    z = []\n",
    "    \n",
    "    for index, img_fn in enumerate(os.listdir(C.aug_dir+class_name)):\n",
    "        try:\n",
    "            x[index] = np.load(C.aug_dir+class_name+\"\\\\\"+img_fn)\n",
    "            z.append(img_fn)\n",
    "        except Exception as e:\n",
    "            print(class_name+\"\\\\\"+img_fn, e)\n",
    "    \n",
    "    #x.resize((index, dims[0], dims[1], dims[2], C.nb_channels))\n",
    "    aug_data_dict[class_name] = [x,np.array(z)]\n",
    "\n",
    "    \n",
    "for class_name in os.listdir(C.orig_dir):\n",
    "    x = np.empty((10000, dims[0], dims[1], dims[2], C.nb_channels))\n",
    "    z = []\n",
    "    \n",
    "    for index, img_fn in enumerate(os.listdir(C.orig_dir+class_name)):\n",
    "        x[index] = np.load(C.orig_dir+class_name+\"\\\\\"+img_fn)\n",
    "        z.append(img_fn)\n",
    "    \n",
    "    #x.resize((index, dims[0], dims[1], dims[2], C.nb_channels))\n",
    "    orig_data_dict[class_name] = [x,np.array(z)]\n",
    "    num_samples[class_name] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'orig_data_dict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-322-d35c838b1544>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m     \u001b[0morder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m     \u001b[0mtrain_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_data_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m     \u001b[0mtest_ids\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_data_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_test\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_data_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_samples\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'orig_data_dict' is not defined"
     ]
    }
   ],
   "source": [
    "train_ids = {} #filenames of training set originals\n",
    "test_ids = {} #filenames of test set\n",
    "X_test = []\n",
    "Y_test = []\n",
    "Z_test = []\n",
    "\n",
    "train_frac = {\"cyst\": 0.7, \"fnh\": 0.7, \"hcc\": 0.7, \"hemangioma\": 0.7}\n",
    "train_samples = {}\n",
    "\n",
    "for cls_num, cls in enumerate(data_dict):\n",
    "    train_samples[cls] = round(num_samples[cls]*train_frac[cls])\n",
    "    \n",
    "    order = np.random.permutation(list(range(num_samples[cls])))\n",
    "    train_ids[cls] = list(orig_data_dict[cls][1][order[:train_samples[cls]]])\n",
    "    test_ids[cls] = list(orig_data_dict[cls][1][order[train_samples[cls]:]])\n",
    "    X_test = X_test + list(orig_data_dict[cls][0][order[train_samples[cls]:]])\n",
    "    Y_test = Y_test + [[0] * cls_num + [1] + [0] * (C.nb_classes - cls_num - 1)] * \\\n",
    "                        (max_samples[cls] - train_samples[cls])\n",
    "    Z_test = Z_test + list(orig_data_dict[cls][1][order[train_samples[cls]:]])\n",
    "    cls_mapping.append(cls)\n",
    "    \n",
    "    print(\"%s has %d samples for training (%d after augmentation) and %d for testing\" %\n",
    "          (cls, train_samples[cls], train_samples[cls] * C.aug_factor, num_samples[cls] - train_samples[cls]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hcc has 279 samples for training and 120 for testing\n",
      "hemangioma has 531 samples for training and 228 for testing\n",
      "fnh has 825 samples for training and 354 for testing\n",
      "cyst has 643 samples for training and 276 for testing\n"
     ]
    }
   ],
   "source": [
    "X_train = []\n",
    "Y_train = []\n",
    "Z_train = []\n",
    "X_val = []\n",
    "Y_val = []\n",
    "Z_val = []\n",
    "cls_mapping = []\n",
    "\n",
    "for cls_num, cls in enumerate(data_dict):\n",
    "    order = np.random.permutation(list(range(num_samples[cls])))\n",
    "    #order = list(range(max_samples[cls]))\n",
    "    X_train = X_train + list(data_dict[cls][0][order[:round(num_samples[cls]*train_frac[cls])]])\n",
    "    Z_train = Z_train + list(data_dict[cls][1][order[:round(num_samples[cls]*train_frac[cls])]])\n",
    "    Y_train = Y_train + [[0] * cls_num + [1] + [0] * (C.nb_classes - cls_num - 1)] * (round(num_samples[cls]*train_frac[cls]))\n",
    "    \n",
    "    X_val = X_val + list(data_dict[cls][0][order[round(num_samples[cls]*train_frac[cls]):]])\n",
    "    Y_val = Y_val + [[0] * cls_num + [1] + [0] * (C.nb_classes - cls_num - 1)] * \\\n",
    "                        (max_samples[cls] - round(num_samples[cls]*train_frac[cls]))\n",
    "    Z_val = Z_val + list(data_dict[cls][1][order[round(num_samples[cls]*train_frac[cls]):]])\n",
    "        \n",
    "    cls_mapping.append(cls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Z_cyst = data_dict[\"cyst\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_[-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Z_cyst[-4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "X_cyst = data_dict[\"cyst\"][0]\n",
    "X_cyst = np.array(X_cyst) # X[:total_size//2]\n",
    "X_cyst /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "X_hcc = data_dict[\"hcc\"][0]\n",
    "X_hcc = np.array(X_hcc) # X[:total_size//2]\n",
    "X_hcc /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = np.array(X_train) # X[:total_size//2]\n",
    "X_train /= 255\n",
    "X_val = np.array(X_test)\n",
    "X_val /= 255\n",
    "#X_val = X[total_size//2:total_size*3//4]\n",
    "#X_test = X[total_size*3//4:]\n",
    "Y_train = np.array(Y_train)\n",
    "Y_val = np.array(Y_test)\n",
    "Z_train = np.array(Z_train)\n",
    "Z_val = np.array(Z_test)\n",
    "#Y_val = Y[total_size//2:total_size*3//4]\n",
    "#Y_test = Y[total_size*3//4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "56.95"
      ]
     },
     "execution_count": 312,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Z_train)/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.45"
      ]
     },
     "execution_count": 313,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Z_val)/40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X = np.array([z for z in x])\n",
    "#Y = np.array(y)\n",
    "#Y = np.array([[0,1] if y[x] == 1 else [1,0] for x in range(len(y))])\n",
    "#Y = K.constant(y, dtype=tf.int32)\n",
    "#Y = K.one_hot(Y, C.nb_classes)\n",
    "\n",
    "#total_size = X.shape[0]\n",
    "\n",
    "#order = np.random.permutation(list(range(total_size)))\n",
    "#X = X[order]\n",
    "#Y = Y[order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def train_generator(X, Y):\n",
    "    while True:\n",
    "        for i in range(len(X)):\n",
    "            yield np.expand_dims(X[i], axis=0), np.expand_dims(Y[i], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "num_epochs = 10\n",
    "epoch_length = 100\n",
    "best_loss = np.Inf\n",
    "losses = np.zeros(epoch_length)\n",
    "acc = np.zeros(epoch_length)\n",
    "\n",
    "data_gen_train = train_generator(X_train, Y_train)\n",
    "for epoch_num in range(num_epochs):\n",
    "    print('Epoch {}/{}'.format(epoch_num + 1, num_epochs))\n",
    "    iter_num = 0\n",
    "    \n",
    "    while True:\n",
    "        X, Y = next(data_gen_train)\n",
    "        losses[iter_num], acc[iter_num] = model.train_on_batch(X, Y)\n",
    "\n",
    "        iter_num += 1\n",
    "        if iter_num == epoch_length:\n",
    "            curr_loss = np.mean(losses)\n",
    "            curr_acc = np.mean(acc)\n",
    "            print(\"Mean Loss:\", curr_loss, \"// Mean Accuracy:\", curr_acc)\n",
    "\n",
    "            if curr_loss < best_loss:\n",
    "                print('Total loss decreased from {} to {}, saving weights'.format(best_loss,curr_loss))\n",
    "                best_loss = curr_loss\n",
    "                model.save_weights(C.model_path)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2278 samples, validate on 978 samples\n",
      "Epoch 1/200\n",
      "2278/2278 [==============================] - 1s - loss: 1.3323 - acc: 0.3327 - val_loss: 1.3099 - val_acc: 0.3620\n",
      "Epoch 2/200\n",
      "2278/2278 [==============================] - 0s - loss: 1.2747 - acc: 0.3885 - val_loss: 1.2449 - val_acc: 0.4233\n",
      "Epoch 3/200\n",
      "2278/2278 [==============================] - 0s - loss: 1.1959 - acc: 0.4333 - val_loss: 1.2048 - val_acc: 0.4980\n",
      "Epoch 4/200\n",
      "2278/2278 [==============================] - 0s - loss: 1.1168 - acc: 0.4776 - val_loss: 1.1463 - val_acc: 0.5409\n",
      "Epoch 5/200\n",
      "2278/2278 [==============================] - 0s - loss: 1.0309 - acc: 0.5356 - val_loss: 1.0497 - val_acc: 0.5460\n",
      "Epoch 6/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.9796 - acc: 0.5505 - val_loss: 1.0204 - val_acc: 0.5593\n",
      "Epoch 7/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.9202 - acc: 0.5948 - val_loss: 0.9743 - val_acc: 0.5869\n",
      "Epoch 8/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.8723 - acc: 0.6277 - val_loss: 0.9555 - val_acc: 0.6462\n",
      "Epoch 9/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.8278 - acc: 0.6488 - val_loss: 0.9202 - val_acc: 0.6697\n",
      "Epoch 10/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.7901 - acc: 0.6659 - val_loss: 0.8697 - val_acc: 0.6656\n",
      "Epoch 11/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.7639 - acc: 0.6809 - val_loss: 0.8453 - val_acc: 0.6677\n",
      "Epoch 12/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.7464 - acc: 0.6883 - val_loss: 0.8271 - val_acc: 0.6902\n",
      "Epoch 13/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.7077 - acc: 0.7019 - val_loss: 0.7886 - val_acc: 0.7025\n",
      "Epoch 14/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.6606 - acc: 0.7353 - val_loss: 0.7950 - val_acc: 0.6912\n",
      "Epoch 15/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.6696 - acc: 0.7204 - val_loss: 0.7905 - val_acc: 0.6861\n",
      "Epoch 16/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.6525 - acc: 0.7318 - val_loss: 0.7787 - val_acc: 0.7321\n",
      "Epoch 17/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.6129 - acc: 0.7419 - val_loss: 0.7162 - val_acc: 0.7372\n",
      "Epoch 18/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.5612 - acc: 0.7730 - val_loss: 0.7113 - val_acc: 0.7168\n",
      "Epoch 19/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.5668 - acc: 0.7651 - val_loss: 0.7095 - val_acc: 0.7485\n",
      "Epoch 20/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.5149 - acc: 0.7946 - val_loss: 0.6598 - val_acc: 0.7434\n",
      "Epoch 21/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.5390 - acc: 0.7717 - val_loss: 0.6756 - val_acc: 0.7536\n",
      "Epoch 22/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.4884 - acc: 0.7985 - val_loss: 0.6332 - val_acc: 0.7740\n",
      "Epoch 23/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.4814 - acc: 0.8090 - val_loss: 0.6338 - val_acc: 0.7679\n",
      "Epoch 24/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.4717 - acc: 0.8095 - val_loss: 0.6298 - val_acc: 0.7710\n",
      "Epoch 25/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.4520 - acc: 0.8191 - val_loss: 0.6138 - val_acc: 0.7873\n",
      "Epoch 26/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.4324 - acc: 0.8288 - val_loss: 0.5812 - val_acc: 0.7924\n",
      "Epoch 27/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.3973 - acc: 0.8385 - val_loss: 0.5686 - val_acc: 0.7843\n",
      "Epoch 28/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.3855 - acc: 0.8459 - val_loss: 0.5527 - val_acc: 0.7996\n",
      "Epoch 29/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.3591 - acc: 0.8617 - val_loss: 0.5400 - val_acc: 0.8139\n",
      "Epoch 30/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.3751 - acc: 0.8529 - val_loss: 0.5847 - val_acc: 0.7802\n",
      "Epoch 31/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.3697 - acc: 0.8573 - val_loss: 0.5217 - val_acc: 0.7914\n",
      "Epoch 32/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.3463 - acc: 0.8644 - val_loss: 0.5179 - val_acc: 0.8027\n",
      "Epoch 33/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.3279 - acc: 0.8714 - val_loss: 0.4983 - val_acc: 0.8262\n",
      "Epoch 34/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.3016 - acc: 0.8775 - val_loss: 0.4980 - val_acc: 0.8149\n",
      "Epoch 35/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2804 - acc: 0.8907 - val_loss: 0.4853 - val_acc: 0.8119\n",
      "Epoch 36/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2796 - acc: 0.8885 - val_loss: 0.4808 - val_acc: 0.8262\n",
      "Epoch 37/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2727 - acc: 0.9034 - val_loss: 0.4704 - val_acc: 0.8221\n",
      "Epoch 38/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2665 - acc: 0.8990 - val_loss: 0.4643 - val_acc: 0.8211\n",
      "Epoch 39/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2380 - acc: 0.9096 - val_loss: 0.4481 - val_acc: 0.8252\n",
      "Epoch 40/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2330 - acc: 0.9144 - val_loss: 0.4501 - val_acc: 0.8282\n",
      "Epoch 41/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2435 - acc: 0.9126 - val_loss: 0.4385 - val_acc: 0.8425\n",
      "Epoch 42/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2384 - acc: 0.9166 - val_loss: 0.4630 - val_acc: 0.8292\n",
      "Epoch 43/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2219 - acc: 0.9249 - val_loss: 0.4472 - val_acc: 0.8354\n",
      "Epoch 44/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2322 - acc: 0.9083 - val_loss: 0.4269 - val_acc: 0.8558\n",
      "Epoch 45/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1983 - acc: 0.9267 - val_loss: 0.4508 - val_acc: 0.8415\n",
      "Epoch 46/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.2108 - acc: 0.9205 - val_loss: 0.4190 - val_acc: 0.8487\n",
      "Epoch 47/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1959 - acc: 0.9289 - val_loss: 0.4178 - val_acc: 0.8589\n",
      "Epoch 48/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1825 - acc: 0.9359 - val_loss: 0.3945 - val_acc: 0.8558\n",
      "Epoch 49/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1769 - acc: 0.9429 - val_loss: 0.4076 - val_acc: 0.8425\n",
      "Epoch 50/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1782 - acc: 0.9363 - val_loss: 0.3927 - val_acc: 0.8579\n",
      "Epoch 51/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1560 - acc: 0.9500 - val_loss: 0.3870 - val_acc: 0.8701\n",
      "Epoch 52/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1679 - acc: 0.9385 - val_loss: 0.3712 - val_acc: 0.8630\n",
      "Epoch 53/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1661 - acc: 0.9399 - val_loss: 0.3933 - val_acc: 0.8609\n",
      "Epoch 54/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1483 - acc: 0.9539 - val_loss: 0.3683 - val_acc: 0.8722\n",
      "Epoch 55/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1674 - acc: 0.9478 - val_loss: 0.3729 - val_acc: 0.8691\n",
      "Epoch 56/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1444 - acc: 0.9543 - val_loss: 0.3603 - val_acc: 0.8691\n",
      "Epoch 57/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1333 - acc: 0.9557 - val_loss: 0.3535 - val_acc: 0.8804\n",
      "Epoch 58/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1369 - acc: 0.9557 - val_loss: 0.3578 - val_acc: 0.8896\n",
      "Epoch 59/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1412 - acc: 0.9500 - val_loss: 0.3719 - val_acc: 0.8681\n",
      "Epoch 60/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1333 - acc: 0.9543 - val_loss: 0.3534 - val_acc: 0.8926\n",
      "Epoch 61/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1274 - acc: 0.9557 - val_loss: 0.3832 - val_acc: 0.8599\n",
      "Epoch 62/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1324 - acc: 0.9587 - val_loss: 0.3703 - val_acc: 0.8640\n",
      "Epoch 63/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1135 - acc: 0.9644 - val_loss: 0.3389 - val_acc: 0.8896\n",
      "Epoch 64/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1009 - acc: 0.9693 - val_loss: 0.3403 - val_acc: 0.8814\n",
      "Epoch 65/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - 0s - loss: 0.0951 - acc: 0.9706 - val_loss: 0.3371 - val_acc: 0.8783\n",
      "Epoch 66/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0969 - acc: 0.9715 - val_loss: 0.3295 - val_acc: 0.8916\n",
      "Epoch 67/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0990 - acc: 0.9715 - val_loss: 0.3362 - val_acc: 0.8824\n",
      "Epoch 68/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0931 - acc: 0.9666 - val_loss: 0.3280 - val_acc: 0.8793\n",
      "Epoch 69/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0958 - acc: 0.9693 - val_loss: 0.3448 - val_acc: 0.8824\n",
      "Epoch 70/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.1076 - acc: 0.9622 - val_loss: 0.3355 - val_acc: 0.8824\n",
      "Epoch 71/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0958 - acc: 0.9697 - val_loss: 0.3442 - val_acc: 0.8804\n",
      "Epoch 72/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0997 - acc: 0.9693 - val_loss: 0.3309 - val_acc: 0.8824\n",
      "Epoch 73/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0913 - acc: 0.9697 - val_loss: 0.3356 - val_acc: 0.8865\n",
      "Epoch 74/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0899 - acc: 0.9715 - val_loss: 0.3219 - val_acc: 0.8916\n",
      "Epoch 75/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0775 - acc: 0.9776 - val_loss: 0.3304 - val_acc: 0.8988\n",
      "Epoch 76/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0824 - acc: 0.9728 - val_loss: 0.3143 - val_acc: 0.8937\n",
      "Epoch 77/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0805 - acc: 0.9754 - val_loss: 0.3124 - val_acc: 0.8978\n",
      "Epoch 78/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0695 - acc: 0.9811 - val_loss: 0.3151 - val_acc: 0.8957\n",
      "Epoch 79/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0662 - acc: 0.9798 - val_loss: 0.3279 - val_acc: 0.8783\n",
      "Epoch 80/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0825 - acc: 0.9737 - val_loss: 0.3118 - val_acc: 0.8988\n",
      "Epoch 81/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0941 - acc: 0.9701 - val_loss: 0.3246 - val_acc: 0.8855\n",
      "Epoch 82/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0777 - acc: 0.9789 - val_loss: 0.3068 - val_acc: 0.9008\n",
      "Epoch 83/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0621 - acc: 0.9833 - val_loss: 0.3017 - val_acc: 0.8998\n",
      "Epoch 84/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0776 - acc: 0.9763 - val_loss: 0.3202 - val_acc: 0.8957\n",
      "Epoch 85/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0604 - acc: 0.9824 - val_loss: 0.3084 - val_acc: 0.8988\n",
      "Epoch 86/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0653 - acc: 0.9816 - val_loss: 0.3058 - val_acc: 0.8926\n",
      "Epoch 87/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0655 - acc: 0.9838 - val_loss: 0.2913 - val_acc: 0.9049\n",
      "Epoch 88/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0689 - acc: 0.9820 - val_loss: 0.2999 - val_acc: 0.9018\n",
      "Epoch 89/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0650 - acc: 0.9807 - val_loss: 0.3133 - val_acc: 0.8988\n",
      "Epoch 90/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0713 - acc: 0.9767 - val_loss: 0.2956 - val_acc: 0.9070\n",
      "Epoch 91/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0582 - acc: 0.9789 - val_loss: 0.3408 - val_acc: 0.8896\n",
      "Epoch 92/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0538 - acc: 0.9868 - val_loss: 0.3320 - val_acc: 0.8906\n",
      "Epoch 93/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0510 - acc: 0.9851 - val_loss: 0.3274 - val_acc: 0.8885\n",
      "Epoch 94/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0429 - acc: 0.9899 - val_loss: 0.3029 - val_acc: 0.9059\n",
      "Epoch 95/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0412 - acc: 0.9881 - val_loss: 0.2923 - val_acc: 0.9039\n",
      "Epoch 96/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0496 - acc: 0.9864 - val_loss: 0.3437 - val_acc: 0.8896\n",
      "Epoch 97/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0645 - acc: 0.9776 - val_loss: 0.3730 - val_acc: 0.8845\n",
      "Epoch 98/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0654 - acc: 0.9781 - val_loss: 0.3009 - val_acc: 0.9018\n",
      "Epoch 99/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0482 - acc: 0.9881 - val_loss: 0.3074 - val_acc: 0.9018\n",
      "Epoch 100/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0506 - acc: 0.9846 - val_loss: 0.3434 - val_acc: 0.8855\n",
      "Epoch 101/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0538 - acc: 0.9873 - val_loss: 0.2955 - val_acc: 0.9070\n",
      "Epoch 102/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0499 - acc: 0.9842 - val_loss: 0.2843 - val_acc: 0.9121\n",
      "Epoch 103/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0740 - acc: 0.9701 - val_loss: 0.3623 - val_acc: 0.8875\n",
      "Epoch 104/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0642 - acc: 0.9763 - val_loss: 0.2868 - val_acc: 0.8967\n",
      "Epoch 105/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0576 - acc: 0.9794 - val_loss: 0.2961 - val_acc: 0.8998\n",
      "Epoch 106/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0561 - acc: 0.9807 - val_loss: 0.2930 - val_acc: 0.9121\n",
      "Epoch 107/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0547 - acc: 0.9824 - val_loss: 0.2759 - val_acc: 0.9182\n",
      "Epoch 108/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0479 - acc: 0.9838 - val_loss: 0.3129 - val_acc: 0.9059\n",
      "Epoch 109/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0569 - acc: 0.9820 - val_loss: 0.2891 - val_acc: 0.9141\n",
      "Epoch 110/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0487 - acc: 0.9842 - val_loss: 0.2896 - val_acc: 0.9080\n",
      "Epoch 111/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0466 - acc: 0.9860 - val_loss: 0.2964 - val_acc: 0.9080\n",
      "Epoch 112/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0358 - acc: 0.9908 - val_loss: 0.2942 - val_acc: 0.9110\n",
      "Epoch 113/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0431 - acc: 0.9868 - val_loss: 0.3027 - val_acc: 0.9110\n",
      "Epoch 114/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0380 - acc: 0.9877 - val_loss: 0.2927 - val_acc: 0.9049\n",
      "Epoch 115/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0421 - acc: 0.9842 - val_loss: 0.3077 - val_acc: 0.9070\n",
      "Epoch 116/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0476 - acc: 0.9842 - val_loss: 0.2919 - val_acc: 0.9192\n",
      "Epoch 117/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0501 - acc: 0.9860 - val_loss: 0.2766 - val_acc: 0.9121\n",
      "Epoch 118/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0430 - acc: 0.9855 - val_loss: 0.3021 - val_acc: 0.9070\n",
      "Epoch 119/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0390 - acc: 0.9868 - val_loss: 0.2972 - val_acc: 0.8998\n",
      "Epoch 120/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0449 - acc: 0.9868 - val_loss: 0.3994 - val_acc: 0.8630\n",
      "Epoch 121/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0925 - acc: 0.9715 - val_loss: 0.3082 - val_acc: 0.9049\n",
      "Epoch 122/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0598 - acc: 0.9781 - val_loss: 0.3039 - val_acc: 0.9049\n",
      "Epoch 123/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0377 - acc: 0.9890 - val_loss: 0.2938 - val_acc: 0.9110\n",
      "Epoch 124/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0383 - acc: 0.9886 - val_loss: 0.2980 - val_acc: 0.9090\n",
      "Epoch 125/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0439 - acc: 0.9820 - val_loss: 0.2719 - val_acc: 0.9100\n",
      "Epoch 126/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0460 - acc: 0.9851 - val_loss: 0.3043 - val_acc: 0.9121\n",
      "Epoch 127/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0628 - acc: 0.9807 - val_loss: 0.3084 - val_acc: 0.8967\n",
      "Epoch 128/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0438 - acc: 0.9886 - val_loss: 0.2739 - val_acc: 0.9100\n",
      "Epoch 129/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - 0s - loss: 0.0439 - acc: 0.9851 - val_loss: 0.2934 - val_acc: 0.9121\n",
      "Epoch 130/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0429 - acc: 0.9855 - val_loss: 0.3729 - val_acc: 0.8824\n",
      "Epoch 131/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0391 - acc: 0.9860 - val_loss: 0.2913 - val_acc: 0.9100\n",
      "Epoch 132/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0319 - acc: 0.9899 - val_loss: 0.3089 - val_acc: 0.8967\n",
      "Epoch 133/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0367 - acc: 0.9899 - val_loss: 0.2901 - val_acc: 0.9039\n",
      "Epoch 134/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0468 - acc: 0.9824 - val_loss: 0.3006 - val_acc: 0.8988\n",
      "Epoch 135/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0592 - acc: 0.9789 - val_loss: 0.3004 - val_acc: 0.9080\n",
      "Epoch 136/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0458 - acc: 0.9842 - val_loss: 0.3018 - val_acc: 0.9080\n",
      "Epoch 137/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0308 - acc: 0.9903 - val_loss: 0.2945 - val_acc: 0.9018\n",
      "Epoch 138/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0272 - acc: 0.9930 - val_loss: 0.2927 - val_acc: 0.9049\n",
      "Epoch 139/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0276 - acc: 0.9921 - val_loss: 0.2888 - val_acc: 0.9131\n",
      "Epoch 140/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0237 - acc: 0.9934 - val_loss: 0.3097 - val_acc: 0.9018\n",
      "Epoch 141/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0279 - acc: 0.9917 - val_loss: 0.3015 - val_acc: 0.9100\n",
      "Epoch 142/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0273 - acc: 0.9912 - val_loss: 0.3007 - val_acc: 0.9059\n",
      "Epoch 143/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0274 - acc: 0.9934 - val_loss: 0.2797 - val_acc: 0.9213\n",
      "Epoch 144/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0251 - acc: 0.9925 - val_loss: 0.3329 - val_acc: 0.9008\n",
      "Epoch 145/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0440 - acc: 0.9855 - val_loss: 0.2852 - val_acc: 0.9090\n",
      "Epoch 146/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0336 - acc: 0.9903 - val_loss: 0.2885 - val_acc: 0.9110\n",
      "Epoch 147/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0318 - acc: 0.9908 - val_loss: 0.2738 - val_acc: 0.9110\n",
      "Epoch 148/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0322 - acc: 0.9903 - val_loss: 0.2889 - val_acc: 0.9059\n",
      "Epoch 149/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0392 - acc: 0.9877 - val_loss: 0.2943 - val_acc: 0.9213\n",
      "Epoch 150/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0291 - acc: 0.9908 - val_loss: 0.3028 - val_acc: 0.9131\n",
      "Epoch 151/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0310 - acc: 0.9921 - val_loss: 0.3028 - val_acc: 0.9039\n",
      "Epoch 152/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0305 - acc: 0.9930 - val_loss: 0.2861 - val_acc: 0.9202\n",
      "Epoch 153/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0246 - acc: 0.9934 - val_loss: 0.2642 - val_acc: 0.9264\n",
      "Epoch 154/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0294 - acc: 0.9917 - val_loss: 0.3275 - val_acc: 0.8998\n",
      "Epoch 155/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0218 - acc: 0.9930 - val_loss: 0.3046 - val_acc: 0.9039\n",
      "Epoch 156/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0213 - acc: 0.9965 - val_loss: 0.2729 - val_acc: 0.9223\n",
      "Epoch 157/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0260 - acc: 0.9930 - val_loss: 0.3234 - val_acc: 0.8998\n",
      "Epoch 158/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0199 - acc: 0.9939 - val_loss: 0.3197 - val_acc: 0.9121\n",
      "Epoch 159/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0164 - acc: 0.9960 - val_loss: 0.2819 - val_acc: 0.9182\n",
      "Epoch 160/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0197 - acc: 0.9947 - val_loss: 0.2865 - val_acc: 0.9192\n",
      "Epoch 161/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0230 - acc: 0.9917 - val_loss: 0.3146 - val_acc: 0.9090\n",
      "Epoch 162/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0215 - acc: 0.9939 - val_loss: 0.2781 - val_acc: 0.9110\n",
      "Epoch 163/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0255 - acc: 0.9925 - val_loss: 0.3235 - val_acc: 0.9080\n",
      "Epoch 164/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0334 - acc: 0.9895 - val_loss: 0.2926 - val_acc: 0.9121\n",
      "Epoch 165/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0274 - acc: 0.9917 - val_loss: 0.2638 - val_acc: 0.9264\n",
      "Epoch 166/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0223 - acc: 0.9930 - val_loss: 0.2958 - val_acc: 0.9100\n",
      "Epoch 167/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0292 - acc: 0.9890 - val_loss: 0.3591 - val_acc: 0.8937\n",
      "Epoch 168/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0358 - acc: 0.9864 - val_loss: 0.3364 - val_acc: 0.8957\n",
      "Epoch 169/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0696 - acc: 0.9719 - val_loss: 0.4131 - val_acc: 0.8599\n",
      "Epoch 170/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0611 - acc: 0.9781 - val_loss: 0.3126 - val_acc: 0.9018\n",
      "Epoch 171/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0468 - acc: 0.9864 - val_loss: 0.2788 - val_acc: 0.9172\n",
      "Epoch 172/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0367 - acc: 0.9899 - val_loss: 0.2907 - val_acc: 0.9110\n",
      "Epoch 173/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0372 - acc: 0.9899 - val_loss: 0.2768 - val_acc: 0.9172\n",
      "Epoch 174/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0241 - acc: 0.9934 - val_loss: 0.2944 - val_acc: 0.9192\n",
      "Epoch 175/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0296 - acc: 0.9908 - val_loss: 0.3061 - val_acc: 0.9141\n",
      "Epoch 176/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0343 - acc: 0.9881 - val_loss: 0.2639 - val_acc: 0.9233\n",
      "Epoch 177/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0295 - acc: 0.9903 - val_loss: 0.2615 - val_acc: 0.9213\n",
      "Epoch 178/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0274 - acc: 0.9930 - val_loss: 0.2930 - val_acc: 0.9151\n",
      "Epoch 179/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0190 - acc: 0.9943 - val_loss: 0.2729 - val_acc: 0.9274\n",
      "Epoch 180/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0181 - acc: 0.9947 - val_loss: 0.2621 - val_acc: 0.9213\n",
      "Epoch 181/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0188 - acc: 0.9960 - val_loss: 0.3276 - val_acc: 0.9070\n",
      "Epoch 182/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0233 - acc: 0.9952 - val_loss: 0.2682 - val_acc: 0.9202\n",
      "Epoch 183/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0245 - acc: 0.9917 - val_loss: 0.3007 - val_acc: 0.9131\n",
      "Epoch 184/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0311 - acc: 0.9903 - val_loss: 0.2719 - val_acc: 0.9182\n",
      "Epoch 185/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0456 - acc: 0.9846 - val_loss: 0.2688 - val_acc: 0.9202\n",
      "Epoch 186/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0326 - acc: 0.9895 - val_loss: 0.2633 - val_acc: 0.9202\n",
      "Epoch 187/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0343 - acc: 0.9890 - val_loss: 0.3176 - val_acc: 0.8967\n",
      "Epoch 188/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0309 - acc: 0.9899 - val_loss: 0.3064 - val_acc: 0.9059\n",
      "Epoch 189/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0306 - acc: 0.9912 - val_loss: 0.3049 - val_acc: 0.9049\n",
      "Epoch 190/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0305 - acc: 0.9881 - val_loss: 0.3024 - val_acc: 0.9141\n",
      "Epoch 191/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0218 - acc: 0.9912 - val_loss: 0.2925 - val_acc: 0.9070\n",
      "Epoch 192/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0302 - acc: 0.9912 - val_loss: 0.2888 - val_acc: 0.9213\n",
      "Epoch 193/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2278/2278 [==============================] - 0s - loss: 0.0234 - acc: 0.9930 - val_loss: 0.3112 - val_acc: 0.9121\n",
      "Epoch 194/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0219 - acc: 0.9934 - val_loss: 0.2812 - val_acc: 0.9192\n",
      "Epoch 195/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0255 - acc: 0.9908 - val_loss: 0.3694 - val_acc: 0.8978\n",
      "Epoch 196/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0257 - acc: 0.9903 - val_loss: 0.2824 - val_acc: 0.9202\n",
      "Epoch 197/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0236 - acc: 0.9934 - val_loss: 0.2665 - val_acc: 0.9131\n",
      "Epoch 198/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0380 - acc: 0.9873 - val_loss: 0.3439 - val_acc: 0.8978\n",
      "Epoch 199/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0328 - acc: 0.9908 - val_loss: 0.3606 - val_acc: 0.9070\n",
      "Epoch 200/200\n",
      "2278/2278 [==============================] - 0s - loss: 0.0300 - acc: 0.9881 - val_loss: 0.3265 - val_acc: 0.9070\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit(X_train, Y_train, batch_size=128, epochs=200, validation_data=(X_val, Y_val)) #callbacks=[early_stopping]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_val)\n",
    "y_true = [max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_val]\n",
    "y_pred = [max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred]\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.90659639730452968"
      ]
     },
     "execution_count": 321,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1])"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_val[1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  4.70719121e-15,   1.00000000e+00],\n",
       "       [  5.68544777e-14,   1.00000000e+00],\n",
       "       [  1.99857548e-18,   1.00000000e+00],\n",
       "       [  1.62834822e-05,   9.99983668e-01],\n",
       "       [  2.65297940e-11,   1.00000000e+00],\n",
       "       [  3.26242852e-12,   1.00000000e+00],\n",
       "       [  4.11501969e-06,   9.99995828e-01],\n",
       "       [  1.92551225e-10,   1.00000000e+00],\n",
       "       [  6.02835673e-04,   9.99397159e-01],\n",
       "       [  2.27131158e-09,   1.00000000e+00],\n",
       "       [  4.50570951e-05,   9.99954939e-01],\n",
       "       [  5.96788668e-05,   9.99940276e-01],\n",
       "       [  5.31585003e-14,   1.00000000e+00],\n",
       "       [  6.96933782e-08,   9.99999881e-01],\n",
       "       [  1.58653242e-13,   1.00000000e+00],\n",
       "       [  1.53077719e-26,   1.00000000e+00],\n",
       "       [  2.25973595e-02,   9.77402627e-01],\n",
       "       [  4.77840245e-01,   5.22159815e-01],\n",
       "       [  2.87815639e-11,   1.00000000e+00],\n",
       "       [  1.31480746e-28,   1.00000000e+00],\n",
       "       [  1.57597730e-12,   1.00000000e+00],\n",
       "       [  1.88092380e-12,   1.00000000e+00]], dtype=float32)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict(X_cyst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "activ = model2.predict(X_train)\n",
    "#activ = model2.predict(np.expand_dims(X_train[10],axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1c8e3907550>"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAADQ9JREFUeJzt3X9sXfV5x/H3g2PnJyKBACoJKqhlmbJKLShFdEzVBusE\nbUX/2SSQQGo1qf+sFLZKFd0/1f6fqlZTVQkBXadmoI2CVFUMxlaqqlKbFgItPwJbRktJSkkYJYmT\nEMf2sz98KQ7L5HPj8/ja371fkhXf65PHz7X98ffc43OfE5mJpDadNeoGJNUx4FLDDLjUMAMuNcyA\nSw0z4FLDDLjUMAMuNcyASw1bVVF0fGJ9rlm7qaI0ADFTe/bd7Or633tnnSx+DGNRWr/6e3DWienS\n+gBUn8U5M1NW+vjsJFP55oLf5JKAr1m7iSuu/mxFaQDGD02V1QaYfPfa0voA616tfQwnNo6X1h8/\nUhvAtXsPltYHYLougACzhw6X1f7R5Lc7becuutQwAy41zIBLDTPgUsMMuNQwAy41zIBLDesU8Ii4\nLiJeiIi9EXFHdVOS+rFgwCNiDPgqcD2wHbgpIrZXNyZp8bqs4FcCezPzxcycAu4DPlHblqQ+dAn4\nFuDlebf3De47RUR8OiIej4jHT04d7as/SYvQ20G2zLwzM3dk5o7xifV9lZW0CF0Cvh+4eN7trYP7\nJC1zXQL+E+CyiLg0IiaAG4FuL2WRNFILvlw0M6cj4jPAI8AYcE9mPlvemaRF6/R68Mx8CHiouBdJ\nPfNMNqlhBlxqmAGXGmbApYYZcKlhBlxqWMnY5NmJYPKiktIAbCieZ736N/UzuY++a6K0/oEPlpZn\ndmNt/XM3r6n9BMD0v28urb9l53/WFT/WbW12BZcaZsClhhlwqWEGXGqYAZcaZsClhhlwqWEGXGpY\nl7HJ90TEgYh4ZikaktSfLiv43wPXFfchqcCCAc/M7wOvL0Evknrmc3CpYb0FfP6FD6aPe+EDaTko\nufDBqrVe+EBaDtxFlxrW5c9k9wI/BLZFxL6I+PP6tiT1ocuFD25aikYk9c9ddKlhBlxqmAGXGmbA\npYYZcKlhBlxqWMnw8plxmNwaFaUBeGPbeFltgHP2lpYH4KKbf15a/8/O+4/S+m/O1n4PLhw/VFof\n4JFNv1da//k/vKCs9tRfdYuuK7jUMAMuNcyASw0z4FLDDLjUMAMuNcyASw0z4FLDugx8uDgiHouI\n5yLi2Yi4bSkak7R4XU6HmQY+l5m7I+Js4ImIeDQznyvuTdIidZmL/kpm7h68fwTYA2ypbkzS4g31\nHDwiLgEuB3ZVNCOpX50DHhEbgG8Bt2fm4dN8/Ldz0WeOORddWg46BTwixpkL987MfOB028yfiz62\nzrno0nLQ5Sh6AHcDezLzS/UtSepLlxX8auAW4JqIeGrw9tHiviT1oMtc9B8AddMbJJXxTDapYQZc\napgBlxpmwKWGGXCpYQZcapgBlxpWcuEDgtJfHTPrZ+uKA/+9o7Y+wOOXPVz+OSr9w+HNpfVnl2Dt\n2TRxrLT+Le/9cVntv1vT7fUeruBSwwy41DADLjXMgEsNM+BSwwy41DADLjWsy0SXNRHx44j46WAu\n+t8sRWOSFq/LiS4ngGsyc3Iwm+0HEfEvmfmj4t4kLVKXiS4JTA5ujg/esrIpSf3oOlV1LCKeAg4A\nj2amc9GlFaBTwDNzJjM/AGwFroyI971zm1Pmoh91Lrq0HAx1FD0z3wAeA647zcfenou+3rno0nLQ\n5Sj6+RGxcfD+WuAjwPPVjUlavC5H0d8FfCMixpj7hfBPmfmd2rYk9aHLUfSfMXfBQUkrjGeySQ0z\n4FLDDLjUMAMuNcyASw0z4FLDDLjUsJK56DED44crKg/qT4/VFQeOv2e6tD7Asdmp0vrjUfs1Oja7\nurT+N166qrQ+wPZNr5bWP/usN8tqj9Ftdr8ruNQwAy41zIBLDTPgUsMMuNQwAy41zIBLDesc8MHg\nxScjwmEP0goxzAp+G7CnqhFJ/es6Nnkr8DHgrtp2JPWp6wr+ZeDz0PH8OEnLQpepqh8HDmTmEwts\n9/Zc9GPORZeWgy4r+NXADRHxC+A+4JqI+OY7NzplLvo656JLy8GCAc/ML2Tm1sy8BLgR+G5m3lze\nmaRF8+/gUsOGej14Zn4P+F5JJ5J65wouNcyASw0z4FLDDLjUMAMuNcyASw0z4FLDSuaij03B2S/P\nVJQG4Nj5tTO/37yw5Mtyipema2evv3e89mu05qyTpfUPHV1bWh/gh0cvKa3/O9vq5q7PZLe12RVc\napgBlxpmwKWGGXCpYQZcapgBlxpmwKWGGXCpYZ3O6BjMYzsCzADTmbmjsilJ/RjmlK0/yszXyjqR\n1Dt30aWGdQ14Av8WEU9ExKdPt8H8uegnT0z216GkM9Z1F/0PMnN/RFwAPBoRz2fm9+dvkJl3AncC\nbDj34uy5T0lnoNMKnpn7B/8eAB4ErqxsSlI/uly6aH1EnP3W+8CfAM9UNyZp8brsol8IPBgRb23/\nj5n5cGlXknqxYMAz80Xg/UvQi6Se+WcyqWEGXGqYAZcaZsClhhlwqWEGXGpYyQDw6TXw+va6udwn\nzqubuQ4wdqz+995HH7u19hPMRG396dqv0cTB2rnuAJs/WDe3HODpI1vKah+fnei0nSu41DADLjXM\ngEsNM+BSwwy41DADLjXMgEsNM+BSwzoFPCI2RsT9EfF8ROyJiA9VNyZp8bqeyfYV4OHM/NOImADW\nFfYkqScLBjwizgE+DHwSIDOngKnatiT1ocsu+qXAQeDrEfFkRNw1GL54ivlz0WeOHe29UUnD6xLw\nVcAVwNcy83LgKHDHOzfKzDszc0dm7hhb97/yL2kEugR8H7AvM3cNbt/PXOAlLXMLBjwzfw28HBHb\nBnddCzxX2pWkXnQ9in4rsHNwBP1F4FN1LUnqS6eAZ+ZTgNcEl1YYz2STGmbApYYZcKlhBlxqmAGX\nGmbApYYZcKlhJRc+yHE4cUHdxQk2PlP7e2nta7Ol9QHeuGx1af3ptVlaf90rtRdWWH24/nvwq/M3\n19YfP7es9uTxbj8/ruBSwwy41DADLjXMgEsNM+BSwwy41DADLjVswYBHxLaIeGre2+GIuH0pmpO0\nOAue6JKZLwAfAIiIMWA/8GBxX5J6MOwu+rXAf2XmSxXNSOrXsAG/Ebi3ohFJ/esc8MHAxRuAf/4/\nPv72hQ8mJ/vqT9IiDLOCXw/szsxXT/fBUy58sGFDP91JWpRhAn4T7p5LK0rXywevBz4CPFDbjqQ+\ndZ2LfhQ4r7gXST3zTDapYQZcapgBlxpmwKWGGXCpYQZcapgBlxpWMhd99euzvOfeExWlAXjzgtqZ\n4ke2jpXWB5g4XF2/dm75uS/UfX8Bxo5Pl9YHOGfnz2o/QdbNpv9NHuu0nSu41DADLjXMgEsNM+BS\nwwy41DADLjXMgEsN6zrw4S8j4tmIeCYi7o2INdWNSVq8Lhc+2AJ8FtiRme8Dxpibrippmeu6i74K\nWBsRq4B1wK/qWpLUlwUDnpn7gb8Ffgm8AhzKzH+tbkzS4nXZRd8EfAK4FLgIWB8RN59mu9/ORZ86\nebT/TiUNrcsu+h8DP8/Mg5l5krnJqr//zo3mz0WfGF/fd5+SzkCXgP8SuCoi1kVEMHd9sj21bUnq\nQ5fn4LuA+4HdwNOD/3NncV+SetB1LvoXgS8W9yKpZ57JJjXMgEsNM+BSwwy41DADLjXMgEsNM+BS\nwyILZjdHxEHgpSH+y2bgtd4bWTr2P3or/TEM2/+7M/P8hTYqCfiwIuLxzNwx6j7OlP2P3kp/DFX9\nu4suNcyASw1bLgFf6S9esf/RW+mPoaT/ZfEcXFKN5bKCSyow0oBHxHUR8UJE7I2IO0bZy5mIiIsj\n4rGIeG4wVvq2Ufd0JiJiLCKejIjvjLqXYUXExoi4PyKej4g9EfGhUfc0jOqR5CMLeESMAV8Frge2\nAzdFxPZR9XOGpoHPZeZ24CrgL1bgYwC4jZU7pecrwMOZ+bvA+1lBj2MpRpKPcgW/EtibmS9m5hRw\nH3PDHVeMzHwlM3cP3j/C3A/XltF2NZyI2Ap8DLhr1L0MKyLOAT4M3A2QmVOZ+cZouxpa6UjyUQZ8\nC/DyvNv7WGHhmC8iLgEuB3aNtpOhfRn4PDA76kbOwKXAQeDrg6cYd0XEipn4uRQjyT3I1oOI2AB8\nC7g9Mw+Pup+uIuLjwIHMfGLUvZyhVcAVwNcy83LgKLBijuV0HUm+GKMM+H7g4nm3tw7uW1EiYpy5\ncO/MzAdG3c+QrgZuiIhfMPcU6ZqI+OZoWxrKPmDfYDAozA0HvWKE/Qyr00jyxRhlwH8CXBYRl0bE\nBHMHF749wn6GNhgjfTewJzO/NOp+hpWZX8jMrZl5CXNf/+9mZq8rSKXM/DXwckRsG9x1LfDcCFsa\nVvlI8k5TVStk5nREfAZ4hLmjh/dk5rOj6ucMXQ3cAjwdEU8N7vvrzHxohD39f3MrsHOwSLwIfGrE\n/XSWmbsi4q2R5NPAk/R8RptnskkN8yCb1DADLjXMgEsNM+BSwwy41DADLjXMgEsNM+BSw/4H529U\nvB8ZGkIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1c8e38b92b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(activ[20][:,:,3,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   0.        ,    0.        ,    0.        ,    0.        ,\n",
       "          0.        ,  121.71251678,   53.23372269,    0.        ,\n",
       "          0.        ,   24.43340492,    0.        ,    0.        ,\n",
       "          0.        ,    0.        ,    7.88301706,    0.        ], dtype=float32)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activ[0][5,5,2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['12296182_23.npy', '13028374_2.npy', 'E104657225_14.npy',\n",
       "       '12972894_6.npy'],\n",
       "      dtype='<U17')"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_val[::30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.47784024,  0.52215981], dtype=float32)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_[-5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8881987577639752"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "143/(143+18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(15+6+7)/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions: ['cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.99998% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.99858% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.99996% confidence)', 'cyst (99.99999% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (99.60698% confidence)', 'hcc (70.35562% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)', 'cyst (100.00000% confidence)']\n"
     ]
    }
   ],
   "source": [
    "import operator\n",
    "#print(\"Ground truth:\", [cls_mapping[max(enumerate(x), key=operator.itemgetter(1))[0]] for x in Y_val[::30]])\n",
    "Y_ = model.predict(X_cyst)\n",
    "print(\"Predictions:\", [cls_mapping[max(enumerate(x), key=operator.itemgetter(1))[0]] + \" (%.5f%% confidence)\" % (max(x)*100) for x in Y_])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAC7CAYAAAB1qmWGAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x266df7b4da0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "voi_df = pd.read_csv('..\\\\liver-mr-processor\\\\vois.csv')\n",
    "img_fn = \"12972894.npy\"\n",
    "img = np.load(\"..\\\\liver-mr-processor\\\\full_imgs\\\\\"+img_fn)\n",
    "plot_section(img, voi_df[voi_df[\"Filename\"] == img_fn].iloc[0], pad=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_section(img, df, pad=30):\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(np.transpose(img[df['x1']-pad:df['x2']+pad,\n",
    "                                df['y2']+pad:df['y1']-pad:-1,\n",
    "                                (df['z1']+df['z2'])//2, 0], (1,0)), cmap='gray')\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(np.transpose(img[df['x1']-pad:df['x2']+pad,\n",
    "                                df['y2']+pad:df['y1']-pad:-1,\n",
    "                                (df['z1']+df['z2'])//2, 1], (1,0)), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  2.78843247e-04,   9.99721110e-01], dtype=float32)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6875"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(sum(y)/len(y), 1-sum(y)/len(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "210"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.20054101943969727\n"
     ]
    }
   ],
   "source": [
    "a=time.time()\n",
    "Y_ = model.predict(X_val)\n",
    "print(time.time()-a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amax(X[650,:,:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x20b42b02b70>"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXIAAAD8CAYAAABq6S8VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x20b424cdcf8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(X[0,:,:,5,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
