{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:57:53.323692Z",
     "start_time": "2018-05-15T14:57:50.859384Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import importlib\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from math import e, exp, log, pi, sqrt\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "import keras.layers as layers\n",
    "import copy\n",
    "import keras.models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from numba import guvectorize, jit, njit, prange, vectorize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from os.path import *\n",
    "import glob\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import dr_methods as drm\n",
    "import feature_interpretation as cnna\n",
    "import niftiutils.helper_fxns as hf\n",
    "import niftiutils.private as prv\n",
    "import niftiutils.transforms as tr\n",
    "import niftiutils.visualization as vis\n",
    "import voi_methods as vm\n",
    "import feature_influence as finf\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:57:53.346118Z",
     "start_time": "2018-05-15T14:57:53.338098Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config()\n",
    "T = config.Hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:57:58.768534Z",
     "start_time": "2018-05-15T14:57:53.851457Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_reader = ['E103312835_1','12823036_0','12569915_0','E102093118_0','E102782525_0','12799652_0','E100894274_0','12874178_3','E100314676_0','12842070_0','13092836_2','12239783_0','12783467_0','13092966_0','E100962970_0','E100183257_1','E102634440_0','E106182827_0','12582632_0','E100121654_0','E100407633_0','E105310461_0','12788616_0','E101225606_0','12678910_1','E101083458_1','12324408_0','13031955_0','E101415263_0','E103192914_0','12888679_2','E106096969_0','E100192709_1','13112385_1','E100718398_0','12207268_0','E105244287_0','E102095465_0','E102613189_0','12961059_0','11907521_0','E105311123_0','12552705_0','E100610622_0','12975280_0','E105918926_0','E103020139_1','E101069048_1','E105427046_0','13028374_0','E100262351_0','12302576_0','12451831_0','E102929168_0','E100383453_0','E105344747_0','12569826_0','E100168661_0','12530153_0','E104697262_0']\n",
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "feat_count.pop(\"homogeneous texture\")\n",
    "#feat_count.pop(\"central scar\")\n",
    "all_features = sorted(list(feat_count.keys()))\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}\n",
    "\n",
    "Z_features = cnna.get_annotated_files(features_by_cls)\n",
    "Z_features.pop(\"homogeneous texture\")\n",
    "#Z_features.pop(\"central scar\")\n",
    "\n",
    "num_features = len(all_features)\n",
    "\n",
    "voi_df = drm.get_voi_dfs()[0]\n",
    "M = keras.models.load_model(join(C.model_dir, \"model_reader_new21.hdf5\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:41:23.282420Z",
     "start_time": "2018-05-14T16:41:18.726Z"
    },
    "collapsed": true
   },
   "source": [
    "M.layers[5].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:57:59.285885Z",
     "start_time": "2018-05-15T14:57:59.275857Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "inf_xls_path = 'D:\\\\feature_analysis\\\\influence.xlsx'\n",
    "\n",
    "#df = pd.DataFrame(columns=all_features)\n",
    "df = pd.read_excel(inf_xls_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:58:01.352588Z",
     "start_time": "2018-05-15T14:58:01.052431Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_full = np.concatenate([orig_data_dict[cls][-1] for cls in C.classes_to_include],0)\n",
    "\n",
    "all_imgs = []\n",
    "all_cls = []\n",
    "\n",
    "for lesion_id in Z_full:\n",
    "    cls = voi_df.loc[lesion_id][\"cls\"]\n",
    "    img = np.load(join(C.orig_dir, cls, lesion_id+\".npy\"))\n",
    "    all_imgs.append(np.expand_dims(img,0))\n",
    "    all_cls.append(C.classes_to_include.index(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:58:07.177727Z",
     "start_time": "2018-05-15T14:58:07.147665Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(finf)\n",
    "IA = finf.InfluenceAnalyzer(M, voi_df, all_imgs, all_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:58:21.487504Z",
     "start_time": "2018-05-15T14:58:21.483494Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:56:02.591350Z",
     "start_time": "2018-05-15T14:40:50.910159Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E100894274_0\n",
      "911.639904499054\n",
      "4.320125579833984\n",
      "12874178_3\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Tensor(\"Const:0\", shape=(), dtype=float32) must be from the same graph as Tensor(\"truediv_175:0\", shape=(?, 6), dtype=float32).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-efee78c754a6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlesion_id\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mZ_reader\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlesion_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mg_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlesion_id\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m     \u001b[0ms_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mIA\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_stest\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mg_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\voi-classifier\\python\\feature_influence.py\u001b[0m in \u001b[0;36mget_grad\u001b[1;34m(self, lesion_id, perturb_W)\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                 \u001b[0my_true\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_categorical\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mC\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclasses_to_include\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcls\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m6\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcategorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/gpu:0'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\keras-2.1.2-py3.5.egg\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36mcategorical_crossentropy\u001b[1;34m(target, output, from_logits)\u001b[0m\n\u001b[0;32m   3003\u001b[0m         \u001b[1;31m# manual computation of crossentropy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3004\u001b[0m         \u001b[0m_epsilon\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3005\u001b[1;33m         \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclip_by_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_epsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1.\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0m_epsilon\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3006\u001b[0m         return - tf.reduce_sum(target * tf.log(output),\n\u001b[0;32m   3007\u001b[0m                                len(output.get_shape()) - 1)\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\ops\\clip_ops.py\u001b[0m in \u001b[0;36mclip_by_value\u001b[1;34m(t, clip_value_min, clip_value_max, name)\u001b[0m\n\u001b[0;32m     60\u001b[0m   \"\"\"\n\u001b[0;32m     61\u001b[0m   with ops.name_scope(name, \"clip_by_value\",\n\u001b[1;32m---> 62\u001b[1;33m                       [t, clip_value_min, clip_value_max]) as name:\n\u001b[0m\u001b[0;32m     63\u001b[0m     return gen_math_ops.clip_by_value(t,\n\u001b[0;32m     64\u001b[0m                                       \u001b[0mclip_value_min\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__enter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5975\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5976\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5977\u001b[1;33m       \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_graph_from_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5978\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5979\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_g_manager\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__enter__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_get_graph_from_inputs\u001b[1;34m(op_input_list, graph)\u001b[0m\n\u001b[0;32m   5635\u001b[0m         \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5636\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0moriginal_graph_element\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5637\u001b[1;33m         \u001b[0m_assert_same_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moriginal_graph_element\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5638\u001b[0m       \u001b[1;32melif\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5639\u001b[0m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not from the passed-in graph.\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mgraph_element\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_assert_same_graph\u001b[1;34m(original_item, item)\u001b[0m\n\u001b[0;32m   5571\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0moriginal_item\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5572\u001b[0m     raise ValueError(\"%s must be from the same graph as %s.\" % (item,\n\u001b[1;32m-> 5573\u001b[1;33m                                                                 original_item))\n\u001b[0m\u001b[0;32m   5574\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5575\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Tensor(\"Const:0\", shape=(), dtype=float32) must be from the same graph as Tensor(\"truediv_175:0\", shape=(?, 6), dtype=float32)."
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "lesion_id = Z_reader[7]\n",
    "print(lesion_id)\n",
    "g_test = IA.get_grad(lesion_id)\n",
    "s_test = IA.get_stest(g_test)\n",
    "\n",
    "I = {}\n",
    "for f in Z_features:\n",
    "    I[f] = IA.get_avg_influence(Z_features[f], s_test)\n",
    "\n",
    "df.loc[lesion_id] = list(I[f] for f in df.columns)\n",
    "\n",
    "print(time.time() - t)\n",
    "pid = os.getpid()\n",
    "py = psutil.Process(pid)\n",
    "print(py.memory_info()[0]/2.**30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:56:55.466867Z",
     "start_time": "2018-05-15T14:56:55.217154Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_excel(inf_xls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T14:58:12.984915Z",
     "start_time": "2018-05-15T14:58:12.977581Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9538345336914062"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pid = os.getpid()\n",
    "py = psutil.Process(pid)\n",
    "py.memory_info()[0]/2.**30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T22:45:33.942966Z",
     "start_time": "2018-05-14T22:45:33.790216Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls = voi_df.loc[lesion_id][\"cls\"]\n",
    "x_test = np.load(join(C.orig_dir, cls, lesion_id+\".npy\"))\n",
    "vis.draw_slices(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:41:23.716962Z",
     "start_time": "2018-05-14T16:41:23.709943Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def perturb_weights(W_true, t):\n",
    "    eps = 1e-5\n",
    "    W = copy.deepcopy(W_true)\n",
    "    t_ix = 0\n",
    "    \n",
    "    for w_ix in range(len(W)):\n",
    "        W[w_ix] += eps * np.reshape(t[t_ix:t_ix+W[w_ix].size], W[w_ix].shape)\n",
    "        t_ix += W[w_ix].size\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:41:23.950594Z",
     "start_time": "2018-05-14T16:41:23.903360Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_grad(lesion_id, perturb_W=None):\n",
    "    C = config.Config()\n",
    "    \n",
    "    cls = voi_df.loc[lesion_id][\"cls\"]\n",
    "    img = np.load(join(C.orig_dir, cls, lesion_id+\".npy\"))\n",
    "    img = np.expand_dims(img,0)\n",
    "\n",
    "    y_true = np_utils.to_categorical(C.classes_to_include.index(cls), 6)\n",
    "    loss = K.categorical_crossentropy(y_true, M.output)\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        g = K.gradients(loss, M.trainable_weights)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        g_i = sess.run(g, feed_dict={M.input:img, K.learning_phase():0})\n",
    "        if perturb_W is not None:\n",
    "            feed_dict_plus = {M.trainable_weights[i]:perturb_W[i] for i in range(len(perturb_W))}\n",
    "            g_i_plus = sess.run(g, feed_dict={**feed_dict_plus, M.input:img, K.learning_phase():0})\n",
    "\n",
    "    g_i = np.concatenate([x.flatten() for x in g_i], 0)\n",
    "    \n",
    "    if perturb_W is not None:\n",
    "        g_i_plus = np.concatenate([x.flatten() for x in g_i_plus], 0)\n",
    "        return g_i, g_i_plus\n",
    "    \n",
    "    return g_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:41:24.068235Z",
     "start_time": "2018-05-14T16:41:24.062220Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_grads(Z_sample):\n",
    "    eps = 1e-5\n",
    "    Ht = np.zeros(g_test.shape)\n",
    "    feed_dict_plus = {M.trainable_weights[i]:perturb_W[i] for i in range(len(perturb_W))}\n",
    "    losses = [K.categorical_crossentropy(y_true, M.output) for y_true in \\\n",
    "              [np_utils.to_categorical(i, 6) for i in range(6)]]\n",
    "        \n",
    "    t = time.time()\n",
    "    with tf.device('/gpu:0'):\n",
    "        g = [K.gradients(loss, M.trainable_weights) for loss in losses]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for ix in range(len(Z_sample)):\n",
    "            g_i = sess.run(g[classes[ix]], feed_dict={M.input:imgs[ix], K.learning_phase():0})\n",
    "            g_i = np.concatenate([x.flatten() for x in g_i], 0)\n",
    "    if verbose:\n",
    "        print(time.time()-t)\n",
    "    \n",
    "    return Ht / len(Z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:47:12.468765Z",
     "start_time": "2018-05-14T16:47:12.429514Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_HVP(perturb_W, Z_sample, verbose=True):\n",
    "    eps = 1e-5\n",
    "    Ht = np.zeros(g_test.shape)\n",
    "    feed_dict_plus = {M.trainable_weights[i]:perturb_W[i] for i in range(len(perturb_W))}\n",
    "    losses = [K.categorical_crossentropy(y_true, M.output) for y_true in \\\n",
    "              [np_utils.to_categorical(i, 6) for i in range(6)]]\n",
    "        \n",
    "    t = time.time()\n",
    "    with tf.device('/gpu:0'):\n",
    "        g = [K.gradients(loss, M.trainable_weights) for loss in losses]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for ix in range(len(Z_sample)):\n",
    "            g_i = sess.run(g[classes[ix]], feed_dict={M.input:imgs[ix], K.learning_phase():0})\n",
    "            g_i_plus = sess.run(g[classes[ix]], feed_dict={**feed_dict_plus, M.input:imgs[ix], K.learning_phase():0})\n",
    "    \n",
    "            g_i = np.concatenate([x.flatten() for x in g_i], 0)\n",
    "            g_i_plus = np.concatenate([x.flatten() for x in g_i_plus], 0)\n",
    "\n",
    "            Ht += (g_i_plus - g_i)/eps\n",
    "    if verbose:\n",
    "        print(time.time()-t)\n",
    "    \n",
    "    return Ht / len(Z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:46:54.464603Z",
     "start_time": "2018-05-14T16:46:54.461595Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t_k = np.zeros(g_test.shape)\n",
    "r_k = g_test#-Ht\n",
    "p_k = r_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:59:44.825692Z",
     "start_time": "2018-05-14T16:47:36.093846Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_iters = 15\n",
    "#phi_hist = []\n",
    "t = time.time()\n",
    "for _ in range(num_iters):\n",
    "    #Z_sample = random.sample(list(Z_full), 15)\n",
    "    W_new = perturb_weights(W, p_k)\n",
    "    Hp = get_HVP(W_new, Z_full)\n",
    "\n",
    "    alpha = np.dot(r_k, r_k) / np.dot(p_k, Hp)\n",
    "    t_k += alpha * p_k\n",
    "    r_k2 = r_k - alpha * Hp\n",
    "    #phi_hist = .5*np.dot(t_k, Ht) - g_test\n",
    "\n",
    "    beta = np.dot(r_k2, r_k2) / np.dot(r_k, r_k)\n",
    "    r_k = r_k2\n",
    "    p_k = r_k + beta*p_k\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T17:02:07.903731Z",
     "start_time": "2018-05-14T17:02:07.899692Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del p_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T17:11:21.860049Z",
     "start_time": "2018-05-14T17:11:21.834986Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_grads(imgs, classes, s_test, verbose=True):\n",
    "    eps = 1e-5\n",
    "    Ht = np.zeros(g_test.shape)\n",
    "    losses = [K.categorical_crossentropy(y_true, M.output) for y_true in \\\n",
    "              [np_utils.to_categorical(i, 6) for i in range(6)]]\n",
    "        \n",
    "    I = 0\n",
    "    t = time.time()\n",
    "    with tf.device('/gpu:0'):\n",
    "        g = [K.gradients(loss, M.trainable_weights) for loss in losses]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for ix in range(len(classes)):\n",
    "            g_i = sess.run(g[classes[ix]], feed_dict={M.input:imgs[ix], K.learning_phase():0})\n",
    "            g_i = np.concatenate([x.flatten() for x in g_i], 0)\n",
    "            I -= np.dot(g_i, s_test)\n",
    "    if verbose:\n",
    "        print(time.time()-t)\n",
    "    \n",
    "    return I / len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:43:33.458882Z",
     "start_time": "2018-05-14T20:43:33.438831Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_0 = hf.pickle_load(join(C.base_dir, \"data\", \"DFs4.bin\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:28:44.605752Z",
     "start_time": "2018-05-14T20:28:44.576671Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\Clinton\\Documents\\voi-classifier\\data\"\n",
    "answer_key = join(data_dir, \"ground_truth.xlsx\")\n",
    "answer_key = pd.read_excel(answer_key, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:42:25.459019Z",
     "start_time": "2018-05-14T20:42:25.454004Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['true_cls', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n",
       "       'ignore_1', 'ignore_2', 'BLANK', 'Unnamed: 9', 'Class', 'Num features'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_key.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:45:12.644340Z",
     "start_time": "2018-05-14T20:45:12.638323Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for z_test, row in answer_key.iterrows():\n",
    "    cls = df_0.loc[z_test, \"pred_cls\"]\n",
    "    feats = row[[f for f in answer_key.columns if f.startswith('feat')]].dropna().values\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:45:36.706311Z",
     "start_time": "2018-05-14T20:45:36.702299Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nodular growth', 'heterogeneous lesion',\n",
       "       'progressive enhancement'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "529px",
    "left": "1032.27px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
