{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T04:01:58.829340Z",
     "start_time": "2018-06-14T04:01:56.562759Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import importlib\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from math import e, exp, log, pi, sqrt\n",
    "import os\n",
    "import psutil\n",
    "\n",
    "import keras.layers as layers\n",
    "import copy\n",
    "import keras.models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from numba import guvectorize, jit, njit, prange, vectorize\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "import tensorflow as tf\n",
    "from os.path import *\n",
    "import glob\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import dr_methods as drm\n",
    "import feature_interpretation as cnna\n",
    "import niftiutils.helper_fxns as hf\n",
    "import niftiutils.private as prv\n",
    "import niftiutils.transforms as tr\n",
    "import niftiutils.visualization as vis\n",
    "import voi_methods as vm\n",
    "import feature_influence as finf\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-14T04:01:58.856754Z",
     "start_time": "2018-06-14T04:01:58.843671Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(drm)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(cnna)\n",
    "importlib.reload(crun)\n",
    "C = config.Config(\"etiology\")\n",
    "T = config.Hyperparams()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-13T23:12:36.409085Z",
     "start_time": "2018-06-13T23:12:36.406115Z"
    }
   },
   "source": [
    "### backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T18:39:39.833525Z",
     "start_time": "2018-05-16T18:39:34.602623Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_num = 21\n",
    "model_path = join(C.model_dir, \"model_reader_new%d.hdf5\" % model_num)\n",
    "\n",
    "Z_reader = ['E103312835_1','12823036_0','12569915_0','E102093118_0','E102782525_0','12799652_0','E100894274_0','12874178_3','E100314676_0','12842070_0','13092836_2','12239783_0','12783467_0','13092966_0','E100962970_0','E100183257_1','E102634440_0','E106182827_0','12582632_0','E100121654_0','E100407633_0','E105310461_0','12788616_0','E101225606_0','12678910_1','E101083458_1','12324408_0','13031955_0','E101415263_0','E103192914_0','12888679_2','E106096969_0','E100192709_1','13112385_1','E100718398_0','12207268_0','E105244287_0','E102095465_0','E102613189_0','12961059_0','11907521_0','E105311123_0','12552705_0','E100610622_0','12975280_0','E105918926_0','E103020139_1','E101069048_1','E105427046_0','13028374_0','E100262351_0','12302576_0','12451831_0','E102929168_0','E100383453_0','E105344747_0','12569826_0','E100168661_0','12530153_0','E104697262_0']\n",
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "feat_count.pop(\"homogeneous texture\")\n",
    "#feat_count.pop(\"central scar\")\n",
    "all_features = sorted(list(feat_count.keys()))\n",
    "cls_features = {f: [c for c in C.cls_names if f in features_by_cls[c]] for f in all_features}\n",
    "\n",
    "Z_features = cnna.get_annotated_files(features_by_cls)\n",
    "Z_features.pop(\"homogeneous texture\")\n",
    "#Z_features.pop(\"central scar\")\n",
    "\n",
    "num_features = len(all_features)\n",
    "\n",
    "voi_df = drm.get_voi_dfs()[0]\n",
    "M = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:41:23.282420Z",
     "start_time": "2018-05-14T16:41:18.726Z"
    },
    "collapsed": true
   },
   "source": [
    "M.layers[5].get_weights()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T18:39:40.526930Z",
     "start_time": "2018-05-16T18:39:40.522920Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def memory():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    print('Memory use:', py.memory_info()[0]/2.**30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T22:53:06.175337Z",
     "start_time": "2018-05-16T22:53:06.168343Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_num = 26\n",
    "model_path = join(C.model_dir, \"model_reader_new%d.hdf5\" % model_num)\n",
    "inf_xls_path = 'D:\\\\feature_analysis\\\\influence%d.xlsx' % model_num\n",
    "\n",
    "df = pd.DataFrame(columns=[\"true_cls\", \"pred_cls\"] + all_features)\n",
    "#df = pd.read_excel(inf_xls_path, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T18:40:01.196009Z",
     "start_time": "2018-05-16T18:40:00.906267Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_full = np.concatenate([orig_data_dict[cls][-1] for cls in C.cls_names],0)\n",
    "\n",
    "all_imgs = []\n",
    "all_cls = []\n",
    "\n",
    "for lesion_id in Z_full:\n",
    "    cls = voi_df.loc[lesion_id][\"cls\"]\n",
    "    img = np.load(join(C.orig_dir, cls, lesion_id+\".npy\"))\n",
    "    all_imgs.append(np.expand_dims(img,0))\n",
    "    all_cls.append(C.cls_names.index(cls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-16T22:53:20.252976Z",
     "start_time": "2018-05-16T22:53:17.016902Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(finf)\n",
    "del M, IA\n",
    "K.clear_session()\n",
    "M = keras.models.load_model(model_path)\n",
    "IA = finf.InfluenceAnalyzer(M, voi_df, all_imgs, all_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T14:26:17.670024Z",
     "start_time": "2018-05-17T01:11:44.778564Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "for lesion_id in Z_reader:\n",
    "    cls = voi_df.loc[lesion_id][\"cls\"]\n",
    "    img = np.load(join(C.orig_dir, cls, lesion_id+\".npy\"))\n",
    "    img = np.expand_dims(img,0)\n",
    "    pred_cls = M.predict(img)[0]\n",
    "    pred_cls = C.cls_names[list(pred_cls).index(pred_cls.max())]\n",
    "        \n",
    "    print(lesion_id)\n",
    "    \n",
    "    g_test = IA.get_grad(lesion_id, pred_cls=pred_cls)\n",
    "    s_test = IA.get_stest(g_test)\n",
    "\n",
    "    del M, IA\n",
    "    K.clear_session()\n",
    "    M = keras.models.load_model(model_path)\n",
    "    IA = finf.InfluenceAnalyzer(M, voi_df, all_imgs, all_cls)\n",
    "\n",
    "    I = {}\n",
    "    for f in Z_features:\n",
    "        I[f] = IA.get_avg_influence(Z_features[f], s_test)\n",
    "\n",
    "    del M, IA\n",
    "    K.clear_session()\n",
    "    M = keras.models.load_model(model_path)\n",
    "    IA = finf.InfluenceAnalyzer(M, voi_df, all_imgs, all_cls)\n",
    "    \n",
    "    df.loc[lesion_id] = [cls, pred_cls] + list(I[f] for f in df.columns[2:])\n",
    "\n",
    "    print(time.time() - t)\n",
    "    memory()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T14:26:50.959760Z",
     "start_time": "2018-05-17T14:26:50.955748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'D:\\\\feature_analysis\\\\influence26.xlsx'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inf_xls_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-17T14:26:59.477623Z",
     "start_time": "2018-05-17T14:26:59.395197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_excel(inf_xls_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T22:45:33.942966Z",
     "start_time": "2018-05-14T22:45:33.790216Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cls = voi_df.loc[lesion_id][\"cls\"]\n",
    "x_test = np.load(join(C.orig_dir, cls, lesion_id+\".npy\"))\n",
    "vis.draw_slices(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-15T15:38:55.913414Z",
     "start_time": "2018-05-15T15:38:37.999551Z"
    }
   },
   "source": [
    "Memory leak test\n",
    "for _ in range(10):\n",
    "    memory()\n",
    "    g_test = IA.get_grad(lesion_id)\n",
    "memory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### extra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:41:23.716962Z",
     "start_time": "2018-05-14T16:41:23.709943Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def perturb_weights(W_true, t):\n",
    "    eps = 1e-5\n",
    "    W = copy.deepcopy(W_true)\n",
    "    t_ix = 0\n",
    "    \n",
    "    for w_ix in range(len(W)):\n",
    "        W[w_ix] += eps * np.reshape(t[t_ix:t_ix+W[w_ix].size], W[w_ix].shape)\n",
    "        t_ix += W[w_ix].size\n",
    "        \n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:41:23.950594Z",
     "start_time": "2018-05-14T16:41:23.903360Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_grad(lesion_id, perturb_W=None):\n",
    "    C = config.Config()\n",
    "    \n",
    "    cls = voi_df.loc[lesion_id][\"cls\"]\n",
    "    img = np.load(join(C.orig_dir, cls, lesion_id+\".npy\"))\n",
    "    img = np.expand_dims(img,0)\n",
    "\n",
    "    y_true = np_utils.to_categorical(C.cls_names.index(cls), 6)\n",
    "    loss = K.categorical_crossentropy(y_true, M.output)\n",
    "    \n",
    "    with tf.device('/gpu:0'):\n",
    "        g = K.gradients(loss, M.trainable_weights)\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        g_i = sess.run(g, feed_dict={M.input:img, K.learning_phase():0})\n",
    "        if perturb_W is not None:\n",
    "            feed_dict_plus = {M.trainable_weights[i]:perturb_W[i] for i in range(len(perturb_W))}\n",
    "            g_i_plus = sess.run(g, feed_dict={**feed_dict_plus, M.input:img, K.learning_phase():0})\n",
    "\n",
    "    g_i = np.concatenate([x.flatten() for x in g_i], 0)\n",
    "    \n",
    "    if perturb_W is not None:\n",
    "        g_i_plus = np.concatenate([x.flatten() for x in g_i_plus], 0)\n",
    "        return g_i, g_i_plus\n",
    "    \n",
    "    return g_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:41:24.068235Z",
     "start_time": "2018-05-14T16:41:24.062220Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_grads(Z_sample):\n",
    "    eps = 1e-5\n",
    "    Ht = np.zeros(g_test.shape)\n",
    "    feed_dict_plus = {M.trainable_weights[i]:perturb_W[i] for i in range(len(perturb_W))}\n",
    "    losses = [K.categorical_crossentropy(y_true, M.output) for y_true in \\\n",
    "              [np_utils.to_categorical(i, 6) for i in range(6)]]\n",
    "        \n",
    "    t = time.time()\n",
    "    with tf.device('/gpu:0'):\n",
    "        g = [K.gradients(loss, M.trainable_weights) for loss in losses]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for ix in range(len(Z_sample)):\n",
    "            g_i = sess.run(g[classes[ix]], feed_dict={M.input:imgs[ix], K.learning_phase():0})\n",
    "            g_i = np.concatenate([x.flatten() for x in g_i], 0)\n",
    "    if verbose:\n",
    "        print(time.time()-t)\n",
    "    \n",
    "    return Ht / len(Z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:47:12.468765Z",
     "start_time": "2018-05-14T16:47:12.429514Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_HVP(perturb_W, Z_sample, verbose=True):\n",
    "    eps = 1e-5\n",
    "    Ht = np.zeros(g_test.shape)\n",
    "    feed_dict_plus = {M.trainable_weights[i]:perturb_W[i] for i in range(len(perturb_W))}\n",
    "    losses = [K.categorical_crossentropy(y_true, M.output) for y_true in \\\n",
    "              [np_utils.to_categorical(i, 6) for i in range(6)]]\n",
    "        \n",
    "    t = time.time()\n",
    "    with tf.device('/gpu:0'):\n",
    "        g = [K.gradients(loss, M.trainable_weights) for loss in losses]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for ix in range(len(Z_sample)):\n",
    "            g_i = sess.run(g[classes[ix]], feed_dict={M.input:imgs[ix], K.learning_phase():0})\n",
    "            g_i_plus = sess.run(g[classes[ix]], feed_dict={**feed_dict_plus, M.input:imgs[ix], K.learning_phase():0})\n",
    "    \n",
    "            g_i = np.concatenate([x.flatten() for x in g_i], 0)\n",
    "            g_i_plus = np.concatenate([x.flatten() for x in g_i_plus], 0)\n",
    "\n",
    "            Ht += (g_i_plus - g_i)/eps\n",
    "    if verbose:\n",
    "        print(time.time()-t)\n",
    "    \n",
    "    return Ht / len(Z_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:46:54.464603Z",
     "start_time": "2018-05-14T16:46:54.461595Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "t_k = np.zeros(g_test.shape)\n",
    "r_k = g_test#-Ht\n",
    "p_k = r_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T16:59:44.825692Z",
     "start_time": "2018-05-14T16:47:36.093846Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "num_iters = 15\n",
    "#phi_hist = []\n",
    "t = time.time()\n",
    "for _ in range(num_iters):\n",
    "    #Z_sample = random.sample(list(Z_full), 15)\n",
    "    W_new = perturb_weights(W, p_k)\n",
    "    Hp = get_HVP(W_new, Z_full)\n",
    "\n",
    "    alpha = np.dot(r_k, r_k) / np.dot(p_k, Hp)\n",
    "    t_k += alpha * p_k\n",
    "    r_k2 = r_k - alpha * Hp\n",
    "    #phi_hist = .5*np.dot(t_k, Ht) - g_test\n",
    "\n",
    "    beta = np.dot(r_k2, r_k2) / np.dot(r_k, r_k)\n",
    "    r_k = r_k2\n",
    "    p_k = r_k + beta*p_k\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T17:02:07.903731Z",
     "start_time": "2018-05-14T17:02:07.899692Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "del p_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T17:11:21.860049Z",
     "start_time": "2018-05-14T17:11:21.834986Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_grads(imgs, classes, s_test, verbose=True):\n",
    "    eps = 1e-5\n",
    "    Ht = np.zeros(g_test.shape)\n",
    "    losses = [K.categorical_crossentropy(y_true, M.output) for y_true in \\\n",
    "              [np_utils.to_categorical(i, 6) for i in range(6)]]\n",
    "        \n",
    "    I = 0\n",
    "    t = time.time()\n",
    "    with tf.device('/gpu:0'):\n",
    "        g = [K.gradients(loss, M.trainable_weights) for loss in losses]\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        for ix in range(len(classes)):\n",
    "            g_i = sess.run(g[classes[ix]], feed_dict={M.input:imgs[ix], K.learning_phase():0})\n",
    "            g_i = np.concatenate([x.flatten() for x in g_i], 0)\n",
    "            I -= np.dot(g_i, s_test)\n",
    "    if verbose:\n",
    "        print(time.time()-t)\n",
    "    \n",
    "    return I / len(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "### others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:43:33.458882Z",
     "start_time": "2018-05-14T20:43:33.438831Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "df_0 = hf.pickle_load(join(C.base_dir, \"data\", \"DFs4.bin\"))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:28:44.605752Z",
     "start_time": "2018-05-14T20:28:44.576671Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\Clinton\\Documents\\voi-classifier\\data\"\n",
    "answer_key = join(data_dir, \"ground_truth.xlsx\")\n",
    "answer_key = pd.read_excel(answer_key, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:42:25.459019Z",
     "start_time": "2018-05-14T20:42:25.454004Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['true_cls', 'feature_1', 'feature_2', 'feature_3', 'feature_4',\n",
       "       'ignore_1', 'ignore_2', 'BLANK', 'Unnamed: 9', 'Class', 'Num features'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer_key.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:45:12.644340Z",
     "start_time": "2018-05-14T20:45:12.638323Z"
    },
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": [
    "for z_test, row in answer_key.iterrows():\n",
    "    cls = df_0.loc[z_test, \"pred_cls\"]\n",
    "    feats = row[[f for f in answer_key.columns if f.startswith('feat')]].dropna().values\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-14T20:45:36.706311Z",
     "start_time": "2018-05-14T20:45:36.702299Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nodular growth', 'heterogeneous lesion',\n",
       "       'progressive enhancement'], dtype=object)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feats.dropna().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hidden": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "529px",
    "left": "1032.27px",
    "right": "20px",
    "top": "116px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
