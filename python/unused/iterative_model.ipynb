{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import NMF\n",
    "\n",
    "from keras.models import Model\n",
    "import keras.models\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from keras import backend as K\n",
    "\n",
    "import copy\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import config\n",
    "import csv\n",
    "import niftiutils.helper_fxns as hf\n",
    "import niftiutils.transforms as tr\n",
    "import importlib\n",
    "import itertools\n",
    "from math import sqrt, log, pi, exp\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange, vectorize, guvectorize\n",
    "from numpy import matmul, diag\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "from os.path import *\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "C = config.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"model_no_pad.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305\n",
    "model_dense_outputs = cbuild.build_pretrain_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 24, 24, 12, 3)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([orig_data_dict['cyst'][0][0]]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "filters_by_cls = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "filter_results = np.concatenate([filters_by_cls[cls] for cls in C.classes_to_include], axis=0)\n",
    "\n",
    "filter_avgs = np.mean(filter_results, axis=0)\n",
    "filter_stds = np.std(filter_results, axis=0)\n",
    "\n",
    "#filter_cls_avg_unscaled = {cls: np.mean(filter_results[cls], axis=0) for cls in C.classes_to_include}\n",
    "#filter_cls_avg_scaled = {cls: np.mean(filter_results[cls], axis=0) / filter_avgs for cls in C.classes_to_include}\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "feat_count.pop(\"central scar\")\n",
    "all_features = list(feat_count.keys())\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = dict(zip(*np.unique(features_by_cls['colorectal'], return_counts=True)))\n",
    "for k in z:\n",
    "    print(k, \" (\",z[k],\")\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "feat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def squash_per_dim(vector, eps = 10**-10):\n",
    "    for i,x in enumerate(vector):\n",
    "        vector[i] *= x / (1 + x**2)\n",
    "    return vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[-3].get_weights()[0]\n",
    "bias = model.layers[-3].get_weights()[1]\n",
    "\n",
    "gamma, beta, mu, var = model.layers[-2].get_weights()\n",
    "\n",
    "eff_bias = (np.zeros(6) + bias - mu) / var**.5 * gamma + beta\n",
    "eff_mult = (np.ones(6) + bias - mu) / var**.5 * gamma + beta - eff_bias\n",
    "\n",
    "W_eff = W * eff_mult# + eff_bias\n",
    "\n",
    "unit_relevance = np.empty(num_units)\n",
    "for u_ix in range(num_units):\n",
    "    unit_relevance[u_ix] = np.amax(W_eff[u_ix]) - np.amin(W_eff[u_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_filter_results = np.apply_along_axis(lambda X: squash_per_dim(X), 0, filter_results / (2*filter_avgs) * unit_relevance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_features = cnna.get_annotated_files(features_by_cls)\n",
    "Z_features.pop(\"central scar\")\n",
    "\n",
    "feature_filter_means = {}\n",
    "feature_filter_stds = {}\n",
    "feature_filters_scaled = {}#{cls: {} for cls in features}\n",
    "feature_filters = {f:np.empty([0,100]) for f in all_features}\n",
    "\n",
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs\n",
    "        \n",
    "    feature_filters[f] = (feature_filters[f] - filter_avgs) / filter_stds\n",
    "    \n",
    "    feature_filter_means[f] = np.mean(feature_filters[f], axis=0)\n",
    "    feature_filter_stds[f] = np.std(feature_filters[f], axis=0)\n",
    "\n",
    "    feature_filters_scaled[f] = feature_filter_means[f] / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(all_features) # number of features\n",
    "num_units = 100 # number of dense units\n",
    "\n",
    "num_annotations = 8\n",
    "Z_test = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "indices_f = [orig_data_dict[cls][1] for cls in C.classes_to_include]\n",
    "indices_f = hf.flatten(indices_f)\n",
    "\n",
    "fixed_indices = np.empty([num_features, num_annotations])\n",
    "for f_ix,f in enumerate(all_features):\n",
    "    fixed_indices[f_ix, :] = np.where(np.isin(indices_f, random.sample(set(Z_features[f]), num_annotations)))[0]\n",
    "fixed_indices = fixed_indices.astype(int)\n",
    "\n",
    "test_indices = np.where(np.isin(indices_f, Z_test))[0]\n",
    "\n",
    "z_states = np.array([z for z in itertools.product([0,1], repeat=num_features) if sum(z) <= 4 and sum(z) >= 2])\n",
    "num_states = len(z_states)\n",
    "num_imgs = filter_results.shape[0]\n",
    "z_states_bool = [tuple([bool(x) for x in z]) for z in z_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM_theta(k, alpha=.5, beta=.5, **kwargs):\n",
    "    if len(kwargs) == 0:\n",
    "        mu = np.empty([num_features, num_units])\n",
    "        sigma = np.empty([num_features, num_units])\n",
    "        for i in range(num_features):\n",
    "            mu[i] = feature_filter_means[all_features[i]]\n",
    "            sigma[i] = feature_filter_stds[all_features[i]]\n",
    "        sigma[sigma > 1] = 1\n",
    "        m = filter_avgs\n",
    "        s = filter_stds\n",
    "\n",
    "        theta_i = scipy.random.normal(size=num_features)\n",
    "        theta_ij = scipy.random.normal(size=[num_features-1, num_features])\n",
    "    else:\n",
    "        mu = kwargs['mu']\n",
    "        sigma = kwargs['sigma']\n",
    "        m = kwargs['m']\n",
    "        s = kwargs['s']\n",
    "        theta_i = kwargs['theta_i']\n",
    "        theta_ij = kwargs['theta_ij']\n",
    "    \n",
    "    print(\"Initializing probabilities...\")\n",
    "    p_x_z = im.get_all_p_x_z(mu, m, sigma, s, z_states, filter_results, fixed_indices)\n",
    "    p_z = im.get_p_z(z_states, theta_i, theta_ij)\n",
    "    p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "    \n",
    "    print(\"Running EM:\")\n",
    "    for jj in range(k):\n",
    "        print(\"   Iteration\", jj, end=\"...Update thetas...\")\n",
    "        theta_i, theta_ij = im.update_thetas(np.sum(p_z_x, axis=0), z_states_bool, theta_i, theta_ij, alpha)\n",
    "\n",
    "        p_z = im.get_p_z(z_states, theta_i, theta_ij)\n",
    "        p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "\n",
    "        print(\"Update means and stdevs...\", end=\"\")\n",
    "        mu_est = im.update_mus(mu, m, sigma, s, z_states, filter_results, p_z_x, fixed_indices, beta)\n",
    "        m_est = im.update_ms(mu, m, sigma, s, z_states, filter_results, p_z_x, beta)\n",
    "        sigma, s = im.update_stdevs_approx(mu, m, sigma, s, z_states, filter_results, p_z_x, beta)\n",
    "\n",
    "        if jj < k-1:\n",
    "            print(\"Update probabilities...\")\n",
    "            p_x_z = im.get_all_p_x_z(mu, m, sigma, s, z_states, filter_results, fixed_indices)\n",
    "            p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "            \n",
    "    return mu, m, sigma, s, theta_i, theta_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'mu': mu,\n",
    "'sigma': sigma,\n",
    "'m': m,\n",
    "'s': s,\n",
    "'theta_i': theta_i,\n",
    "'theta_ij': theta_ij}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing probabilities...\n",
      "Running EM:\n",
      "   Iteration 0...Update thetas...Update means and stdevs...Update probabilities...\n",
      "   Iteration 1...Update thetas...Update means and stdevs...Update probabilities...\n",
      "   Iteration 2...Update thetas...Update means and stdevs...Update probabilities...\n",
      "   Iteration 3...Update thetas...Update means and stdevs...Update probabilities...\n",
      "   Iteration 4...Update thetas...Update means and stdevs..."
     ]
    }
   ],
   "source": [
    "mu, m, sigma, s, theta_i, theta_ij = EM_theta(5, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importlib.reload(im)\n",
    "p_x_z = im.get_all_p_x_z(mu, m, sigma, s, z_states, filter_results, fixed_indices)\n",
    "p_z = im.get_p_z(z_states, theta_i, theta_ij)\n",
    "p_z_x = im.get_all_p_z_x(p_x_z, p_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for _ in range(num_states*10):\n",
    "    np.nan_to_num(0)\n",
    "    #pass\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for test_ix in test_indices[3:]:\n",
    "    p_zi_x = np.zeros([num_features])\n",
    "    for f_ix in range(num_features):\n",
    "        state_ixs = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "        p_zi_x[f_ix] = sum(p_z_x[test_ix, state_ixs])\n",
    "        #for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "        #    output[z] = output[z] + [f, strength]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arterial enhancement 82%\n",
      "thin well-defined walls 74%\n",
      "heterogeneous 62%\n",
      "continuous enhancing rim 56%\n"
     ]
    }
   ],
   "source": [
    "for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "    if strength<0.3:\n",
    "        break\n",
    "    print(\"%s %d%%\" % (all_features[f], strength*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAACFCAYAAABsdIKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQxYRmQQdsojIJOiQRUQm4T+WenD2fSyHOQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1bb084a6588>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for cls in C.classes_to_include:\n",
    "    x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == indices_f[test_ix])]\n",
    "    if len(x_test_quick) > 0:\n",
    "        break\n",
    "hf.draw_slices(x_test_quick[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@njit\n",
    "def update_stdevs(mu, m, sigma, s, z_states, filter_results, p_z_x):\n",
    "    sigma_est = np.empty([num_features, num_units])\n",
    "    s_est = np.empty([num_units])\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "\n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in range(num_states):\n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - \\\n",
    "                       np.dot(mu[:, u_ix], z_states_bool[state_ix]) - m[u_ix])**2 for img_ix in range(num_imgs)]) / 2\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "        temp = scipy.optimize.minimize(\\\n",
    "                    lambda Var: sum([a_i[state_ix] / (sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) + c_i[state_ix] * log(sqrt(sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) * sqrt(2*pi)) for state_ix in range(num_states)]),\n",
    "                      np.concatenate([sigma[:, u_ix]**2, [s[u_ix]**2]]), \n",
    "                      bounds=tuple(itertools.repeat((.001, 1000),num_features+1)))\n",
    "\n",
    "        sigma_est[:, u_ix] = [sqrt(i) for i in temp['x'][:-1]]\n",
    "        s_est[u_ix] = sqrt(temp['x'][-1])\n",
    "    \n",
    "    return sigma_est, s_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([a_i[state_ix]/(x+var_adj[state_ix])**2 - \\\n",
    "                                         c_i[state_ix]/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "#state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "#p_z_x does not need to be rescaled because the factor cancels out\n",
    "var_adj = np.zeros([num_states])\n",
    "a_i = np.zeros([num_states])\n",
    "c_i = np.zeros([num_states])\n",
    "\n",
    "for u_ix in range(num_units):\n",
    "    for state_ix in range(num_states):\n",
    "        mean = np.dot(mu[:, u_ix], z_states[state_ix]) + m[u_ix]\n",
    "        a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "        c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "    temp = scipy.optimize.linearmixing(\\\n",
    "                lambda Var: sum([a_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2)**2 - \\\n",
    "                 c_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2) for state_ix in range(num_states)]), \\\n",
    "                  np.concatenate([sigma[:, u_ix], [s[u_ix]]]), verbose=True, maxiter=10000)\n",
    "    \n",
    "    #sigma_est[:, u_ix]\n",
    "    if u_ix % 20 == 0:\n",
    "        print(u_ix, time.time()-t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([(a_i[state_ix]/(x+var_adj[state_ix]) - \\\n",
    "                                         c_i[state_ix])/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_test_features = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares approach\n",
    "Theta = np.array([feature_filter_means[f] for f in all_features])\n",
    "Theta = np.transpose(Theta, (1,0))\n",
    "\n",
    "filters_test = {}\n",
    "features_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)\n",
    "    filters_test[cls] = (filters_test[cls] - filter_avgs) / filter_stds\n",
    "    features_test[cls] = np.linalg.lstsq(Theta, np.transpose(filters_test[cls], (1,0)))[0]\n",
    "    #filters_test[cls] = np.apply_along_axis(lambda x: x / filter_avgs, 1, filters_test[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covar_to_corr(covar):\n",
    "    A = np.diag(np.diag(covar)**(-0.5))\n",
    "    return np.matmul(np.matmul(A, covar), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = covar_to_corr(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(corr - np.linalg.pinv(np.diag(np.diag(np.linalg.pinv(corr)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cutoff_eigenval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.random.normal(size=filter_results.shape)\n",
    "dummy_fa = FactorAnalysis()\n",
    "dummy_fa.fit(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.linalg.eigvals(fa.get_covariance()) > np.mean(np.linalg.eigvals(dummy_fa.get_covariance())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=10,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.fit(filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89081373261106078"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=3\n",
    "fa.get_covariance()[x,x] - fa.noise_variance_[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = cnna.get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))\n",
    "\n",
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "for cls in C.classes_to_include:\n",
    "    for img_num in range(len(filters_test[cls])):\n",
    "        z = Z_test[cls][img_num]\n",
    "        x = np.expand_dims(x_test[cls][img_num], axis=0)\n",
    "        evidence = {}\n",
    "        \n",
    "        output[z] = [cls]\n",
    "        \n",
    "        preds = model.predict(x, verbose=False)[0]\n",
    "        for pred_cls, pred_conf in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:2]:\n",
    "            output[z] = output[z] + [pred_cls]\n",
    "        \n",
    "        #for f in all_features:\n",
    "        #    evidence[f + \"/\" + str(cls_features[f])] = get_evidence_strength(feature_filters[f], filters_test[cls][img_num])\n",
    "        \n",
    "        for i in range(len(all_features)):\n",
    "            evidence[all_features[i] + \"/\" + str(cls_features[all_features[i]])] = features_test[cls][i, img_num]\n",
    "        \n",
    "        f1='infiltrative'\n",
    "        f2='lobulated margins'\n",
    "        if evidence[f1 + \"/\" + str(cls_features[f1])] < evidence[f2 + \"/\" + str(cls_features[f2])]:\n",
    "            evidence.pop(f1 + \"/\" + str(cls_features[f1]))\n",
    "        else:\n",
    "            evidence.pop(f2 + \"/\" + str(cls_features[f2]))\n",
    "        \n",
    "        for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True):\n",
    "            output[z] = output[z] + [f, strength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred5.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_cls2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_features)):\n",
    "        writer.writerow([Z_test_features[z_num]] + [output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][3], \\\n",
    "                        output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][5]] + output[Z_test_features[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:\\\\filters_pred3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "for _,row in df.iterrows():\n",
    "    if row[\"pred_cls1\"] in row[\"feature_1\"]:\n",
    "        agree += 1\n",
    "print(agree/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
