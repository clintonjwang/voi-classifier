{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.models\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from keras import backend as K\n",
    "\n",
    "import argparse\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import csv\n",
    "import niftiutils.helper_fxns as hf\n",
    "import importlib\n",
    "import inference_methods as im\n",
    "import itertools\n",
    "from math import sqrt, log, pi, exp\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange, vectorize, guvectorize\n",
    "from numpy import matmul, diag\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config()\n",
    "T = config.Hyperparams()\n",
    "#T.steps_per_epoch = 50\n",
    "#T.epochs= 1\n",
    "#T.get_best_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crun.run_fixed_hyperparams([C], hyperparams=T)#C_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = cbuild.build_cnn_hyperparams(T)\n",
    "model.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_reader = ['E103312835_1','12823036_0','12569915_0','E102093118_0','E102782525_0','12799652_0','E100894274_0','12874178_3','E100314676_0','12842070_0','13092836_2','12239783_0','12783467_0','13092966_0','E100962970_0','E100183257_1','E102634440_0','E106182827_0','12582632_0','E100121654_0','E100407633_0','E105310461_0','12788616_0','E101225606_0','12678910_1','E101083458_1','12324408_0','13031955_0','E101415263_0','E103192914_0','12888679_2','E106096969_0','E100192709_1','13112385_1','E100718398_0','12207268_0','E105244287_0','E102095465_0','E102613189_0','12961059_0','11907521_0','E105311123_0','12552705_0','E100610622_0','12975280_0','E105918926_0','E103020139_1','E101069048_1','E105427046_0','13028374_0','E100262351_0','12302576_0','12451831_0','E102929168_0','E100383453_0','E105344747_0','12569826_0','E100168661_0','12530153_0','E104697262_0']\n",
    "X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4, n_art=0)#, Z_test_fixed=Z_reader)\n",
    "Z_test, Z_train_orig = Z\n",
    "X_train_orig, Y_train_orig = train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.utils import multi_gpu_model\n",
    "#model = multi_gpu_model(model, gpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_generator, steps_per_epoch=T.steps_per_epoch, epochs=T.epochs, validation_data=[X_test, Y_test])#, callbacks=[T.early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('E:\\\\models\\\\model_reader_rcnn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dr_methods as drm\n",
    "voi_df_art, voi_df_ven, voi_df_eq = drm.get_voi_dfs()\n",
    "small_voi_df = pd.read_csv(C.small_voi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_with_bbox(fn_list[2], cls_mapping[wrong_guesses[2]])\n",
    "Y_pred = model.predict(X_test)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_test])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "#save_output(Z_test, y_pred, y_true)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "#y_true_simp, y_pred_simp, _ = cnna.merge_classes(y_true, y_pred)\n",
    "#print(accuracy_score(y_true_simp, y_pred_simp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cnn_builder' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\cnn_builder.py'>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(cbuild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4, n_art=0, Z_test=Z_reader)\n",
    "Z_test, Z_train_orig = Z\n",
    "Y_pred = model.predict(X_test)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_test])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "y_true_simp, y_pred_simp, _ = cbuild.merge_classes(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = [C.classes_to_include[y] for y in y_pred]\n",
    "Y_true = [C.classes_to_include[y] for y in y_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame([Z_test,Y_pred,Y_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.to_csv('E:\\\\temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc6cls = []\n",
    "acc3cls = []\n",
    "\n",
    "for i in range(19):\n",
    "    model_num = 306+i\n",
    "    X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4, n_art=0, Z_test=df[df['model_num'] == model_num][\"z_test\"])\n",
    "    X_train_orig, Y_train_orig = train_orig\n",
    "    model = keras.models.load_model(os.path.join(C.model_dir, \"models_%d.hdf5\" % model_num)) #models_305\n",
    "    \n",
    "    Y_pred = model.predict(X_train_orig)\n",
    "    y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_train_orig])\n",
    "    y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "    acc6cls.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "    y_true_simp, y_pred_simp, _ = cbuild.merge_classes(y_true, y_pred)\n",
    "\n",
    "    acc3cls.append(accuracy_score(y_true_simp, y_pred_simp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Z:\\\\Inter-reader study\\\\Answer key.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = df[\"Class\"].values\n",
    "y_pred = df[\"Model\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, labels=C.classes_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\t0\t1\t0\t0\t1\n",
      "0\t9\t1\t0\t0\t0\n",
      "0\t1\t8\t0\t0\t0\n",
      "0\t0\t0\t10\t0\t0\n",
      "0\t0\t0\t0\t9\t0\n",
      "0\t0\t0\t0\t1\t9\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cm)):\n",
    "    print('\\t'.join(cm[:,i].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_list = fn_list + list(Z_test[~np.equal(y_pred, y_true)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_list = fn_list + list(Z_train_orig[~np.equal(y_pred, y_true)])\n",
    "fn_list = fn_list + list(Z_train_orig[~np.equal(y_pred, y_true)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_x_list = [x+\"(\"+str(voi_df_art[voi_df_art[\"id\"] == x[:-4]][\"x1\"].values[0])+\")\" for x in fn_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(fn_x_list, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_train_orig)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_train_orig])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "#cbuild.save_output(Z_train_orig, y_pred, y_true)#, save_dir=C.output_img_dir+\"\\\\training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cnn_builder' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\cnn_builder.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(cbuild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dropout = cbuild.build_model_forced_dropout(model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cnna.visualize_layer(model, 'conv3d_148')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "filters_by_cls = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "fig = cnna.tsne(filters_by_cls)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hf.flatten([list(feature_sheet[\"evidence1\"+cls].dropna().values) for cls in C.classes_to_include]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(cbuild)\n",
    "C=config.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305\n",
    "model_dense_outputs = cbuild.build_pretrain_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "filters_by_cls = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "filter_results = np.concatenate([filters_by_cls[cls] for cls in C.classes_to_include], axis=0)\n",
    "\n",
    "filter_avgs = np.mean(filter_results, axis=0)\n",
    "filter_stds = np.std(filter_results, axis=0)\n",
    "\n",
    "#filter_cls_avg_unscaled = {cls: np.mean(filter_results[cls], axis=0) for cls in C.classes_to_include}\n",
    "#filter_cls_avg_scaled = {cls: np.mean(filter_results[cls], axis=0) / filter_avgs for cls in C.classes_to_include}\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "all_features = list(feat_count.keys())\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = dict(zip(*np.unique(features_by_cls['colorectal'], return_counts=True)))\n",
    "for k in z:\n",
    "    print(k, \" (\",z[k],\")\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "feat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_features = cnna.get_annotated_files(features_by_cls)\n",
    "\n",
    "feature_filter_means = {}\n",
    "feature_filter_stds = {}\n",
    "feature_filters_scaled = {}#{cls: {} for cls in features}\n",
    "feature_filters = {f:np.empty([0,100]) for f in all_features}\n",
    "\n",
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs\n",
    "        \n",
    "    feature_filters[f] = (feature_filters[f] - filter_avgs) / filter_stds\n",
    "    \n",
    "    feature_filter_means[f] = np.mean(feature_filters[f], axis=0)# / filter_avgs\n",
    "    feature_filter_stds[f] = np.std(feature_filters[f], axis=0)# / filter_avgs\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > np.mean(ff), ff, 0)\n",
    "\n",
    "    #ff = feature_filters[cls][f]\n",
    "    #feature_filters[cls][f] = np.where(ff > 1*filter_cls_avg_unscaled[cls], ff, 0)\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > 1.3*filter_avgs, ff, 0)\n",
    "\n",
    "    feature_filters_scaled[f] = feature_filter_means[f] / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(all_features) # number of features\n",
    "num_units = 100 # number of dense units\n",
    "\n",
    "num_annotations = 5\n",
    "Z_test = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "indices_f = [orig_data_dict[cls][1] for cls in C.classes_to_include]\n",
    "indices_f = hf.flatten(indices_f)\n",
    "\n",
    "fixed_indices = np.empty([num_features, num_annotations])\n",
    "for f_ix,f in enumerate(all_features):\n",
    "    fixed_indices[f_ix, :] = np.where(np.isin(indices_f, random.sample(set(Z_features[f]), num_annotations)))[0]\n",
    "fixed_indices = fixed_indices.astype(int)\n",
    "\n",
    "test_indices = np.where(np.isin(indices_f, Z_test))[0]\n",
    "\n",
    "z_states = np.array([z for z in itertools.product([0,1], repeat=num_features) if sum(z) <= 4 and sum(z) >= 2])\n",
    "num_states = len(z_states)\n",
    "num_imgs = filter_results.shape[0]\n",
    "z_states_bool = [tuple([bool(x) for x in z]) for z in z_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EM_theta(k, k2=None, alpha=.5, beta=.5, **kwargs):\n",
    "    if len(kwargs) == 0:\n",
    "        mu = np.empty([num_features, num_units])\n",
    "        sigma = np.empty([num_features, num_units])\n",
    "        for i in range(num_features):\n",
    "            mu[i] = feature_filter_means[all_features[i]]\n",
    "            sigma[i] = feature_filter_stds[all_features[i]]\n",
    "        sigma[sigma > 1] = 1\n",
    "        m = filter_avgs\n",
    "        s = filter_stds\n",
    "\n",
    "        theta_i = scipy.random.normal(size=num_features)\n",
    "        theta_ij = scipy.random.normal(size=[num_features-1, num_features])\n",
    "    else:\n",
    "        mu = kwargs['mu']\n",
    "        sigma = kwargs['sigma']\n",
    "        m = kwargs['m']\n",
    "        s = kwargs['s']\n",
    "        theta_i = kwargs['theta_i']\n",
    "        theta_ij = kwargs['theta_ij']\n",
    "    \n",
    "    print(\"Initializing probabilities...\")\n",
    "    p_x_z = im.get_all_p_x_z(mu, m, sigma, s, z_states, filter_results, fixed_indices)\n",
    "    p_z = im.get_p_z(z_states, theta_i, theta_ij)\n",
    "    p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "    \n",
    "    print(\"Running EM...\")\n",
    "    if k2 is None:\n",
    "        for jj in range(k):\n",
    "            print(\"   \", jj, end=\"...Update thetas...\")\n",
    "            theta_i, theta_ij = im.update_thetas(np.sum(p_z_x, axis=0), z_states_bool, theta_i, theta_ij, alpha)\n",
    "\n",
    "            p_z = im.get_p_z(z_states, theta_i, theta_ij)\n",
    "            p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "\n",
    "            print(\"Update means and stdevs...\", end=\"\")\n",
    "            mu_est = im.update_mus(mu, m, sigma, s, z_states, filter_results, p_z_x, beta)\n",
    "            m_est = im.update_ms(mu, m, sigma, s, z_states, filter_results, p_z_x, beta)\n",
    "            sigma, s = im.update_stdevs_approx(mu, m, sigma, s, z_states, filter_results, p_z_x, beta)\n",
    "\n",
    "            print(\"Update probabilities...\")\n",
    "            p_x_z = im.get_all_p_x_z(mu, m, sigma, s, z_states, filter_results, fixed_indices)\n",
    "            p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "    \n",
    "    else:\n",
    "        for kk in range(k2):\n",
    "            print(kk, \"=======\")\n",
    "            for jj in range(k):\n",
    "                print(\"   \", jj)\n",
    "                theta_i, theta_ij = im.update_thetas(np.sum(p_z_x, axis=0), z_states_bool, theta_i, theta_ij, alpha)\n",
    "\n",
    "                p_z = im.get_p_z(z_states, theta_i, theta_ij)\n",
    "                p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "\n",
    "                mu_est = im.update_mus(mu, m, sigma, s, z_states, filter_results, p_z_x, beta)\n",
    "                m_est = im.update_ms(mu, m, sigma, s, z_states, filter_results, p_z_x, beta)\n",
    "\n",
    "                p_x_z = im.get_all_p_x_z(mu, m, sigma, s, z_states, filter_results, fixed_indices)\n",
    "                p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "\n",
    "            print(\"Updating stdev...\")\n",
    "            sigma, s = im.update_stdevs_approx(mu, m, sigma, s, z_states, filter_results, p_z_x)\n",
    "\n",
    "            if kk != k2-1:\n",
    "                p_x_z = im.get_all_p_x_z(mu, m, sigma, s, z_states, filter_results, fixed_indices)\n",
    "                p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "\n",
    "    return mu, m, sigma, s, theta_i, theta_ij"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {'mu': mu,\n",
    "'sigma': sigma,\n",
    "'m': m,\n",
    "'s': s,\n",
    "'theta_i': theta_i,\n",
    "'theta_ij': theta_ij}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing probabilities...\n",
      "Running EM...\n",
      "    0...Update means...Update stdevs...    1...Update means...Update stdevs..."
     ]
    }
   ],
   "source": [
    "mu, m, sigma, s, theta_i, theta_ij = EM_theta(5, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importlib.reload(im)\n",
    "p_x_z = im.get_all_p_x_z(mu, m, sigma, s, z_states, filter_results, fixed_indices)\n",
    "p_z = im.get_p_z(z_states, theta_i, theta_ij)\n",
    "p_z_x = im.get_all_p_z_x(p_x_z, p_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for _ in range(num_states*10):\n",
    "    np.nan_to_num(0)\n",
    "    #pass\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for test_ix in test_indices[2:]:\n",
    "    p_zi_x = np.zeros([num_features])\n",
    "    for f_ix in range(num_features):\n",
    "        state_ixs = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "        p_zi_x[f_ix] = sum(p_z_x[test_ix, state_ixs])\n",
    "        #for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "        #    output[z] = output[z] + [f, strength]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arterial enhancement 49%\n",
      "venous washout 43%\n",
      "regular spherical hypointense mass 35%\n"
     ]
    }
   ],
   "source": [
    "for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "    if strength<0.3:\n",
    "        break\n",
    "    print(\"%s %d%%\" % (all_features[f], strength*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAACFCAYAAABsdIKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8F9/0U5gY+EJOwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b87bfa9240>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for cls in C.classes_to_include:\n",
    "    x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == indices_f[test_ix])]\n",
    "    if len(x_test_quick) > 0:\n",
    "        break\n",
    "hf.draw_slices(x_test_quick[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@njit\n",
    "def update_stdevs(mu, m, sigma, s, z_states, filter_results, p_z_x):\n",
    "    sigma_est = np.empty([num_features, num_units])\n",
    "    s_est = np.empty([num_units])\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "\n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in range(num_states):\n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - \\\n",
    "                       np.dot(mu[:, u_ix], z_states_bool[state_ix]) - m[u_ix])**2 for img_ix in range(num_imgs)]) / 2\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "        temp = scipy.optimize.minimize(\\\n",
    "                    lambda Var: sum([a_i[state_ix] / (sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) + c_i[state_ix] * log(sqrt(sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) * sqrt(2*pi)) for state_ix in range(num_states)]),\n",
    "                      np.concatenate([sigma[:, u_ix]**2, [s[u_ix]**2]]), \n",
    "                      bounds=tuple(itertools.repeat((.001, 1000),num_features+1)))\n",
    "\n",
    "        sigma_est[:, u_ix] = [sqrt(i) for i in temp['x'][:-1]]\n",
    "        s_est[u_ix] = sqrt(temp['x'][-1])\n",
    "    \n",
    "    return sigma_est, s_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([a_i[state_ix]/(x+var_adj[state_ix])**2 - \\\n",
    "                                         c_i[state_ix]/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "#state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "#p_z_x does not need to be rescaled because the factor cancels out\n",
    "var_adj = np.zeros([num_states])\n",
    "a_i = np.zeros([num_states])\n",
    "c_i = np.zeros([num_states])\n",
    "\n",
    "for u_ix in range(num_units):\n",
    "    for state_ix in range(num_states):\n",
    "        mean = np.dot(mu[:, u_ix], z_states[state_ix]) + m[u_ix]\n",
    "        a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "        c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "    temp = scipy.optimize.linearmixing(\\\n",
    "                lambda Var: sum([a_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2)**2 - \\\n",
    "                 c_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2) for state_ix in range(num_states)]), \\\n",
    "                  np.concatenate([sigma[:, u_ix], [s[u_ix]]]), verbose=True, maxiter=10000)\n",
    "    \n",
    "    #sigma_est[:, u_ix]\n",
    "    if u_ix % 20 == 0:\n",
    "        print(u_ix, time.time()-t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([(a_i[state_ix]/(x+var_adj[state_ix]) - \\\n",
    "                                         c_i[state_ix])/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_test_features = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares approach\n",
    "Theta = np.array([feature_filter_means[f] for f in all_features])\n",
    "Theta = np.transpose(Theta, (1,0))\n",
    "\n",
    "filters_test = {}\n",
    "features_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)\n",
    "    filters_test[cls] = (filters_test[cls] - filter_avgs) / filter_stds\n",
    "    features_test[cls] = np.linalg.lstsq(Theta, np.transpose(filters_test[cls], (1,0)))[0]\n",
    "    #filters_test[cls] = np.apply_along_axis(lambda x: x / filter_avgs, 1, filters_test[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covar_to_corr(covar):\n",
    "    A = np.diag(np.diag(covar)**(-0.5))\n",
    "    return np.matmul(np.matmul(A, covar), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = covar_to_corr(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(corr - np.linalg.pinv(np.diag(np.diag(np.linalg.pinv(corr)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cutoff_eigenval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.random.normal(size=filter_results.shape)\n",
    "dummy_fa = FactorAnalysis()\n",
    "dummy_fa.fit(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.linalg.eigvals(fa.get_covariance()) > np.mean(np.linalg.eigvals(dummy_fa.get_covariance())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=10,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.fit(filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89081373261106078"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=3\n",
    "fa.get_covariance()[x,x] - fa.noise_variance_[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = cnna.get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))\n",
    "\n",
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "for cls in C.classes_to_include:\n",
    "    for img_num in range(len(filters_test[cls])):\n",
    "        z = Z_test[cls][img_num]\n",
    "        x = np.expand_dims(x_test[cls][img_num], axis=0)\n",
    "        evidence = {}\n",
    "        \n",
    "        output[z] = [cls]\n",
    "        \n",
    "        preds = model.predict(x, verbose=False)[0]\n",
    "        for pred_cls, pred_conf in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:2]:\n",
    "            output[z] = output[z] + [pred_cls]\n",
    "        \n",
    "        #for f in all_features:\n",
    "        #    evidence[f + \"/\" + str(cls_features[f])] = get_evidence_strength(feature_filters[f], filters_test[cls][img_num])\n",
    "        \n",
    "        for i in range(len(all_features)):\n",
    "            evidence[all_features[i] + \"/\" + str(cls_features[all_features[i]])] = features_test[cls][i, img_num]\n",
    "        \n",
    "        f1='infiltrative'\n",
    "        f2='lobulated margins'\n",
    "        if evidence[f1 + \"/\" + str(cls_features[f1])] < evidence[f2 + \"/\" + str(cls_features[f2])]:\n",
    "            evidence.pop(f1 + \"/\" + str(cls_features[f1]))\n",
    "        else:\n",
    "            evidence.pop(f2 + \"/\" + str(cls_features[f2]))\n",
    "        \n",
    "        for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True):\n",
    "            output[z] = output[z] + [f, strength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred5.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_cls2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_features)):\n",
    "        writer.writerow([Z_test_features[z_num]] + [output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][3], \\\n",
    "                        output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][5]] + output[Z_test_features[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:\\\\filters_pred3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "for _,row in df.iterrows():\n",
    "    if row[\"pred_cls1\"] in row[\"feature_1\"]:\n",
    "        agree += 1\n",
    "print(agree/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
