{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T20:06:53.283788Z",
     "start_time": "2018-05-29T20:06:46.485202Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\dicom\\__init__.py:53: UserWarning: \n",
      "This code is using an older version of pydicom, which is no longer \n",
      "maintained as of Jan 2017.  You can access the new pydicom features and API \n",
      "by installing `pydicom` from PyPI.\n",
      "See 'Transitioning to pydicom 1.x' section at pydicom.readthedocs.org \n",
      "for more information.\n",
      "\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import glob\n",
    "import importlib\n",
    "import itertools\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from os.path import *\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "import keras.layers as layers\n",
    "import keras.models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import ddpg.env as denv\n",
    "import ddpg.main as qmain\n",
    "import ddpg.learning_nets as ln\n",
    "import ddpg.task_nets as tn\n",
    "import dr_methods as drm\n",
    "import seg_methods as sm\n",
    "import niftiutils.helper_fxns as hf\n",
    "import niftiutils.transforms as tr\n",
    "import niftiutils.visualization as vis\n",
    "import voi_methods as vm\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(3, suppress=True)\n",
    "#np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T20:06:54.697830Z",
     "start_time": "2018-05-29T20:06:54.692845Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def memory():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    gigs = py.memory_info()[0]/2.**30\n",
    "    print('Memory use:', gigs)\n",
    "    return gigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T20:06:55.689968Z",
     "start_time": "2018-05-29T20:06:55.683983Z"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(cbuild)\n",
    "C = config.Config()\n",
    "dqn_generator = cbuild._train_gen_ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-29T20:06:55.760Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Episode 0 Replay self.q_buffer 367\n",
      "D:\\Etiology\\imgs\\full_imgs\\4267520.npy\n",
      "\tAction [0.151 0.075 0.554 0.16  0.524 0.63  0.703 0.412 0.093 0.074] Reward -0.0\n",
      "\tAction [0.316 0.67  0.533 0.851 0.114 0.245 0.491 0.494 0.561 0.855] Reward -0.3\n",
      "\tAction [0.591 0.548 0.291 0.474 0.453 0.516 0.662 0.019 0.775 0.933] Reward -0.9\n",
      "\tAction [0.045 0.937 0.85  0.372 0.419 0.686 0.225 0.257 0.577 0.396] Reward -0.1\n",
      "\tAction [0.216 0.019 0.577 0.767 0.467 0.145 0.835 0.697 0.518 0.918] Reward -0.2\n",
      "\tAction [0.433 0.95  0.225 0.808 0.552 0.751 0.262 0.866 0.281 0.48 ] Reward -1.2\n",
      "\tAction [0.796 0.661 0.825 0.391 0.405 0.422 0.327 0.894 0.198 0.153] Reward -0.5\n",
      "\tAction [0.405 0.876 0.788 0.635 0.402 0.824 0.165 0.596 0.68  0.13 ] Reward -1.0\n",
      "\tAction [0.899 0.652 0.057 0.546 0.959 0.963 0.953 0.508 0.48  0.404] Reward -0.2\n",
      "\tAction [0.301 0.888 0.24  0.894 0.315 0.032 0.356 0.318 0.586 0.973] Reward -1.0\n",
      "Memory use: 4.337425231933594\n",
      "TOTAL REWARD: -5.6 (10 steps)\n",
      "\n",
      "Episode 1 Replay self.q_buffer 377\n",
      "D:\\Etiology\\imgs\\full_imgs\\4969947.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5041655.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4096056.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4969947.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4235279.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5173932.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4515877.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5173932.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4759616.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4162152.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\3966711.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4973789.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5122452.npy\n",
      "\tAction [0.965 0.227 0.488 0.687 0.06  0.52  0.814 0.279 0.854 0.294] Reward -1.6\n",
      "\tAction [0.426 0.328 0.13  0.728 0.244 0.116 0.587 0.475 0.478 0.656] Reward 0.9\n",
      "\tAction [0.695 0.137 0.896 0.892 0.159 0.443 0.898 0.264 0.886 0.798] Reward -0.5\n",
      "\tAction [0.51  0.012 0.522 0.909 0.568 0.498 0.529 0.127 0.243 0.353] Reward 0.5\n",
      "\tAction [0.855 0.451 0.958 0.73  0.608 0.233 0.829 0.408 0.347 0.841] Reward 1.4\n",
      "\tAction [0.758 0.528 0.606 0.057 0.703 0.229 0.394 0.309 0.344 0.762] Reward 0.1\n",
      "\tAction [0.72  0.339 0.881 0.032 0.911 0.498 0.16  0.77  0.117 0.915] Reward 0.4\n",
      "\tAction [0.805 0.261 0.582 0.3   0.566 0.065 0.672 0.573 0.051 0.334] Reward 0.2\n",
      "\tAction [0.311 0.144 0.096 0.623 0.529 0.798 0.801 0.883 0.208 0.492] Reward 1.4\n",
      "\tAction [0.654 0.228 0.746 0.79  0.191 0.572 0.222 0.144 0.597 0.398] Reward 0.6\n",
      "\tAction [0.81  0.77  0.938 0.951 0.338 0.454 0.221 0.116 0.719 0.425] Reward 0.9\n",
      "\tAction [0.84  0.154 0.344 0.505 0.123 0.75  0.064 0.387 0.133 0.905] Reward 0.1\n",
      "\tAction [0.783 0.155 0.556 0.747 0.531 0.232 0.098 0.138 0.039 0.946] Reward 0.1\n",
      "\tAction [0.544 0.225 0.164 0.641 0.784 0.875 0.086 0.588 0.419 0.854] Reward 0.2\n",
      "\tAction [0.289 0.213 0.415 0.908 0.812 0.105 0.331 0.752 0.128 0.933] Reward 0.0\n",
      "\tAction [0.933 0.068 0.219 0.968 0.747 0.744 0.001 0.8   0.282 0.943] Reward -0.0\n",
      "\tAction [0.879 0.43  0.278 0.792 0.593 0.942 0.686 0.192 0.022 0.928] Reward -0.2\n",
      "\tAction [0.52  0.043 0.306 0.745 0.275 0.86  0.007 0.075 0.504 0.496] Reward -1.5\n",
      "\tAction [0.54  0.71  0.35  0.71  0.877 0.719 0.277 0.068 0.059 0.794] Reward 1.9\n",
      "\tAction [0.505 0.163 0.273 0.994 0.73  0.956 0.189 0.553 0.06  0.943] Reward -0.6\n",
      "\tAction [0.369 0.429 0.253 0.407 0.353 0.921 0.099 0.082 0.116 0.565] Reward 0.4\n",
      "\tAction [0.94  0.522 0.525 0.868 0.538 0.868 0.198 0.431 0.006 0.897] Reward -1.5\n",
      "\tAction [0.317 0.492 0.375 0.93  0.768 0.264 0.183 0.532 0.249 0.313] Reward -0.8\n",
      "\tAction [0.831 0.043 0.12  0.874 0.602 0.575 0.334 0.166 0.316 0.909] Reward -0.3\n",
      "\tAction [0.701 0.076 0.059 0.97  0.855 0.155 0.171 0.111 0.023 0.759] Reward -0.4\n",
      "\tAction [0.874 0.026 0.063 0.714 0.737 0.519 0.074 0.225 0.241 0.69 ] Reward -0.2\n",
      "\tAction [0.894 0.063 0.479 0.768 0.753 0.59  0.253 0.011 0.149 0.915] Reward 0.0\n",
      "\tAction [0.96  0.009 0.216 0.507 0.751 0.804 0.016 0.131 0.211 0.715] Reward -0.3\n",
      "\tAction [0.573 0.057 0.236 0.875 0.847 0.786 0.072 0.01  0.359 0.121] Reward -0.3\n",
      "\tAction [0.912 0.574 0.772 0.835 0.648 0.533 0.025 0.094 0.041 0.462] Reward -0.3\n",
      "\tAction [0.568 0.224 0.015 0.97  0.847 0.986 0.017 0.417 0.116 0.703] Reward -1.6\n",
      "\tAction [0.77  0.058 0.174 0.944 0.941 0.847 0.08  0.034 0.62  0.679] Reward -0.3\n",
      "\tAction [0.685 0.03  0.214 0.92  0.458 0.841 0.144 0.25  0.356 0.886] Reward -0.4\n",
      "\tAction [0.911 0.038 0.29  0.965 0.693 0.816 0.459 0.275 0.076 0.781] Reward -0.4\n",
      "\tAction [0.879 0.574 0.172 0.896 0.579 0.256 0.212 0.001 0.222 0.929] Reward -0.5\n",
      "\tAction [0.907 0.047 0.138 0.819 0.625 0.722 0.017 0.235 0.696 0.477] Reward -0.3\n",
      "\tAction [0.926 0.402 0.358 0.532 0.426 0.796 0.007 0.25  0.203 0.95 ] Reward -1.0\n",
      "Memory use: 4.3529815673828125\n",
      "TOTAL REWARD: -3.8 (37 steps)\n",
      "\n",
      "Episode 2 Replay self.q_buffer 414\n",
      "D:\\Etiology\\imgs\\full_imgs\\4264253.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4656990.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4354964.npy\n",
      "\tAction [0.644 0.94  0.039 0.598 0.698 0.644 0.004 0.301 0.187 0.23 ] Reward -0.6\n",
      "\tAction [0.587 0.801 0.861 0.829 0.591 0.892 0.522 0.008 0.113 0.722] Reward 7.2\n",
      "\tAction [0.571 0.202 0.12  0.734 0.167 0.611 0.236 0.011 0.111 0.927] Reward 2.3\n",
      "\tAction [0.094 0.24  0.034 0.367 0.976 0.565 0.032 0.181 0.235 0.607] Reward -1.3\n",
      "\tAction [0.799 0.503 0.13  0.656 0.887 0.157 0.206 0.176 0.034 0.718] Reward -0.4\n",
      "\tAction [0.989 0.184 0.386 0.492 0.974 0.679 0.159 0.233 0.206 0.969] Reward -1.0\n",
      "Memory use: 4.370414733886719\n",
      "TOTAL REWARD: 6.2 (6 steps)\n",
      "\n",
      "Episode 3 Replay self.q_buffer 420\n",
      "\tAction [0.64  0.158 0.587 0.348 0.594 0.943 0.064 0.019 0.038 0.655] Reward 2.1\n",
      "\tAction [0.866 0.412 0.386 0.904 0.967 0.712 0.049 0.06  0.076 0.745] Reward 4.6\n",
      "\tAction [0.788 0.127 0.201 0.838 0.582 0.728 0.139 0.249 0.012 0.935] Reward -1.2\n",
      "\tAction [0.928 0.18  0.051 0.947 0.951 0.854 0.011 0.123 0.562 0.281] Reward 0.4\n",
      "\tAction [0.989 0.076 0.779 0.55  0.593 0.88  0.544 0.197 0.048 0.905] Reward -0.1\n",
      "\tAction [0.885 0.227 0.024 0.541 0.578 0.955 0.09  0.12  0.05  0.59 ] Reward -0.2\n",
      "\tAction [0.941 0.443 0.114 0.757 0.78  0.278 0.374 0.073 0.13  0.217] Reward 0.0\n",
      "\tAction [0.964 0.126 0.763 0.656 0.739 0.456 0.084 0.891 0.077 0.998] Reward -1.0\n",
      "Memory use: 4.381019592285156\n",
      "TOTAL REWARD: 4.7 (8 steps)\n",
      "\n",
      "Episode 4 Replay self.q_buffer 428\n",
      "D:\\Etiology\\imgs\\full_imgs\\4346391.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4073013.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4547885.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4379952.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4812872.npy\n",
      "\tAction [0.423 0.26  0.534 0.697 0.974 0.943 0.651 0.616 0.02  0.776] Reward 0.8\n",
      "\tAction [0.858 0.851 0.119 0.047 0.649 0.78  0.151 0.053 0.055 0.936] Reward 0.1\n",
      "\tAction [0.363 0.55  0.274 0.742 0.869 0.989 0.024 0.034 0.02  0.929] Reward 2.6\n",
      "\tAction [0.681 0.065 0.118 0.615 0.822 0.982 0.153 0.916 0.155 0.872] Reward 0.1\n",
      "\tAction [0.636 0.624 0.034 0.395 0.933 0.953 0.345 0.233 0.186 0.544] Reward 0.4\n",
      "\tAction [0.973 0.488 0.075 0.316 0.414 0.884 0.056 0.005 0.035 0.937] Reward 0.2\n",
      "\tAction [0.986 0.484 0.398 0.616 0.54  0.891 0.162 0.657 0.103 0.903] Reward 0.5\n",
      "\tAction [0.904 0.1   0.112 0.614 0.282 0.554 0.054 0.152 0.458 0.972] Reward -1.0\n",
      "Memory use: 4.384944915771484\n",
      "TOTAL REWARD: 3.8 (8 steps)\n",
      "\n",
      "Episode 5 Replay self.q_buffer 436\n",
      "D:\\Etiology\\imgs\\full_imgs\\3584014.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4876680.npy\n",
      "\tAction [0.989 0.121 0.233 0.707 0.866 0.362 0.269 0.271 0.027 0.822] Reward 0.2\n",
      "\tAction [0.78  0.016 0.118 0.85  0.72  0.922 0.174 0.371 0.224 0.932] Reward 0.8\n",
      "\tAction [0.897 0.518 0.383 0.877 0.743 0.883 0.03  0.892 0.093 0.861] Reward 0.5\n",
      "\tAction [0.339 0.412 0.037 0.982 0.882 0.956 0.086 0.135 0.75  0.379] Reward 0.3\n",
      "\tAction [0.744 0.34  0.073 0.835 0.913 0.968 0.024 0.597 0.407 0.926] Reward 0.2\n",
      "\tAction [0.449 0.914 0.249 0.422 0.801 0.616 0.347 0.398 0.046 0.983] Reward -1.0\n",
      "Memory use: 4.400478363037109\n",
      "TOTAL REWARD: 1.1 (6 steps)\n",
      "\n",
      "Episode 6 Replay self.q_buffer 442\n",
      "D:\\Etiology\\imgs\\full_imgs\\4969947.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4782059.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Etiology\\imgs\\full_imgs\\4651151.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5353048.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4782059.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\3613398.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4231650.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4308573.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4194658.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4854319.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5160536.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4122429.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4854319.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4800749.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5226445.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\3691024.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4149378.npy\n",
      "\tAction [0.829 0.336 0.099 0.95  0.136 0.395 0.08  0.185 0.115 0.207] Reward -0.2\n",
      "\tAction [0.804 0.405 0.447 0.987 0.948 0.906 0.053 0.173 0.02  0.859] Reward 0.1\n",
      "\tAction [0.953 0.024 0.469 0.589 0.908 0.894 0.127 0.013 0.162 0.124] Reward 0.6\n",
      "\tAction [0.56  0.304 0.203 0.965 0.926 0.439 0.021 0.023 0.086 0.896] Reward 0.5\n",
      "\tAction [0.74  0.042 0.015 0.805 0.223 0.363 0.024 0.037 0.024 0.956] Reward -1.0\n",
      "Memory use: 4.4193572998046875\n",
      "TOTAL REWARD: -0.0 (5 steps)\n",
      "\n",
      "Episode 7 Replay self.q_buffer 447\n",
      "D:\\Etiology\\imgs\\full_imgs\\3641352.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4235279.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4135521.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4101432.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4553296.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4101432.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5532336.npy\n",
      "\tAction [0.403 0.043 0.128 0.794 0.839 0.208 0.052 0.005 0.028 0.947] Reward 1.7\n",
      "\tAction [0.099 0.341 0.07  0.935 0.721 0.874 0.526 0.666 0.129 0.839] Reward 0.0\n",
      "\tAction [0.669 0.565 0.625 0.423 0.977 0.673 0.073 0.13  0.027 0.692] Reward 1.4\n",
      "\tAction [0.763 0.188 0.347 0.307 0.622 0.941 0.125 0.309 0.14  0.967] Reward -1.0\n",
      "Memory use: 4.418754577636719\n",
      "TOTAL REWARD: 2.1 (4 steps)\n",
      "\n",
      "Episode 8 Replay self.q_buffer 451\n",
      "D:\\Etiology\\imgs\\full_imgs\\4021167.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5198397.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5421952.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4506937.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5267504.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4257957.npy\n",
      "\tAction [0.904 0.031 0.015 0.819 0.962 0.496 0.199 0.048 0.177 0.922] Reward 0.1\n",
      "\tAction [0.92  0.052 0.276 0.872 0.989 0.689 0.092 0.217 0.327 0.471] Reward -0.6\n",
      "\tAction [0.975 0.534 0.514 0.907 0.63  0.977 0.254 0.035 0.029 0.708] Reward 7.6\n",
      "\tAction [0.977 0.036 0.095 0.935 0.779 0.912 0.078 0.028 0.186 0.879] Reward 0.2\n",
      "\tAction [0.565 0.417 0.008 0.132 0.962 0.888 0.074 0.005 0.068 0.518] Reward 0.2\n",
      "\tAction [0.92  0.121 0.052 0.3   0.86  0.935 0.32  0.022 0.324 0.73 ] Reward 0.0\n",
      "\tAction [0.815 0.589 0.113 0.93  0.55  0.444 0.192 0.286 0.3   0.934] Reward 0.3\n",
      "\tAction [0.945 0.272 0.506 0.809 0.706 0.939 0.007 0.077 0.044 0.652] Reward 0.2\n",
      "\tAction [0.401 0.265 0.023 0.531 0.953 0.923 0.04  0.204 0.06  0.767] Reward -1.3\n",
      "\tAction [0.847 0.555 0.789 0.885 0.853 0.75  0.097 0.608 0.044 0.542] Reward 1.7\n",
      "\tAction [0.881 0.019 0.015 0.895 0.729 0.881 0.221 0.072 0.318 0.391] Reward -0.0\n",
      "\tAction [0.921 0.087 0.313 0.83  0.763 0.891 0.039 0.444 0.166 0.633] Reward -0.6\n",
      "\tAction [0.187 0.194 0.138 0.911 0.978 0.764 0.444 0.044 0.409 0.535] Reward 1.1\n",
      "\tAction [0.166 0.147 0.292 0.75  0.956 0.662 0.404 0.828 0.164 0.488] Reward -1.0\n",
      "\tAction [0.781 0.062 0.267 0.786 0.869 0.925 0.012 0.513 0.112 0.839] Reward -0.6\n",
      "\tAction [0.911 0.232 0.197 0.563 0.225 0.295 0.017 0.457 0.047 0.378] Reward -0.2\n",
      "\tAction [0.925 0.041 0.269 0.989 0.473 0.654 0.069 0.421 0.502 0.612] Reward -0.3\n",
      "\tAction [0.97  0.135 0.064 0.346 0.746 0.911 0.08  0.565 0.066 0.979] Reward -1.0\n",
      "Memory use: 4.41900634765625\n",
      "TOTAL REWARD: 5.7 (18 steps)\n",
      "\n",
      "Episode 9 Replay self.q_buffer 469\n",
      "D:\\Etiology\\imgs\\full_imgs\\4382013.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4122429.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4581407.npy\n",
      "\tAction [0.762 0.012 0.072 0.952 0.839 0.519 0.724 0.701 0.041 0.933] Reward 2.0\n",
      "\tAction [0.792 0.112 0.155 0.792 0.922 0.485 0.032 0.983 0.28  0.695] Reward -0.9\n",
      "\tAction [0.807 0.019 0.313 0.86  0.953 0.532 0.02  0.972 0.035 0.944] Reward -0.4\n",
      "\tAction [0.551 0.351 0.032 0.34  0.468 0.636 0.077 0.834 0.233 0.958] Reward -1.0\n",
      "Memory use: 4.444644927978516\n",
      "TOTAL REWARD: -0.2 (4 steps)\n",
      "\n",
      "Episode 10 Replay self.q_buffer 473\n",
      "D:\\Etiology\\imgs\\full_imgs\\3576920.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4969947.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4925860.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4312226.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4937800.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4750142.npy\n",
      "\tAction [0.914 0.114 0.109 0.773 0.924 0.547 0.046 0.995 0.237 0.739] Reward -0.6\n",
      "\tAction [0.878 0.339 0.145 0.568 0.883 0.819 0.051 0.826 0.133 0.139] Reward -1.1\n",
      "\tAction [0.928 0.392 0.644 0.488 0.694 0.933 0.101 0.638 0.039 0.445] Reward 0.1\n",
      "\tAction [0.794 0.038 0.317 0.601 0.957 0.764 0.014 0.792 0.453 0.926] Reward -1.1\n",
      "\tAction [0.656 0.11  0.04  0.62  0.678 0.517 0.569 0.74  0.187 0.929] Reward 0.3\n",
      "\tAction [0.993 0.191 0.22  0.307 0.85  0.335 0.153 0.463 0.129 0.753] Reward -0.2\n",
      "\tAction [0.556 0.473 0.378 0.964 0.821 0.775 0.071 0.895 0.026 0.896] Reward 0.3\n",
      "\tAction [0.987 0.083 0.46  0.517 0.959 0.729 0.101 0.471 0.014 0.121] Reward 0.2\n",
      "\tAction [0.909 0.58  0.119 0.995 0.846 0.711 0.026 0.958 0.243 0.788] Reward 0.8\n",
      "\tAction [0.921 0.652 0.787 0.09  0.675 0.944 0.157 0.94  0.289 0.973] Reward -1.0\n",
      "Memory use: 4.451236724853516\n",
      "TOTAL REWARD: -2.3 (10 steps)\n",
      "\n",
      "Episode 11 Replay self.q_buffer 483\n",
      "D:\\Etiology\\imgs\\full_imgs\\4735186.npy\n",
      "\tAction [0.709 0.304 0.253 0.44  0.794 0.961 0.142 0.872 0.084 0.749] Reward -0.8\n",
      "\tAction [0.97  0.825 0.09  0.933 0.478 0.675 0.027 0.955 0.018 0.611] Reward -0.3\n",
      "\tAction [0.717 0.802 0.012 0.975 0.392 0.91  0.206 0.997 0.154 0.82 ] Reward 0.1\n",
      "\tAction [0.604 0.038 0.109 0.814 0.909 0.651 0.055 0.977 0.029 0.467] Reward -0.7\n",
      "\tAction [0.902 0.142 0.017 0.35  0.844 0.929 0.145 0.931 0.192 0.927] Reward -0.7\n",
      "\tAction [0.956 0.297 0.183 0.154 0.653 0.337 0.335 0.944 0.079 0.799] Reward -0.1\n",
      "\tAction [0.792 0.767 0.414 0.699 0.761 0.882 0.036 0.948 0.059 0.81 ] Reward 1.0\n",
      "\tAction [0.689 0.075 0.098 0.139 0.879 0.915 0.01  0.969 0.04  0.641] Reward -0.4\n",
      "\tAction [0.931 0.389 0.535 0.767 0.655 0.932 0.034 0.99  0.609 0.959] Reward -1.0\n",
      "Memory use: 4.481876373291016\n",
      "TOTAL REWARD: -2.8 (9 steps)\n",
      "\n",
      "Episode 12 Replay self.q_buffer 492\n",
      "D:\\Etiology\\imgs\\full_imgs\\4610631.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4001503.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\3603901.npy\n",
      "\tAction [0.599 0.011 0.346 0.424 0.95  0.693 0.199 0.657 0.255 0.849] Reward -3.4\n",
      "\tAction [0.986 0.067 0.154 0.484 0.79  0.832 0.095 0.718 0.07  0.557] Reward -1.2\n",
      "\tAction [0.992 0.66  0.571 0.136 0.409 0.553 0.758 0.816 0.299 0.93 ] Reward 1.5\n",
      "\tAction [0.152 0.091 0.285 0.457 0.366 0.971 0.045 0.893 0.02  0.309] Reward -1.1\n",
      "\tAction [0.843 0.077 0.047 0.427 0.613 0.969 0.029 0.98  0.007 0.927] Reward -1.0\n",
      "\tAction [0.747 0.016 0.685 0.959 0.768 0.345 0.075 0.923 0.112 0.951] Reward -1.0\n",
      "Memory use: 4.475101470947266\n",
      "TOTAL REWARD: -6.3 (6 steps)\n",
      "\n",
      "Episode 13 Replay self.q_buffer 498\n",
      "D:\\Etiology\\imgs\\full_imgs\\4506937.npy\n",
      "\tAction [0.395 0.183 0.072 0.963 0.874 0.885 0.802 0.781 0.313 0.539] Reward 1.7\n",
      "\tAction [0.829 0.563 0.436 0.387 0.446 0.821 0.045 0.849 0.129 0.886] Reward 0.7\n",
      "\tAction [0.991 0.616 0.106 0.563 0.878 0.48  0.159 0.988 0.003 0.979] Reward -1.0\n",
      "Memory use: 4.496921539306641\n",
      "TOTAL REWARD: 1.4 (3 steps)\n",
      "\n",
      "Episode 14 Replay self.q_buffer 501\n",
      "D:\\Etiology\\imgs\\full_imgs\\5013931.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4651151.npy\n",
      "\tAction [0.586 0.253 0.222 0.5   0.975 0.768 0.108 0.94  0.496 0.882] Reward -3.0\n",
      "\tAction [0.743 0.071 0.025 0.994 0.325 0.24  0.127 0.998 0.057 0.899] Reward -0.3\n",
      "\tAction [0.831 0.468 0.241 0.993 0.92  0.849 0.105 0.955 0.048 0.707] Reward -2.9\n",
      "\tAction [0.365 0.517 0.176 0.726 0.966 0.885 0.044 0.799 0.31  0.949] Reward -0.9\n",
      "\tAction [0.948 0.762 0.444 0.945 0.925 0.948 0.021 0.828 0.285 0.995] Reward -1.0\n",
      "Memory use: 4.501182556152344\n",
      "TOTAL REWARD: -8.1 (5 steps)\n",
      "\n",
      "Episode 15 Replay self.q_buffer 506\n",
      "\tAction [0.577 0.296 0.079 0.751 0.32  0.81  0.549 0.981 0.137 0.617] Reward -0.2\n",
      "\tAction [0.73  0.129 0.182 0.908 0.378 0.791 0.082 0.93  0.323 0.668] Reward -0.8\n",
      "\tAction [0.96  0.473 0.464 0.644 0.828 0.832 0.076 0.751 0.079 0.975] Reward -1.0\n",
      "Memory use: 4.5042266845703125\n",
      "TOTAL REWARD: -2.0 (3 steps)\n",
      "\n",
      "Episode 16 Replay self.q_buffer 509\n",
      "D:\\Etiology\\imgs\\full_imgs\\3596277.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4515877.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Etiology\\imgs\\full_imgs\\4090578.npy\n",
      "\tAction [0.189 0.142 0.292 0.981 0.919 0.221 0.12  0.923 0.013 0.996] Reward -1.0\n",
      "Memory use: 4.503162384033203\n",
      "TOTAL REWARD: -1.0 (1 steps)\n",
      "\n",
      "Episode 17 Replay self.q_buffer 510\n",
      "D:\\Etiology\\imgs\\full_imgs\\4365804.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5532336.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4366235.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4072335.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4067703.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4072335.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4250455.npy\n",
      "\tAction [0.854 0.51  0.135 0.601 0.649 0.164 0.384 0.897 0.07  0.994] Reward -1.0\n",
      "Memory use: 4.502368927001953\n",
      "TOTAL REWARD: -1.0 (1 steps)\n",
      "\n",
      "Episode 18 Replay self.q_buffer 511\n",
      "D:\\Etiology\\imgs\\full_imgs\\4947882.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4940582.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\651494.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4072335.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4782059.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4165057.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4875130.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4973789.npy\n",
      "\tAction [0.844 0.028 0.072 0.73  0.875 0.823 0.021 0.807 0.017 0.96 ] Reward -1.0\n",
      "Memory use: 4.503429412841797\n",
      "TOTAL REWARD: -1.0 (1 steps)\n",
      "\n",
      "Episode 19 Replay self.q_buffer 512\n",
      "D:\\Etiology\\imgs\\full_imgs\\4601427.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4895076.npy\n",
      "\tAction [0.965 0.106 0.715 0.922 0.96  0.94  0.059 0.511 0.105 0.823] Reward 0.5\n",
      "\tAction [0.816 0.256 0.071 0.331 0.886 0.322 0.008 0.986 0.011 0.984] Reward -1.0\n",
      "Memory use: 4.5473175048828125\n",
      "TOTAL REWARD: -0.5 (2 steps)\n",
      "\n",
      "Episode 20 Replay self.q_buffer 514\n",
      "D:\\Etiology\\imgs\\full_imgs\\4987127.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4108270.npy\n",
      "\tAction [0.961 0.013 0.062 0.436 0.961 0.564 0.377 0.987 0.081 0.985] Reward -1.0\n",
      "Memory use: 4.563758850097656\n",
      "TOTAL REWARD: -1.0 (1 steps)\n",
      "\n",
      "Episode 21 Replay self.q_buffer 515\n",
      "D:\\Etiology\\imgs\\full_imgs\\4180822.npy\n",
      "\tAction [0.722 0.042 0.011 0.939 0.368 0.579 0.05  0.971 0.049 0.728] Reward -0.7\n",
      "\tAction [0.594 0.653 0.251 0.719 0.854 0.536 0.005 0.979 0.035 0.798] Reward -4.4\n",
      "\tAction [0.527 0.05  0.108 0.44  0.056 0.746 0.051 0.78  0.443 0.57 ] Reward -0.2\n",
      "\tAction [0.924 0.031 0.306 0.952 0.413 0.898 0.83  0.908 0.047 0.486] Reward -0.9\n",
      "\tAction [0.646 0.444 0.196 0.809 0.668 0.528 0.031 0.307 0.253 0.574] Reward -2.2\n",
      "\tAction [0.851 0.022 0.407 0.876 0.923 0.876 0.081 0.894 0.041 0.728] Reward -0.6\n",
      "\tAction [0.955 0.323 0.127 0.85  0.797 0.486 0.014 0.77  0.423 0.943] Reward -0.7\n",
      "\tAction [0.68  0.696 0.182 0.782 0.862 0.783 0.069 0.984 0.339 0.821] Reward -1.7\n",
      "\tAction [0.979 0.071 0.286 0.062 0.722 0.765 0.199 0.91  0.02  0.54 ] Reward -0.3\n",
      "\tAction [0.661 0.108 0.443 0.777 0.831 0.248 0.906 0.849 0.179 0.847] Reward 0.0\n",
      "\tAction [0.571 0.028 0.163 0.709 0.584 0.978 0.014 0.985 0.026 0.957] Reward -1.0\n",
      "Memory use: 4.582202911376953\n",
      "TOTAL REWARD: -12.8 (11 steps)\n",
      "\n",
      "Episode 22 Replay self.q_buffer 526\n",
      "D:\\Etiology\\imgs\\full_imgs\\4195600.npy\n",
      "\tAction [0.934 0.117 0.016 0.921 0.206 0.733 0.027 0.805 0.012 0.698] Reward -0.7\n",
      "\tAction [0.776 0.532 0.854 0.873 0.849 0.619 0.009 0.906 0.087 0.305] Reward 5.5\n",
      "\tAction [0.881 0.031 0.03  0.944 0.089 0.602 0.018 0.986 0.018 0.857] Reward -0.2\n",
      "\tAction [0.911 0.437 0.157 0.974 0.103 0.655 0.072 0.912 0.079 0.894] Reward -0.5\n",
      "\tAction [0.873 0.032 0.312 0.362 0.261 0.772 0.046 0.673 0.523 0.919] Reward 0.1\n",
      "\tAction [0.498 0.111 0.178 0.824 0.558 0.879 0.068 0.88  0.134 0.824] Reward 0.4\n",
      "\tAction [0.924 0.281 0.491 0.365 0.835 0.984 0.25  0.77  0.204 0.917] Reward -0.0\n",
      "\tAction [0.946 0.04  0.095 0.682 0.839 0.985 0.264 0.23  0.21  0.718] Reward -0.3\n",
      "\tAction [0.152 0.785 0.192 0.81  0.952 0.733 0.07  0.987 0.014 0.879] Reward 0.1\n",
      "\tAction [0.83  0.544 0.184 0.946 0.966 0.869 0.022 0.929 0.14  0.159] Reward -2.1\n",
      "\tAction [0.389 0.189 0.442 0.689 0.66  0.977 0.008 0.942 0.017 0.655] Reward 0.3\n",
      "\tAction [0.385 0.906 0.112 0.722 0.732 0.674 0.035 0.881 0.028 0.925] Reward -1.0\n",
      "\tAction [0.571 0.099 0.268 0.714 0.816 0.437 0.061 0.885 0.105 0.871] Reward -0.1\n",
      "\tAction [0.904 0.47  0.098 0.649 0.714 0.784 0.436 0.973 0.812 0.709] Reward -0.6\n",
      "\tAction [0.928 0.072 0.1   0.979 0.911 0.583 0.334 0.71  0.213 0.686] Reward -0.6\n",
      "\tAction [0.912 0.039 0.581 0.977 0.413 0.919 0.012 0.216 0.072 0.974] Reward -1.0\n",
      "Memory use: 4.547939300537109\n",
      "TOTAL REWARD: -0.6 (16 steps)\n",
      "\n",
      "Episode 23 Replay self.q_buffer 542\n",
      "\tAction [0.985 0.213 0.634 0.951 0.731 0.99  0.076 0.943 0.065 0.844] Reward 0.2\n",
      "\tAction [0.963 0.279 0.814 0.33  0.571 0.809 0.064 0.944 0.009 0.852] Reward 0.3\n",
      "\tAction [0.896 0.334 0.588 0.374 0.151 0.868 0.738 0.386 0.078 0.975] Reward -1.0\n",
      "Memory use: 4.582477569580078\n",
      "TOTAL REWARD: -0.5 (3 steps)\n",
      "\n",
      "Episode 24 Replay self.q_buffer 545\n",
      "D:\\Etiology\\imgs\\full_imgs\\4655933.npy\n",
      "\tAction [0.43  0.479 0.365 0.673 0.553 0.865 0.002 0.378 0.144 0.95 ] Reward -1.0\n",
      "Memory use: 4.584564208984375\n",
      "TOTAL REWARD: -1.0 (1 steps)\n",
      "\n",
      "Episode 25 Replay self.q_buffer 546\n",
      "\tAction [0.431 0.175 0.162 0.289 0.916 0.886 0.27  0.8   0.258 0.701] Reward 0.1\n",
      "\tAction [0.361 0.185 0.305 0.316 0.96  0.953 0.07  0.865 0.078 0.848] Reward 0.6\n",
      "\tAction [0.973 0.128 0.141 0.662 0.745 0.923 0.011 0.907 0.012 0.904] Reward -1.3\n",
      "\tAction [0.74  0.224 0.628 0.676 0.629 0.52  0.328 0.795 0.019 0.989] Reward -1.0\n",
      "Memory use: 4.546154022216797\n",
      "TOTAL REWARD: -1.6 (4 steps)\n",
      "\n",
      "Episode 26 Replay self.q_buffer 550\n",
      "D:\\Etiology\\imgs\\full_imgs\\4651151.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\3966711.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\3584014.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4314900.npy\n",
      "\tAction [0.871 0.043 0.089 0.967 0.545 0.853 0.151 0.634 0.034 0.897] Reward -1.6\n",
      "\tAction [0.824 0.128 0.027 0.843 0.771 0.886 0.077 0.849 0.051 0.92 ] Reward -1.6\n",
      "\tAction [0.707 0.004 0.633 0.923 0.407 0.428 0.162 0.958 0.422 0.852] Reward 0.0\n",
      "\tAction [0.365 0.054 0.238 0.677 0.569 0.664 0.176 0.689 0.021 0.382] Reward -1.3\n",
      "\tAction [0.806 0.02  0.327 0.885 0.113 0.801 0.118 0.966 0.148 0.216] Reward -0.3\n",
      "\tAction [0.27  0.673 0.133 0.439 0.522 0.953 0.319 0.813 0.47  0.702] Reward -0.1\n",
      "\tAction [0.896 0.299 0.262 0.538 0.664 0.977 0.005 0.406 0.09  0.369] Reward -1.5\n",
      "\tAction [0.761 0.427 0.036 0.706 0.428 0.881 0.168 0.898 0.033 0.551] Reward -0.9\n",
      "\tAction [0.981 0.056 0.174 0.795 0.677 0.852 0.122 0.982 0.147 0.675] Reward -0.5\n",
      "\tAction [0.966 0.104 0.072 0.34  0.812 0.968 0.009 0.816 0.143 0.987] Reward -1.0\n",
      "Memory use: 4.559360504150391\n",
      "TOTAL REWARD: -8.8 (10 steps)\n",
      "\n",
      "Episode 27 Replay self.q_buffer 560\n",
      "D:\\Etiology\\imgs\\full_imgs\\5363130.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4021167.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4314900.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4925860.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4135521.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4504910.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4314900.npy\n",
      "\tAction [0.87  0.486 0.082 0.823 0.761 0.934 0.164 0.969 0.064 0.607] Reward -2.8\n",
      "\tAction [0.707 0.38  0.023 0.776 0.096 0.901 0.497 0.593 0.039 0.841] Reward -0.2\n",
      "\tAction [0.398 0.143 0.074 0.958 0.832 0.382 0.164 0.951 0.039 0.972] Reward -1.0\n",
      "Memory use: 3.414234161376953\n",
      "TOTAL REWARD: -4.0 (3 steps)\n",
      "\n",
      "Episode 28 Replay self.q_buffer 563\n",
      "\tAction [0.768 0.112 0.352 0.729 0.603 0.822 0.076 0.836 0.024 0.979] Reward -1.0\n",
      "Memory use: 3.4135284423828125\n",
      "TOTAL REWARD: -1.0 (1 steps)\n",
      "\n",
      "Episode 29 Replay self.q_buffer 564\n",
      "D:\\Etiology\\imgs\\full_imgs\\4272496.npy\n",
      "\tAction [0.924 0.209 0.023 0.644 0.685 0.798 0.044 0.534 0.052 0.541] Reward -1.3\n",
      "\tAction [0.943 0.031 0.21  0.029 0.562 0.916 0.116 0.939 0.015 0.626] Reward -0.1\n",
      "\tAction [0.835 0.331 0.055 0.669 0.7   0.606 0.03  0.844 0.056 0.58 ] Reward -1.3\n",
      "\tAction [0.716 0.153 0.049 0.813 0.685 0.913 0.024 0.966 0.042 0.907] Reward -2.0\n",
      "\tAction [0.964 0.206 0.862 0.728 0.467 0.662 0.246 0.979 0.102 0.842] Reward 1.8\n",
      "\tAction [0.693 0.177 0.116 0.815 0.611 0.933 0.231 0.964 0.071 0.877] Reward -1.1\n",
      "\tAction [0.957 0.286 0.125 0.859 0.555 0.699 0.172 0.945 0.184 0.877] Reward -0.6\n",
      "\tAction [0.873 0.506 0.514 0.911 0.814 0.711 0.236 0.3   0.144 0.301] Reward 0.5\n",
      "\tAction [0.99  0.79  0.245 0.641 0.933 0.347 0.009 0.939 0.012 0.63 ] Reward -0.6\n",
      "\tAction [0.726 0.131 0.252 0.966 0.753 0.838 0.239 0.87  0.032 0.881] Reward -0.3\n",
      "\tAction [0.774 0.033 0.277 0.94  0.677 0.506 0.056 0.97  0.023 0.881] Reward -0.4\n",
      "\tAction [0.673 0.231 0.339 0.572 0.058 0.688 0.068 0.868 0.047 0.691] Reward -0.3\n",
      "\tAction [0.946 0.28  0.026 0.509 0.684 0.601 0.116 0.779 0.099 0.985] Reward -1.0\n",
      "Memory use: 3.4386672973632812\n",
      "TOTAL REWARD: -6.6 (13 steps)\n",
      "\n",
      "Episode 30 Replay self.q_buffer 577\n",
      "D:\\Etiology\\imgs\\full_imgs\\3596277.npy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAction [0.522 0.094 0.09  0.872 0.772 0.943 0.104 0.461 0.01  0.939] Reward -2.4\n",
      "\tAction [0.18  0.41  0.355 0.621 0.906 0.746 0.03  0.877 0.037 0.938] Reward 0.6\n",
      "\tAction [0.18  0.057 0.018 0.869 0.609 0.784 0.035 0.986 0.045 0.939] Reward -0.7\n",
      "\tAction [0.844 0.217 0.096 0.901 0.862 0.767 0.05  0.956 0.217 0.778] Reward -1.8\n",
      "\tAction [0.857 0.139 0.256 0.876 0.554 0.713 0.019 0.499 0.068 0.936] Reward -0.8\n",
      "\tAction [0.58  0.045 0.071 0.924 0.537 0.625 0.323 0.982 0.02  0.803] Reward -0.6\n",
      "\tAction [0.35  0.025 0.132 0.825 0.768 0.93  0.018 0.927 0.095 0.66 ] Reward -0.4\n",
      "\tAction [0.854 0.068 0.351 0.295 0.972 0.561 0.047 0.545 0.099 0.947] Reward -0.5\n",
      "\tAction [0.947 0.202 0.31  0.826 0.67  0.47  0.087 0.981 0.275 0.823] Reward -0.3\n",
      "\tAction [0.969 0.864 0.204 0.127 0.529 0.609 0.08  0.865 0.15  0.852] Reward -0.4\n",
      "\tAction [0.743 0.173 0.606 0.681 0.797 0.565 0.275 0.941 0.056 0.284] Reward 1.7\n",
      "\tAction [0.988 0.397 0.025 0.458 0.39  0.497 0.016 0.938 0.092 0.963] Reward -1.0\n",
      "Memory use: 2.6534461975097656\n",
      "TOTAL REWARD: -6.7 (12 steps)\n",
      "\n",
      "Episode 31 Replay self.q_buffer 589\n",
      "D:\\Etiology\\imgs\\full_imgs\\4255216.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4101432.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4372592.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5205315.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4193028.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5247505.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\3691024.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4042268.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4122429.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4193028.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4782059.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4108270.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4294632.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5203648.npy\n",
      "\tAction [0.944 0.45  0.18  0.926 0.469 0.833 0.113 0.99  0.096 0.947] Reward -1.5\n",
      "\tAction [0.965 0.079 0.094 0.795 0.815 0.965 0.264 0.861 0.229 0.154] Reward -1.5\n",
      "\tAction [0.997 0.398 0.099 0.227 0.942 0.934 0.076 0.941 0.175 0.645] Reward -0.8\n",
      "\tAction [0.612 0.913 0.028 0.566 0.717 0.597 0.06  0.93  0.016 0.978] Reward -1.0\n",
      "Memory use: 2.6804046630859375\n",
      "TOTAL REWARD: -4.8 (4 steps)\n",
      "\n",
      "Episode 32 Replay self.q_buffer 593\n",
      "D:\\Etiology\\imgs\\full_imgs\\4372592.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4448512.npy\n",
      "\tAction [0.799 0.45  0.3   0.914 0.75  0.866 0.019 0.738 0.151 0.39 ] Reward -3.1\n",
      "\tAction [0.97  0.546 0.209 0.929 0.254 0.835 0.314 0.535 0.007 0.655] Reward -0.6\n",
      "\tAction [0.65  0.005 0.194 0.396 0.226 0.555 0.155 0.849 0.136 0.614] Reward -0.4\n",
      "\tAction [0.861 0.047 0.211 0.762 0.735 0.789 0.615 0.265 0.056 0.665] Reward -0.9\n",
      "\tAction [0.445 0.006 0.635 0.747 0.292 0.885 0.033 0.724 0.017 0.47 ] Reward 0.6\n",
      "\tAction [0.818 0.201 0.37  0.983 0.721 0.84  0.427 0.956 0.092 0.054] Reward -0.7\n",
      "\tAction [0.988 0.105 0.326 0.952 0.58  0.806 0.185 0.978 0.162 0.817] Reward -0.5\n",
      "\tAction [0.715 0.128 0.213 0.775 0.379 0.929 0.071 0.988 0.125 0.925] Reward -0.3\n",
      "\tAction [0.854 0.056 0.054 0.737 0.652 0.69  0.029 0.988 0.043 0.501] Reward -0.4\n",
      "\tAction [0.799 0.01  0.251 0.955 0.725 0.987 0.215 0.988 0.128 0.535] Reward -0.2\n",
      "\tAction [0.971 0.045 0.056 0.719 0.949 0.383 0.024 0.984 0.071 0.898] Reward -0.3\n",
      "\tAction [0.927 0.037 0.05  0.704 0.573 0.109 0.053 0.97  0.144 0.894] Reward -0.1\n",
      "\tAction [0.066 0.104 0.568 0.58  0.99  0.898 0.082 0.987 0.059 0.891] Reward 0.3\n",
      "\tAction [0.862 0.472 0.172 0.958 0.24  0.86  0.155 0.912 0.249 0.254] Reward -0.2\n",
      "\tAction [0.634 0.051 0.576 0.934 0.717 0.975 0.023 0.909 0.063 0.879] Reward 0.5\n",
      "\tAction [0.97  0.054 0.146 0.863 0.314 0.241 0.375 0.951 0.23  0.193] Reward -0.2\n",
      "\tAction [0.542 0.312 0.184 0.902 0.673 0.689 0.096 0.252 0.059 0.819] Reward -0.6\n",
      "\tAction [0.777 0.274 0.038 0.47  0.555 0.847 0.061 0.986 0.176 0.932] Reward -0.4\n",
      "\tAction [0.892 0.031 0.167 0.734 0.058 0.933 0.041 0.964 0.37  0.975] Reward -1.0\n",
      "Memory use: 2.7009315490722656\n",
      "TOTAL REWARD: -8.4 (19 steps)\n",
      "\n",
      "Episode 33 Replay self.q_buffer 612\n",
      "\tAction [0.301 0.64  0.075 0.944 0.171 0.97  0.042 0.983 0.052 0.561] Reward -0.3\n",
      "\tAction [0.914 0.263 0.029 0.957 0.51  0.967 0.143 0.947 0.116 0.872] Reward -1.6\n",
      "\tAction [0.781 0.719 0.153 0.443 0.191 0.81  0.027 0.888 0.105 0.725] Reward -0.7\n",
      "\tAction [0.603 0.554 0.086 0.31  0.718 0.904 0.021 0.522 0.107 0.689] Reward -0.5\n",
      "\tAction [0.743 0.007 0.08  0.741 0.016 0.833 0.014 0.911 0.005 0.938] Reward -0.4\n",
      "\tAction [0.152 0.043 0.218 0.978 0.844 0.746 0.064 0.247 0.375 0.739] Reward 1.3\n",
      "\tAction [0.499 0.344 0.786 0.714 0.1   0.679 0.035 0.412 0.588 0.07 ] Reward -0.5\n",
      "\tAction [0.785 0.244 0.418 0.548 0.889 0.984 0.036 0.988 0.851 0.92 ] Reward 0.5\n",
      "\tAction [0.824 0.248 0.263 0.91  0.904 0.946 0.1   0.993 0.037 0.115] Reward -0.2\n",
      "\tAction [0.888 0.27  0.042 0.311 0.869 0.594 0.043 0.434 0.358 0.881] Reward -0.4\n",
      "\tAction [0.689 0.101 0.131 0.729 0.6   0.79  0.076 0.866 0.052 0.651] Reward -0.3\n",
      "\tAction [0.978 0.078 0.225 0.873 0.167 0.955 0.142 0.836 0.026 0.769] Reward -0.4\n",
      "\tAction [0.816 0.074 0.386 0.473 0.183 0.232 0.079 0.974 0.053 0.853] Reward 0.0\n",
      "\tAction [0.915 0.01  0.117 0.929 0.738 0.674 0.099 0.78  0.523 0.989] Reward -1.0\n",
      "Memory use: 2.644012451171875\n",
      "TOTAL REWARD: -4.5 (14 steps)\n",
      "\n",
      "Episode 34 Replay self.q_buffer 626\n",
      "D:\\Etiology\\imgs\\full_imgs\\4895076.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5353048.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5122452.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4581407.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4366235.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4506937.npy\n",
      "\tAction [0.891 0.133 0.036 0.734 0.468 0.895 0.095 0.979 0.034 0.99 ] Reward -1.0\n",
      "Memory use: 2.6701622009277344\n",
      "TOTAL REWARD: -1.0 (1 steps)\n",
      "\n",
      "Episode 35 Replay self.q_buffer 627\n",
      "D:\\Etiology\\imgs\\full_imgs\\5160536.npy\n",
      "\tAction [0.886 0.043 0.471 0.875 0.578 0.49  0.043 0.658 0.029 0.314] Reward -0.2\n",
      "\tAction [0.876 0.626 0.309 0.727 0.777 0.987 0.022 0.979 0.03  0.942] Reward -1.9\n",
      "\tAction [0.904 0.425 0.302 0.751 0.519 0.82  0.069 0.917 0.106 0.077] Reward -1.0\n",
      "\tAction [0.953 0.268 0.013 0.787 0.872 0.663 0.012 0.893 0.115 0.977] Reward -1.0\n",
      "Memory use: 2.7235260009765625\n",
      "TOTAL REWARD: -4.1 (4 steps)\n",
      "\n",
      "Episode 36 Replay self.q_buffer 631\n",
      "\tAction [0.693 0.479 0.334 0.82  0.903 0.92  0.345 0.811 0.033 0.791] Reward -2.8\n",
      "\tAction [0.318 0.068 0.159 0.855 0.561 0.922 0.196 0.394 0.165 0.708] Reward -1.1\n",
      "\tAction [0.822 0.071 0.077 0.893 0.818 0.282 0.011 0.99  0.014 0.933] Reward -0.6\n",
      "\tAction [0.996 0.272 0.178 0.785 0.937 0.651 0.032 0.882 0.194 0.692] Reward -1.4\n",
      "\tAction [0.237 0.757 0.035 0.912 0.575 0.972 0.516 0.995 0.304 0.965] Reward -1.0\n",
      "Memory use: 2.5841217041015625\n",
      "TOTAL REWARD: -6.9 (5 steps)\n",
      "\n",
      "Episode 37 Replay self.q_buffer 636\n",
      "D:\\Etiology\\imgs\\full_imgs\\4180822.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5041655.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4262153.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4121933.npy\n",
      "\tAction [0.939 0.024 0.018 0.941 0.894 0.326 0.656 0.993 0.06  0.885] Reward -0.7\n",
      "\tAction [0.85  0.008 0.263 0.813 0.788 0.913 0.203 0.938 0.204 0.562] Reward -0.3\n",
      "\tAction [0.882 0.931 0.077 0.375 0.971 0.575 0.228 0.87  0.012 0.921] Reward -1.2\n",
      "\tAction [0.778 0.111 0.122 0.754 0.796 0.925 0.092 0.979 0.064 0.962] Reward -1.0\n",
      "Memory use: 2.4578514099121094\n",
      "TOTAL REWARD: -3.1 (4 steps)\n",
      "\n",
      "Episode 38 Replay self.q_buffer 640\n",
      "D:\\Etiology\\imgs\\full_imgs\\4183607.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4928445.npy\n",
      "\tAction [0.964 0.48  0.022 0.845 0.921 0.613 0.134 0.637 0.558 0.988] Reward -1.0\n",
      "Memory use: 2.468181610107422\n",
      "TOTAL REWARD: -1.0 (1 steps)\n",
      "\n",
      "Episode 39 Replay self.q_buffer 641\n",
      "D:\\Etiology\\imgs\\full_imgs\\4937800.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4466605.npy\n",
      "\tAction [0.993 0.103 0.433 0.615 0.703 0.354 0.019 0.836 0.316 0.145] Reward -1.3\n",
      "\tAction [0.289 0.807 0.324 0.836 0.454 0.637 0.337 0.671 0.049 0.914] Reward -1.4\n",
      "\tAction [0.43  0.579 0.021 0.857 0.627 0.723 0.729 0.797 0.107 0.74 ] Reward -0.7\n",
      "\tAction [0.335 0.39  0.189 0.817 0.875 0.675 0.013 0.968 0.21  0.63 ] Reward -1.1\n",
      "\tAction [0.963 0.486 0.123 0.833 0.818 0.857 0.024 0.869 0.042 0.86 ] Reward -1.8\n",
      "\tAction [0.859 0.04  0.044 0.628 0.957 0.931 0.04  0.929 0.016 0.797] Reward -0.6\n",
      "\tAction [0.845 0.833 0.185 0.587 0.967 0.978 0.075 0.963 0.137 0.864] Reward -1.6\n",
      "\tAction [0.733 0.209 0.434 0.772 0.946 0.799 0.329 0.614 0.091 0.793] Reward 0.5\n",
      "\tAction [0.824 0.023 0.286 0.808 0.902 0.885 0.374 0.943 0.004 0.943] Reward -0.3\n",
      "\tAction [0.423 0.112 0.107 0.787 0.606 0.768 0.014 0.904 0.185 0.99 ] Reward -1.0\n",
      "Memory use: 2.4689064025878906\n",
      "TOTAL REWARD: -9.2 (10 steps)\n",
      "\n",
      "Episode 40 Replay self.q_buffer 651\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\Etiology\\imgs\\full_imgs\\4162152.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\651494.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4267520.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4487803.npy\n",
      "\tAction [0.659 0.047 0.233 0.813 0.979 0.933 0.173 0.662 0.009 0.731] Reward -1.4\n",
      "\tAction [0.983 0.313 0.33  0.961 0.953 0.831 0.013 0.773 0.089 0.977] Reward -1.0\n",
      "Memory use: 2.4864540100097656\n",
      "TOTAL REWARD: -2.4 (2 steps)\n",
      "\n",
      "Episode 41 Replay self.q_buffer 653\n",
      "\tAction [0.298 0.058 0.075 0.956 0.864 0.803 0.178 0.335 0.012 0.887] Reward -1.3\n",
      "\tAction [0.97  0.278 0.643 0.898 0.249 0.92  0.086 0.951 0.033 0.656] Reward 0.4\n",
      "\tAction [0.387 0.279 0.371 0.876 0.893 0.982 0.128 0.939 0.025 0.75 ] Reward 0.7\n",
      "\tAction [0.986 0.178 0.146 0.853 0.468 0.868 0.101 0.372 0.12  0.27 ] Reward -0.9\n",
      "\tAction [0.831 0.81  0.053 0.608 0.933 0.85  0.014 0.942 0.15  0.991] Reward -1.0\n",
      "Memory use: 2.508838653564453\n",
      "TOTAL REWARD: -2.1 (5 steps)\n",
      "\n",
      "Episode 42 Replay self.q_buffer 658\n",
      "D:\\Etiology\\imgs\\full_imgs\\4162152.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4547885.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4001503.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4585733.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4231650.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\5530755.npy\n",
      "\tAction [0.82  0.44  0.216 0.947 0.839 0.531 0.019 0.427 0.49  0.787] Reward -3.6\n",
      "\tAction [0.888 0.099 0.28  0.535 0.835 0.791 0.626 0.564 0.238 0.967] Reward -1.0\n",
      "Memory use: 2.4960098266601562\n",
      "TOTAL REWARD: -4.6 (2 steps)\n",
      "\n",
      "Episode 43 Replay self.q_buffer 660\n",
      "D:\\Etiology\\imgs\\full_imgs\\4876680.npy\n",
      "D:\\Etiology\\imgs\\full_imgs\\4655933.npy\n",
      "\tAction [0.914 0.372 0.119 0.986 0.695 0.854 0.284 0.822 0.167 0.86 ] Reward 0.0\n",
      "\tAction [0.426 0.074 0.192 0.975 0.884 0.929 0.026 0.546 0.174 0.504] Reward 2.1\n",
      "\tAction [0.976 0.234 0.826 0.888 0.914 0.989 0.054 0.476 0.074 0.977] Reward -1.0\n",
      "Memory use: 2.533039093017578\n",
      "TOTAL REWARD: 1.1 (3 steps)\n",
      "\n",
      "Episode 44 Replay self.q_buffer 663\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(qmain)\n",
    "Q = qmain.CRSNet(1.)\n",
    "Q.train(dqn_generator, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T16:24:15.920842Z",
     "start_time": "2018-05-29T16:23:41.342074Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T14:48:55.729060Z",
     "start_time": "2018-05-29T14:48:55.724043Z"
    }
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(C.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T14:48:56.767254Z",
     "start_time": "2018-05-29T14:48:56.763264Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(C.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T19:44:49.300614Z",
     "start_time": "2018-05-29T19:44:49.284657Z"
    }
   },
   "outputs": [],
   "source": [
    "cropI = np.random.normal(size=(1,*C.dims,3))\n",
    "\n",
    "crop_true_seg = np.random.normal(size=(1,*C.dims,3))\n",
    "true_cls = np.random.uniform(size=(1,3))\n",
    "\n",
    "y_true = np.random.normal(size=(1,*C.dims,3))\n",
    "y_pred = np.random.normal(size=(1,*C.dims,4))\n",
    "\n",
    "loss_layer = train_model.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T19:44:57.204848Z",
     "start_time": "2018-05-29T19:44:54.411315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05015051\n",
      "Memory use: 2.0087203979492188\n",
      "-0.050445613\n",
      "Memory use: 2.0087203979492188\n",
      "-0.050510425\n",
      "Memory use: 2.008716583251953\n",
      "-0.049581293\n",
      "Memory use: 2.008716583251953\n",
      "-0.049884304\n",
      "Memory use: 2.008716583251953\n",
      "-0.048125546\n",
      "Memory use: 2.008716583251953\n",
      "-0.049017023\n",
      "Memory use: 2.008716583251953\n",
      "-0.04843785\n",
      "Memory use: 2.0087203979492188\n",
      "-0.050015423\n",
      "Memory use: 2.0087203979492188\n",
      "-0.049520925\n",
      "Memory use: 2.0087203979492188\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(loss_layer.get_loss(0, y_true, y_pred, C.loss_weights[1:]))\n",
    "    memory();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T18:18:34.280289Z",
     "start_time": "2018-05-29T18:18:18.388368Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(tn)\n",
    "for _ in range(10):\n",
    "    log_vars = [K.get_value(x)[0] for x in train_model.layers[-1].log_vars]\n",
    "    loss = np.sum(np.exp(-log_vars[0]) * K.get_value(tn.hetero_cls_loss(y_true, y_pred)) + log_vars[0], -1)\n",
    "    print(np.mean(loss))\n",
    "    memory();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T16:55:26.869123Z",
     "start_time": "2018-05-29T16:55:26.846162Z"
    }
   },
   "outputs": [],
   "source": [
    "A = prediction_model.predict(cropI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T17:00:15.134130Z",
     "start_time": "2018-05-29T17:00:14.176132Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    train_model.train_on_batch([cropI, crop_true_seg, true_cls], None)\n",
    "    A=prediction_model.predict(cropI);\n",
    "    print(A[1])\n",
    "    memory();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T18:11:58.749023Z",
     "start_time": "2018-05-28T18:11:47.096324Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(qmain)\n",
    "Q = qmain.CRSNet(1.)\n",
    "\n",
    "replay_conf = {'size': 10000,\n",
    "        'learn_start': 100,\n",
    "        'partition_num': 100,\n",
    "        'total_step': 10000,\n",
    "        'batch_size': 4}\n",
    "BATCH_SIZE = replay_conf[\"batch_size\"]\n",
    "\n",
    "Q.load_models(replay_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T18:12:16.001164Z",
     "start_time": "2018-05-28T18:12:15.992160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.critic.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T18:12:17.440415Z",
     "start_time": "2018-05-28T18:12:17.431412Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.actor.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T18:12:19.496073Z",
     "start_time": "2018-05-28T18:12:19.480144Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.env.pred_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T14:54:39.886003Z",
     "start_time": "2018-05-28T14:54:26.248967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(qmain)\n",
    "Q = qmain.CRSNet(.5)\n",
    "img, true_seg, true_cls = next(dqn_generator)\n",
    "seg, seg_var, cls = Q.run(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T14:54:41.512682Z",
     "start_time": "2018-05-28T14:54:41.378015Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis.draw_slices(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T14:56:11.067887Z",
     "start_time": "2018-05-28T14:56:10.964966Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis.draw_slices(true_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T14:55:59.697285Z",
     "start_time": "2018-05-28T14:55:59.556932Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis.draw_slices(seg, normalize=[0,.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T19:51:04.504974Z",
     "start_time": "2018-05-26T19:51:01.162486Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = M.fit_generator(gen, steps_per_epoch=2, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction_model = get_prediction_model()\n",
    "trainable_model = get_trainable_model(prediction_model)\n",
    "trainable_model.compile(optimizer='adam', loss=None)\n",
    "assert len(trainable_model.layers[-1].trainable_weights) == 2  # two log_vars, one for each output\n",
    "assert len(trainable_model.losses) == 1\n",
    "hist = trainable_model.fit([X, Y1, Y2], nb_epoch=nb_epoch, batch_size=batch_size, verbose=0)\n",
    "\n",
    "[np.exp(K.get_value(log_var[0]))**0.5 for log_var in trainable_model.layers[-1].log_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T01:08:37.572870Z",
     "start_time": "2018-05-26T01:08:37.552924Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config('etiology')\n",
    "T = config.Hyperparams()\n",
    "T.get_best_hyperparams()\n",
    "T.epochs = 30\n",
    "T.steps_per_epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-21T19:36:55.197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(cbuild)\n",
    "importlib.reload(dqna)\n",
    "#dqn_generator = cbuild._train_gen_dqn([])\n",
    "agent = dqna.train_dqn(dqn_generator, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.cls_model.fit(cls_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T03:15:14.525488Z",
     "start_time": "2018-05-21T03:15:14.521478Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T.padding = ['same', 'same']\n",
    "T.pool_sizes = [(2,2,2),(2,2,2)]\n",
    "T.f = [64,64,64,64,64,64,64]\n",
    "T.skip_con = True\n",
    "T.epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:34:45.693522Z",
     "start_time": "2018-05-19T21:34:45.678482Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drm.dcm2npy_batch(acc_nums=[\"E100113043\"])\n",
    "#vm.reset_accnum('E105464882')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T19:22:14.662640Z",
     "start_time": "2018-04-29T19:22:14.527783Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vm.plot_check(2, \"E102088195\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T04:11:27.139653Z",
     "start_time": "2018-05-02T04:11:26.670940Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vm.xref_dirs_with_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T13:53:09.930757Z",
     "start_time": "2018-05-02T04:11:28.356875Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crun.run_fixed_hyperparams([C], hyperparams=T)#C_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T15:18:23.881939Z",
     "start_time": "2018-05-22T15:18:20.054251Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(cbuild)\n",
    "model = cbuild.build_cnn_hyperparams(T)\n",
    "model.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T21:56:12.245374Z",
     "start_time": "2018-05-21T21:33:39.982268Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(cbuild)\n",
    "#Z_reader = ['E103312835_1','12823036_0','12569915_0','E102093118_0','E102782525_0','12799652_0','E100894274_0','12874178_3','E100314676_0','12842070_0','13092836_2','12239783_0','12783467_0','13092966_0','E100962970_0','E100183257_1','E102634440_0','E106182827_0','12582632_0','E100121654_0','E100407633_0','E105310461_0','12788616_0','E101225606_0','12678910_1','E101083458_1','12324408_0','13031955_0','E101415263_0','E103192914_0','12888679_2','E106096969_0','E100192709_1','13112385_1','E100718398_0','12207268_0','E105244287_0','E102095465_0','E102613189_0','12961059_0','11907521_0','E105311123_0','12552705_0','E100610622_0','12975280_0','E105918926_0','E103020139_1','E101069048_1','E105427046_0','13028374_0','E100262351_0','12302576_0','12451831_0','E102929168_0','E100383453_0','E105344747_0','12569826_0','E100168661_0','12530153_0','E104697262_0']\n",
    "X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4)#, Z_test_fixed=Z_reader)\n",
    "Z_test, Z_train_orig = Z\n",
    "X_train_orig, Y_train_orig = train_orig\n",
    "hist = model.fit_generator(train_generator, steps_per_epoch=T.steps_per_epoch, epochs=T.epochs, validation_data=[X_test, Y_test])#, callbacks=[T.early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T15:42:30.363207Z",
     "start_time": "2018-05-12T15:42:29.920558Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(join(C.model_dir, \"model_.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T22:46:55.730496Z",
     "start_time": "2018-05-10T22:46:55.703429Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict(X_train_orig[20:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T22:44:57.324854Z",
     "start_time": "2018-05-10T22:44:57.319840Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_orig[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_generator, steps_per_epoch=T.steps_per_epoch, epochs=T.epochs, validation_data=[X_test, Y_test])#, callbacks=[T.early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_with_bbox(fn_list[2], cls_mapping[wrong_guesses[2]])\n",
    "Y_pred = model.predict(X_test)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_test])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "#save_output(Z_test, y_pred, y_true)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "#y_true_simp, y_pred_simp, _ = cnna.merge_classes(y_true, y_pred)\n",
    "#print(accuracy_score(y_true_simp, y_pred_simp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
