{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.models\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from keras import backend as K\n",
    "from keras_contrib.layers.normalization import InstanceNormalization\n",
    "\n",
    "import argparse\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import csv\n",
    "import niftiutils.helper_fxns as hf\n",
    "import importlib\n",
    "import itertools\n",
    "from math import sqrt, log, pi, exp\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange, vectorize, guvectorize\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config()\n",
    "T = config.Hyperparams()\n",
    "#T.steps_per_epoch = 50\n",
    "#T.epochs= 1\n",
    "#T.get_best_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crun.run_fixed_hyperparams([C], hyperparams=T)#C_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = cbuild.build_cnn_hyperparams(T)\n",
    "model.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_reader = ['E103312835_1','12823036_0','12569915_0','E102093118_0','E102782525_0','12799652_0','E100894274_0','12874178_3','E100314676_0','12842070_0','13092836_2','12239783_0','12783467_0','13092966_0','E100962970_0','E100183257_1','E102634440_0','E106182827_0','12582632_0','E100121654_0','E100407633_0','E105310461_0','12788616_0','E101225606_0','12678910_1','E101083458_1','12324408_0','13031955_0','E101415263_0','E103192914_0','12888679_2','E106096969_0','E100192709_1','13112385_1','E100718398_0','12207268_0','E105244287_0','E102095465_0','E102613189_0','12961059_0','11907521_0','E105311123_0','12552705_0','E100610622_0','12975280_0','E105918926_0','E103020139_1','E101069048_1','E105427046_0','13028374_0','E100262351_0','12302576_0','12451831_0','E102929168_0','E100383453_0','E105344747_0','12569826_0','E100168661_0','12530153_0','E104697262_0']\n",
    "X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4, n_art=0)#, Z_test_fixed=Z_reader)\n",
    "Z_test, Z_train_orig = Z\n",
    "X_train_orig, Y_train_orig = train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.utils import multi_gpu_model\n",
    "#model = multi_gpu_model(model, gpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_generator, steps_per_epoch=T.steps_per_epoch, epochs=T.epochs, validation_data=[X_test, Y_test])#, callbacks=[T.early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('E:\\\\models\\\\model_reader_rcnn.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dr_methods as drm\n",
    "voi_df_art, voi_df_ven, voi_df_eq = drm.get_voi_dfs()\n",
    "small_voi_df = pd.read_csv(C.small_voi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_with_bbox(fn_list[2], cls_mapping[wrong_guesses[2]])\n",
    "Y_pred = model.predict(X_test)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_test])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "#save_output(Z_test, y_pred, y_true)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "#y_true_simp, y_pred_simp, _ = cnna.merge_classes(y_true, y_pred)\n",
    "#print(accuracy_score(y_true_simp, y_pred_simp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cnn_builder' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\cnn_builder.py'>"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(cbuild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4, n_art=0, Z_test=Z_reader)\n",
    "Z_test, Z_train_orig = Z\n",
    "Y_pred = model.predict(X_test)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_test])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "y_true_simp, y_pred_simp, _ = cbuild.merge_classes(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = [C.classes_to_include[y] for y in y_pred]\n",
    "Y_true = [C.classes_to_include[y] for y in y_true]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = pd.DataFrame([Z_test,Y_pred,Y_true])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a.to_csv('E:\\\\temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc6cls = []\n",
    "acc3cls = []\n",
    "\n",
    "for i in range(19):\n",
    "    model_num = 306+i\n",
    "    X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4, n_art=0, Z_test=df[df['model_num'] == model_num][\"z_test\"])\n",
    "    X_train_orig, Y_train_orig = train_orig\n",
    "    model = keras.models.load_model(os.path.join(C.model_dir, \"models_%d.hdf5\" % model_num)) #models_305\n",
    "    \n",
    "    Y_pred = model.predict(X_train_orig)\n",
    "    y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_train_orig])\n",
    "    y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "    acc6cls.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "    y_true_simp, y_pred_simp, _ = cbuild.merge_classes(y_true, y_pred)\n",
    "\n",
    "    acc3cls.append(accuracy_score(y_true_simp, y_pred_simp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Z:\\\\Inter-reader study\\\\Answer key.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = df[\"Class\"].values\n",
    "y_pred = df[\"Model\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, labels=C.classes_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\t0\t1\t0\t0\t1\n",
      "0\t9\t1\t0\t0\t0\n",
      "0\t1\t8\t0\t0\t0\n",
      "0\t0\t0\t10\t0\t0\n",
      "0\t0\t0\t0\t9\t0\n",
      "0\t0\t0\t0\t1\t9\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cm)):\n",
    "    print('\\t'.join(cm[:,i].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_list = fn_list + list(Z_test[~np.equal(y_pred, y_true)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_list = fn_list + list(Z_train_orig[~np.equal(y_pred, y_true)])\n",
    "fn_list = fn_list + list(Z_train_orig[~np.equal(y_pred, y_true)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_x_list = [x+\"(\"+str(voi_df_art[voi_df_art[\"id\"] == x[:-4]][\"x1\"].values[0])+\")\" for x in fn_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(fn_x_list, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_train_orig)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_train_orig])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "#cbuild.save_output(Z_train_orig, y_pred, y_true)#, save_dir=C.output_img_dir+\"\\\\training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'cnn_builder' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\cnn_builder.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(cbuild)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305\n",
    "#cnna.visualize_layer(model, 'conv3d_148')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dense_outputs = cbuild.build_pretrain_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_dropout = cbuild.build_model_forced_dropout(model, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filters_by_cls = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "fig = cnna.tsne(filters_by_cls)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hf.flatten([list(feature_sheet[\"evidence1\"+cls].dropna().values) for cls in C.classes_to_include]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "C=config.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filters_by_cls = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "filter_results = np.concatenate([filters_by_cls[cls] for cls in C.classes_to_include], axis=0)\n",
    "\n",
    "filter_avgs = np.mean(filter_results, axis=0)\n",
    "filter_stds = np.std(filter_results, axis=0)\n",
    "\n",
    "#filter_cls_avg_unscaled = {cls: np.mean(filter_results[cls], axis=0) for cls in C.classes_to_include}\n",
    "#filter_cls_avg_scaled = {cls: np.mean(filter_results[cls], axis=0) / filter_avgs for cls in C.classes_to_include}\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "all_features = list(feat_count.keys())\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = dict(zip(*np.unique(features_by_cls['colorectal'], return_counts=True)))\n",
    "for k in z:\n",
    "    print(k, \" (\",z[k],\")\", sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_features = cnna.get_annotated_files(features_by_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_filter_means = {}\n",
    "feature_filter_stds = {}\n",
    "feature_filters_scaled = {}#{cls: {} for cls in features}\n",
    "feature_filters = {f:np.empty([0,100]) for f in all_features}\n",
    "\n",
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs\n",
    "        \n",
    "    feature_filters[f] = (feature_filters[f] - filter_avgs) / filter_stds\n",
    "    \n",
    "    feature_filter_means[f] = np.mean(feature_filters[f], axis=0)# / filter_avgs\n",
    "    feature_filter_stds[f] = np.std(feature_filters[f], axis=0)# / filter_avgs\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > np.mean(ff), ff, 0)\n",
    "\n",
    "    #ff = feature_filters[cls][f]\n",
    "    #feature_filters[cls][f] = np.where(ff > 1*filter_cls_avg_unscaled[cls], ff, 0)\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > 1.3*filter_avgs, ff, 0)\n",
    "\n",
    "    feature_filters_scaled[f] = feature_filter_means[f] / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(all_features) # number of features\n",
    "num_units = 100 # number of dense units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_annotations = 5\n",
    "Z_test = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "indices_f = [orig_data_dict[cls][1] for cls in C.classes_to_include]\n",
    "indices_f = hf.flatten(indices_f)\n",
    "\n",
    "fixed_indices = np.empty([num_features, num_annotations])\n",
    "for f_ix,f in enumerate(all_features):\n",
    "    print(f)\n",
    "    fixed_indices[f_ix, :] = np.where(np.isin(indices_f, random.sample(set(Z_features[f]), num_annotations)))[0]\n",
    "fixed_indices = fixed_indices.astype(int)\n",
    "\n",
    "test_indices = np.where(np.isin(indices_f, Z_test))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = np.empty([num_features, num_units])\n",
    "sigma = np.empty([num_features, num_units])\n",
    "m = filter_avgs#np.empty(n) #dense unit mean without features\n",
    "s = filter_stds#np.empty(n) #dense unit stdev without features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta_11 = scipy.random.uniform(.2,.3, size=[num_features, num_features])\n",
    "theta_10 = scipy.random.uniform(.2,.3, size=[num_features, num_features])\n",
    "theta_01 = scipy.random.uniform(.2,.3, size=[num_features, num_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(num_features):\n",
    "    mu[i] = feature_filter_means[all_features[i]]\n",
    "    sigma[i] = feature_filter_stds[all_features[i]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "z_states = np.array([z for z in itertools.product([0,1], repeat=num_features) if sum(z) <= 4 and sum(z) >= 2])\n",
    "num_states = len(z_states)\n",
    "num_imgs = filter_results.shape[0]\n",
    "z_states_bool = [tuple([bool(x) for x in z]) for z in z_states]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_z_x = np.empty([num_imgs, num_states])\n",
    "p_x_z = np.empty([num_imgs, num_states])\n",
    "p_x = np.empty(num_imgs)\n",
    "Z = np.empty(num_imgs)\n",
    "\n",
    "eps = 10**-300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.02306342124938965\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "for _ in range(num_states*10):\n",
    "    np.nan_to_num(0)\n",
    "    #pass\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numba import float64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@guvectorize([(float64[:], float64, float64[:], float64, float64, float64[:], float64)], '(n),(),(n),(),(),(n)->(n)')\n",
    "def get_p_x_z(mu, m, sigma, s, x, z, res):\n",
    "    mean = np.dot(mu, z) + m\n",
    "    stdev = sqrt(np.dot(sigma**2, z) + s**2)\n",
    "    #stdev = sqrt(np.sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2)\n",
    "\n",
    "    res = 1/stdev * exp(-((x-mean)/stdev)**2/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_ix = 531"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1925"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "469"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(state_ixs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "@njit\n",
    "def get_all_p_x_z(mu, m, sigma, s, z_states, filter_results):\n",
    "    num_imgs = filter_results.shape[0]\n",
    "    num_states = len(z_states)\n",
    "    num_units = m.shape[0]\n",
    "    \n",
    "    tmp = np.empty(num_units)\n",
    "    p_x_z = np.zeros((num_imgs, num_states))\n",
    "    \n",
    "    for img_ix in range(num_imgs):\n",
    "        for state_ix in range(num_states):\n",
    "            z = np.array([float(z) for z in z_states[state_ix]])\n",
    "\n",
    "            for u_ix in range(num_units):\n",
    "                mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "                stdev = sqrt(np.dot(sigma[:, u_ix]**2, z) + s[u_ix]**2)\n",
    "                #stdev = sqrt(np.sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2)\n",
    "\n",
    "                #tmp[u_ix] = get_p_x_z(mu[:, u_ix], m[u_ix], sigma[:, u_ix], s[u_ix], filter_results[img_ix, u_ix], z)\n",
    "                tmp[u_ix] = 1/stdev * exp(-((filter_results[img_ix, u_ix]-mean)/stdev)**2/2)\n",
    "\n",
    "            #p_x_z[img_ix, state_ix] = get_p_x_z(mu, m, sigma, s, filter_results[img_ix], z)\n",
    "            p_x_z[img_ix, state_ix] = tmp.prod()\n",
    "    \n",
    "    return p_x_z\n",
    "\n",
    "@njit\n",
    "def get_all_p_z_x(p_x_z, Z, p_z):\n",
    "    p_z_x = np.empty(p_x_z.shape)\n",
    "    for img_ix in range(p_x_z.shape[0]):\n",
    "        for state_ix in range(p_x_z.shape[1]):\n",
    "            p_z_x[img_ix, state_ix] = p_x_z[img_ix, state_ix] * p_z[state_ix] / Z[img_ix]\n",
    "    return p_z_x\n",
    "\n",
    "@njit\n",
    "def update_thetas(p_z_x_sum, z_states_bool):\n",
    "    num_states = len(z_states_bool)\n",
    "    num_features = len(z_states_bool[0])\n",
    "    theta_11 = np.empty((num_features, num_features))\n",
    "    theta_10 = np.empty((num_features, num_features))\n",
    "    theta_01 = np.empty((num_features, num_features))\n",
    "    \n",
    "    for a in range(num_features):\n",
    "        for b in range(num_features):\n",
    "            num11 = 0\n",
    "            den11 = eps\n",
    "            num10 = 0\n",
    "            den10 = eps\n",
    "            num01 = 0\n",
    "            den01 = eps\n",
    "            for state_ix in range(num_states):\n",
    "                z = z_states_bool[state_ix]\n",
    "                num11 += p_z_x_sum[state_ix] * z[a]*z[b] * (1 - theta_10[a,b] - theta_01[a,b])\n",
    "                den11 += p_z_x_sum[state_ix] * (z[a]*z[b] + (not z[a] and not z[b]))\n",
    "                num10 += p_z_x_sum[state_ix] * (z[a] and not z[b]) * (1 - theta_11[a,b] - theta_01[a,b])\n",
    "                den10 += p_z_x_sum[state_ix] * ((z[a] and not z[b]) + (not z[a] and not z[b]))\n",
    "                num01 += p_z_x_sum[state_ix] * (not z[a] and z[b]) * (1 - theta_11[a,b] - theta_10[a,b])\n",
    "                den01 += p_z_x_sum[state_ix] * ((not z[a] and z[b]) + (not z[a] and not z[b]))\n",
    "\n",
    "            theta_11[a,b] = num11 / den11\n",
    "            theta_10[a,b] = num10 / den10\n",
    "            theta_01[a,b] = num01 / den01\n",
    "            \n",
    "    return theta_11, theta_10, theta_01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.15743041038513\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "p_x_z = get_all_p_x_z(mu, m, sigma, s, z_states, filter_results)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for img_ix in range(num_imgs):\n",
    "    f_ixs = [f_ix for f_ix in range(num_features) if img_ix in list(fixed_indices[f_ix, :])]\n",
    "    state_ixs = [state_ix for state_ix in range(num_states) \\\n",
    "                 if not np.all([z_states_bool[state_ix][f_ix] for f_ix in f_ixs])]\n",
    "    for state_ix in state_ixs:\n",
    "        p_x_z[img_ix, state_ix] = 0\n",
    "        \n",
    "    p_x_z[img_ix, :] = p_x_z[img_ix, :] / np.amax(p_x_z[img_ix, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_z = np.ones(num_states)\n",
    "for state_ix in range(num_states):\n",
    "    z = z_states[state_ix]\n",
    "    for a in range(num_features):\n",
    "        p_z[state_ix] *= np.product([theta_11[a,b]**(z[a]*z[b]) * theta_10[a,b]**(z[a]*(1-z[b])) \\\n",
    "                                  * theta_01[a,b]**((1-z[a])*z[b]) * \\\n",
    "                                (1 - theta_11[a,b] - theta_10[a,b] - theta_01[a,b]) ** ((1-z[a])*(1-z[b])) \\\n",
    "                                   for b in range(num_features) if a!=b])\n",
    "p_z = p_z / np.sum(p_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0060160160064697266\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "for img_ix in range(num_imgs):\n",
    "    Z[img_ix] = np.dot(p_x_z[img_ix, :], p_z)\n",
    "    #sum([p_x_z[img_ix, state_ix]*p_z[state_ix] for state_ix in range(num_states)])\n",
    "\n",
    "p_z_x = get_all_p_z_x(p_x_z, Z, p_z)\n",
    "#for img_ix in range(num_imgs):\n",
    "#    for state_ix in range(num_states):\n",
    "#        p_z_x[img_ix, state_ix] = np.nan_to_num(p_x_z[img_ix, state_ix] / Z[img_ix] * p_z[state_ix])\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5469076633453369\n"
     ]
    }
   ],
   "source": [
    "p_z_x_sum = np.sum(p_z_x, axis=0)\n",
    "\n",
    "t=time.time()\n",
    "theta_11, theta_10, theta_01 = update_thetas(p_z_x_sum, z_states_bool)\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu_est = np.empty([num_features, num_units])\n",
    "sigma_est = np.empty([num_features, num_units])\n",
    "m_est = np.empty([num_units])\n",
    "s_est = np.empty([num_units])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0 0.8428058624267578\n",
      "0 20 17.681885242462158\n",
      "0 40 34.561784505844116\n",
      "0 60 51.402652978897095\n",
      "0 80 68.29440379142761\n",
      "1 0 85.17049765586853\n",
      "1 20 102.0084981918335\n",
      "1 40 118.83294034004211\n",
      "1 60 135.69523072242737\n",
      "1 80 152.6137089729309\n",
      "2 0 169.44687271118164\n",
      "2 20 186.29037714004517\n",
      "2 40 203.12522220611572\n",
      "2 60 219.99858331680298\n",
      "2 80 236.8211965560913\n",
      "3 0 253.69192624092102\n",
      "3 20 270.5899968147278\n",
      "3 40 287.4365110397339\n",
      "3 60 304.2954292297363\n",
      "3 80 321.22752380371094\n",
      "4 0 338.09299492836\n",
      "4 20 354.9206337928772\n",
      "4 40 371.7900929450989\n",
      "4 60 388.69201493263245\n",
      "4 80 405.5725450515747\n",
      "5 0 422.41166520118713\n",
      "5 20 439.31202697753906\n",
      "5 40 456.1804099082947\n",
      "5 60 473.02006006240845\n",
      "5 80 489.9057469367981\n",
      "6 0 506.82116889953613\n",
      "6 20 523.7082476615906\n",
      "6 40 540.6029245853424\n",
      "6 60 557.5463864803314\n",
      "6 80 574.9331634044647\n",
      "7 0 591.7805843353271\n",
      "7 20 608.6805324554443\n",
      "7 40 625.6790654659271\n",
      "7 60 642.5873913764954\n",
      "7 80 659.4829688072205\n",
      "8 0 676.3667731285095\n",
      "8 20 693.290554523468\n",
      "8 40 710.2080421447754\n",
      "8 60 727.1249589920044\n",
      "8 80 744.0715997219086\n",
      "9 0 760.9892404079437\n",
      "9 20 777.9056169986725\n",
      "9 40 794.8053965568542\n",
      "9 60 811.7396101951599\n",
      "9 80 828.6712598800659\n",
      "10 0 845.5988304615021\n",
      "10 20 862.5519607067108\n",
      "10 40 879.4774866104126\n",
      "10 60 896.4449882507324\n",
      "10 80 913.407692193985\n",
      "11 0 930.3803570270538\n",
      "11 20 947.2970063686371\n",
      "11 40 964.2027645111084\n",
      "11 60 981.1504716873169\n",
      "11 80 998.0518243312836\n",
      "12 0 1014.9521524906158\n",
      "12 20 1031.8836617469788\n",
      "12 40 1048.811870098114\n",
      "12 60 1065.7271347045898\n",
      "12 80 1082.664538860321\n",
      "13 0 1099.6297006607056\n",
      "13 20 1116.602153301239\n",
      "13 40 1133.5202221870422\n",
      "13 60 1150.4842808246613\n",
      "13 80 1167.414934873581\n",
      "14 0 1184.3488357067108\n",
      "14 20 1201.264607667923\n",
      "14 40 1218.2115166187286\n",
      "14 60 1235.2081894874573\n",
      "14 80 1252.1242644786835\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        num = 0\n",
    "        den = eps\n",
    "            \n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean_adj = np.dot(mu[:, u_ix], z) + m[u_ix] - mu[f_ix, u_ix]\n",
    "            var = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2\n",
    "            \n",
    "            den += sum(p_z_x[:, state_ix])/var\n",
    "        \n",
    "            for img_ix in range(num_imgs):\n",
    "                x = filter_results[img_ix]\n",
    "                \n",
    "                num += p_z_x[img_ix, state_ix]/var * (x[u_ix] - mean_adj)\n",
    "            \n",
    "        mu_est[f_ix, u_ix] = num / den\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 3.4662528038024902\n",
      "20 72.56013035774231\n",
      "40 141.05970907211304\n",
      "60 209.7487871646881\n",
      "80 278.78854632377625\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "for u_ix in range(num_units):\n",
    "    num = 0\n",
    "    den = eps\n",
    "\n",
    "    for state_ix in range(num_states):\n",
    "        z = z_states[state_ix]\n",
    "\n",
    "        mean_adj = np.dot(mu[:, u_ix], z)\n",
    "        var = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2\n",
    "\n",
    "        den += sum(p_z_x[:, state_ix])/var\n",
    "\n",
    "        for img_ix in range(num_imgs):\n",
    "            x = filter_results[img_ix]\n",
    "\n",
    "            num += p_z_x[img_ix, state_ix]/var * (x[u_ix] - mean_adj)\n",
    "\n",
    "    m_est[u_ix] = num / den\n",
    "\n",
    "    if u_ix % 20 == 0:\n",
    "        print(u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\ipykernel_launcher.py:15: RuntimeWarning: overflow encountered in double_scalars\n",
      "  from ipykernel import kernelapp as app\n",
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\scipy\\optimize\\optimize.py:628: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  grad[k] = (f(*((xk + d,) + args)) - f0) / d[k]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 5.373857259750366\n",
      "20 121.10477900505066\n",
      "40 233.05421948432922\n",
      "60 346.0559916496277\n",
      "80 459.7740137577057\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "#state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "var_adj = np.zeros([num_states])\n",
    "a_i = np.zeros([num_states])\n",
    "c_i = np.zeros([num_states])\n",
    "\n",
    "for u_ix in range(num_units):\n",
    "    for state_ix in range(num_states):\n",
    "        mean = np.dot(mu[:, u_ix], z_states[state_ix]) + m[u_ix]\n",
    "        a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)]) / 2\n",
    "        c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "    temp = scipy.optimize.minimize(\\\n",
    "                lambda Var: sum([a_i[state_ix] * (sum(np.where(z_states[state_ix]==1,\n",
    "                   Var[:-1], 0))+Var[-1])**(-2) - c_i[state_ix]*log((sum(np.where(z_states[state_ix]==1,\n",
    "                   Var[:-1], 0))+Var[-1])**(-.5) / sqrt(2*pi)) for state_ix in range(num_states)]),\n",
    "                  np.concatenate([sigma[:, u_ix]**2, [s[u_ix]**2]]), \n",
    "                  bounds=tuple(itertools.repeat((eps, None),num_features+1)))\n",
    "    \n",
    "    s_est[u_ix] = temp['x'][-1]\n",
    "    sigma_est[:, u_ix] = temp['x'][:-1]\n",
    "    if u_ix % 20 == 0:\n",
    "        print(u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigma = sigma_est\n",
    "s = s_est\n",
    "m = m_est\n",
    "mu = mu_est"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2.619457721710205\n",
      "50 130.82673168182373\n",
      "100 258.78862023353577\n",
      "150 386.9386417865753\n",
      "200 515.1857304573059\n",
      "250 643.0928471088409\n",
      "300 769.9665620326996\n",
      "350 898.1800892353058\n",
      "400 1026.0915036201477\n",
      "450 1156.1958348751068\n",
      "500 1285.743127822876\n",
      "550 1414.3263275623322\n",
      "600 1543.4027721881866\n"
     ]
    }
   ],
   "source": [
    "t=time.time()\n",
    "for img_ix in range(num_imgs):\n",
    "    x = filter_results[img_ix]\n",
    "    \n",
    "    #f_ixs = [f_ix for f_ix in range(num_features) if img_ix in fixed_indices[f_ix, :]]\n",
    "    \n",
    "    for state_ix in range(num_states):\n",
    "        z = z_states[state_ix]\n",
    "        p_x_z[img_ix, state_ix] = 1\n",
    "        \n",
    "        for u_ix in range(num_units):\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            stdev = sqrt(sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2)\n",
    "            \n",
    "            p_x_z[img_ix, state_ix] *= 1/stdev * exp(-((x[u_ix]-mean)/stdev)**2/2) #norm.pdf(x[u_ix], mean, stdev)\n",
    "    \n",
    "    if img_ix % 50 == 0:\n",
    "        print(img_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([a_i[state_ix]/(x+var_adj[state_ix])**2 - \\\n",
    "                                         c_i[state_ix]/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for test_ix in test_indices[13:]:\n",
    "    p_zi_x = np.zeros([num_features])\n",
    "    for f_ix in range(num_features):\n",
    "        state_ixs = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "        p_zi_x[f_ix] = sum(p_z_x[test_ix, state_ixs])\n",
    "        #for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "        #    output[z] = output[z] + [f, strength]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "infiltrative 0.6193980256219328\n",
      "progressive or concentric enhancement 0.5101249562784003\n",
      "progressive centripetal filling 0.4365394360701675\n",
      "regular spherical hypointense mass 0.3413705999347494\n",
      "arterial enhancement 0.3247717447201581\n",
      "venous washout 0.3241805702526745\n",
      "hypointense without enhancement 0.2986651979541879\n",
      "delayed isointensity 0.15911879699347423\n",
      "thin well-defined walls 0.1174462897171592\n"
     ]
    }
   ],
   "source": [
    "for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "    if strength==0:\n",
    "        break\n",
    "    print(all_features[f], -80/log(strength))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for cls in C.classes_to_include:\n",
    "    x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == indices_f[test_ix])]\n",
    "    if len(x_test_quick) > 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAACFCAYAAABsdIKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nvCAbY0wheEE2xphC+AeUzK7N10l60QAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1d104cc55f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf.draw_slices(x_test_quick[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "#state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "#p_z_x does not need to be rescaled because the factor cancels out\n",
    "var_adj = np.zeros([num_states])\n",
    "a_i = np.zeros([num_states])\n",
    "c_i = np.zeros([num_states])\n",
    "\n",
    "for u_ix in range(num_units):\n",
    "    for state_ix in range(num_states):\n",
    "        mean = np.dot(mu[:, u_ix], z_states[state_ix]) + m[u_ix]\n",
    "        a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "        c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "    temp = scipy.optimize.linearmixing(\\\n",
    "                lambda Var: sum([a_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2)**2 - \\\n",
    "                 c_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2) for state_ix in range(num_states)]), \\\n",
    "                  np.concatenate([sigma[:, u_ix], [s[u_ix]]]), verbose=True, maxiter=10000)\n",
    "    \n",
    "    #sigma_est[:, u_ix]\n",
    "    if u_ix % 20 == 0:\n",
    "        print(u_ix, time.time()-t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([a_i[state_ix]/(x+var_adj[state_ix])**2 - \\\n",
    "                                         c_i[state_ix]/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_test_features = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares approach\n",
    "Theta = np.array([feature_filter_means[f] for f in all_features])\n",
    "Theta = np.transpose(Theta, (1,0))\n",
    "\n",
    "filters_test = {}\n",
    "features_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)\n",
    "    filters_test[cls] = (filters_test[cls] - filter_avgs) / filter_stds\n",
    "    features_test[cls] = np.linalg.lstsq(Theta, np.transpose(filters_test[cls], (1,0)))[0]\n",
    "    #filters_test[cls] = np.apply_along_axis(lambda x: x / filter_avgs, 1, filters_test[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import matmul, diag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covar_to_corr(covar):\n",
    "    A = np.diag(np.diag(covar)**(-0.5))\n",
    "    return np.matmul(np.matmul(A, covar), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = covar_to_corr(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(corr - np.linalg.pinv(np.diag(np.diag(np.linalg.pinv(corr)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cutoff_eigenval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.random.normal(size=filter_results.shape)\n",
    "dummy_fa = FactorAnalysis()\n",
    "dummy_fa.fit(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.linalg.eigvals(fa.get_covariance()) > np.mean(np.linalg.eigvals(dummy_fa.get_covariance())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=10,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.fit(filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89081373261106078"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=3\n",
    "fa.get_covariance()[x,x] - fa.noise_variance_[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = cnna.get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))\n",
    "\n",
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "for cls in C.classes_to_include:\n",
    "    for img_num in range(len(filters_test[cls])):\n",
    "        z = Z_test[cls][img_num]\n",
    "        x = np.expand_dims(x_test[cls][img_num], axis=0)\n",
    "        evidence = {}\n",
    "        \n",
    "        output[z] = [cls]\n",
    "        \n",
    "        preds = model.predict(x, verbose=False)[0]\n",
    "        for pred_cls, pred_conf in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:2]:\n",
    "            output[z] = output[z] + [pred_cls]\n",
    "        \n",
    "        #for f in all_features:\n",
    "        #    evidence[f + \"/\" + str(cls_features[f])] = get_evidence_strength(feature_filters[f], filters_test[cls][img_num])\n",
    "        \n",
    "        for i in range(len(all_features)):\n",
    "            evidence[all_features[i] + \"/\" + str(cls_features[all_features[i]])] = features_test[cls][i, img_num]\n",
    "        \n",
    "        f1='infiltrative'\n",
    "        f2='lobulated margins'\n",
    "        if evidence[f1 + \"/\" + str(cls_features[f1])] < evidence[f2 + \"/\" + str(cls_features[f2])]:\n",
    "            evidence.pop(f1 + \"/\" + str(cls_features[f1]))\n",
    "        else:\n",
    "            evidence.pop(f2 + \"/\" + str(cls_features[f2]))\n",
    "        \n",
    "        for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True):\n",
    "            output[z] = output[z] + [f, strength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred5.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_cls2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_features)):\n",
    "        writer.writerow([Z_test_features[z_num]] + [output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][3], \\\n",
    "                        output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][5]] + output[Z_test_features[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:\\\\filters_pred3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "for _,row in df.iterrows():\n",
    "    if row[\"pred_cls1\"] in row[\"feature_1\"]:\n",
    "        agree += 1\n",
    "print(agree/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
