{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T21:18:39.013292Z",
     "start_time": "2018-05-29T21:18:35.077711Z"
    }
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import importlib\n",
    "import itertools\n",
    "import math\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from os.path import *\n",
    "import psutil\n",
    "import shutil\n",
    "\n",
    "import keras.layers as layers\n",
    "import keras.models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import ddpg.env as denv\n",
    "import ddpg.main as qmain\n",
    "import ddpg.learning_nets as ln\n",
    "import ddpg.task_nets as tn\n",
    "import dr_methods as drm\n",
    "import seg_methods as sm\n",
    "import niftiutils.helper_fxns as hf\n",
    "import niftiutils.transforms as tr\n",
    "import niftiutils.visualization as vis\n",
    "import voi_methods as vm\n",
    "\n",
    "%matplotlib inline\n",
    "np.set_printoptions(3, suppress=True)\n",
    "#np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T21:18:41.307116Z",
     "start_time": "2018-05-29T21:18:41.302131Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def memory():\n",
    "    pid = os.getpid()\n",
    "    py = psutil.Process(pid)\n",
    "    gigs = py.memory_info()[0]/2.**30\n",
    "    print('Memory use:', gigs)\n",
    "    return gigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T21:18:43.497011Z",
     "start_time": "2018-05-29T21:18:43.493023Z"
    }
   },
   "outputs": [],
   "source": [
    "importlib.reload(cbuild)\n",
    "C = config.Config()\n",
    "dqn_generator = cbuild._train_gen_ddpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-29T21:18:48.229Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3176: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Episode 0 Replay self.q_buffer 660\n",
      "4853084','4987127','4673851','\tAction [0.829 0.793 0.283 0.449 0.8   0.121 0.709 0.901 0.205 0.291] Reward 0.1\n",
      "\tAction [0.31  0.956 0.694 0.598 0.339 0.557 0.466 0.559 0.692 0.708] Reward 8.5\n",
      "\tAction [0.6   0.572 0.15  0.736 0.203 0.22  0.473 0.348 0.702 0.831] Reward -0.7\n",
      "\tAction [0.112 0.147 0.577 0.75  0.856 0.861 0.64  0.615 0.53  0.726] Reward 23.6\n",
      "\tAction [0.824 0.84  0.438 0.43  0.34  0.405 0.783 0.898 0.577 0.799] Reward 11.3\n",
      "\tAction [0.806 0.684 0.452 0.541 0.964 0.934 0.524 0.576 0.776 0.589] Reward 24.6\n",
      "\tAction [0.531 0.237 0.503 0.289 0.843 0.886 0.742 0.606 0.618 0.157] Reward 6.6\n",
      "\tAction [0.648 0.829 0.615 0.432 0.394 0.852 0.548 0.963 0.179 0.894] Reward 4.0\n",
      "\tAction [0.638 0.81  0.712 0.477 0.167 0.119 0.544 0.566 0.424 0.91 ] Reward 0.6\n",
      "\tAction [0.715 0.933 0.273 0.43  0.446 0.481 0.311 0.864 0.539 0.047] Reward -0.1\n",
      "\tAction [0.684 0.237 0.944 0.854 0.595 0.46  0.055 0.343 0.761 0.52 ] Reward -5.6\n",
      "\tAction [0.084 0.364 0.691 0.392 0.19  0.745 0.615 0.485 0.552 0.667] Reward 5.1\n",
      "\tAction [0.871 0.365 0.847 0.747 0.419 0.201 0.911 0.249 0.315 0.33 ] Reward 0.7\n",
      "\tAction [0.374 0.517 0.879 0.659 0.35  0.161 0.077 0.802 0.212 0.649] Reward -0.5\n",
      "\tAction [0.829 0.252 0.187 0.82  0.372 0.688 0.944 0.791 0.345 0.259] Reward -1.6\n",
      "\tAction [0.227 0.321 0.076 0.853 0.502 0.147 0.119 0.675 0.327 0.667] Reward -6.4\n",
      "\tAction [0.567 0.578 0.563 0.75  0.492 0.795 0.152 0.781 0.761 0.42 ] Reward -20.0\n",
      "\tAction [0.926 0.33  0.699 0.134 0.582 0.283 0.556 0.559 0.312 0.269] Reward -0.7\n",
      "\tAction [0.437 0.216 0.802 0.233 0.254 0.575 0.221 0.304 0.803 0.8  ] Reward -2.5\n",
      "\tAction [0.152 0.882 0.72  0.346 0.733 0.782 0.673 0.93  0.409 0.309] Reward 27.5\n",
      "\tAction [0.626 0.914 0.754 0.208 0.112 0.037 0.836 0.404 0.804 0.833] Reward -0.3\n",
      "\tAction [0.666 0.754 0.405 0.357 0.714 0.515 0.661 0.355 0.192 0.307] Reward 11.4\n",
      "\tAction [0.197 0.4   0.783 0.645 0.869 0.249 0.072 0.555 0.537 0.581] Reward -5.0\n",
      "\tAction [0.167 0.685 0.476 0.417 0.403 0.686 0.892 0.959 0.71  0.103] Reward 7.3\n",
      "\tAction [0.215 0.019 0.222 0.078 0.013 0.286 0.059 0.549 0.497 0.307] Reward -0.2\n",
      "\tAction [0.761 0.662 0.035 0.342 0.088 0.536 0.175 0.708 0.244 0.608] Reward -0.2\n",
      "\tAction [0.369 0.914 0.246 0.595 0.742 0.328 0.086 0.579 0.568 0.47 ] Reward 7.8\n",
      "\tAction [0.937 0.487 0.501 0.811 0.424 0.068 0.833 0.949 0.105 0.732] Reward 1.2\n",
      "\tAction [0.684 0.618 0.79  0.529 0.476 0.351 0.334 0.256 0.728 0.149] Reward 0.8\n",
      "\tAction [0.155 0.857 0.732 0.186 0.738 0.563 0.269 0.732 0.664 0.575] Reward -0.0\n",
      "\tAction [0.156 0.82  0.837 0.6   0.312 0.612 0.192 0.238 0.178 0.102] Reward 7.2\n",
      "\tAction [0.184 0.372 0.714 0.151 0.641 0.52  0.131 0.932 0.324 0.222] Reward 1.6\n",
      "\tAction [0.311 0.595 0.462 0.287 0.383 0.477 0.185 0.718 0.828 0.365] Reward 0.3\n",
      "\tAction [0.09  0.637 0.584 0.869 0.335 0.324 0.051 0.733 0.238 0.481] Reward 3.2\n",
      "\tAction [0.451 0.359 0.803 0.312 0.077 0.927 0.05  0.915 0.569 0.705] Reward -0.0\n",
      "\tAction [0.729 0.545 0.461 0.347 0.395 0.131 0.078 0.981 0.23  0.093] Reward 0.2\n",
      "\tAction [0.223 0.717 0.375 0.329 0.432 0.822 0.072 0.982 0.063 0.797] Reward 9.5\n",
      "\tAction [0.49  0.634 0.701 0.134 0.667 0.838 0.07  0.532 0.554 0.84 ] Reward -0.4\n",
      "\tAction [0.744 0.925 0.328 0.289 0.894 0.737 0.181 0.959 0.317 0.199] Reward 0.8\n",
      "\tAction [0.778 0.369 0.377 0.306 0.73  0.614 0.141 0.824 0.062 0.34 ] Reward 5.2\n",
      "\tAction [0.431 0.599 0.541 0.734 0.816 0.677 0.043 0.812 0.128 0.176] Reward -7.8\n",
      "\tAction [0.479 0.679 0.124 0.594 0.168 0.943 0.037 0.951 0.037 0.767] Reward -0.8\n",
      "\tAction [0.702 0.152 0.873 0.247 0.164 0.671 0.325 0.863 0.81  0.242] Reward -0.4\n",
      "\tAction [0.585 0.874 0.813 0.007 0.196 0.542 0.115 0.917 0.334 0.18 ] Reward -0.0\n",
      "\tAction [0.139 0.357 0.512 0.087 0.487 0.705 0.088 0.818 0.205 0.371] Reward -1.2\n",
      "\tAction [0.36  0.74  0.499 0.015 0.487 0.893 0.067 0.922 0.46  0.619] Reward -1.2\n",
      "\tAction [0.195 0.232 0.688 0.276 0.074 0.959 0.082 0.912 0.092 0.287] Reward -0.6\n",
      "\tAction [0.164 0.647 0.717 0.037 0.155 0.852 0.197 0.469 0.067 0.255] Reward 0.4\n",
      "\tAction [0.229 0.738 0.826 0.062 0.283 0.851 0.088 0.943 0.524 0.038] Reward -0.5\n",
      "\tAction [0.141 0.658 0.814 0.129 0.213 0.706 0.014 0.966 0.502 0.265] Reward 0.3\n",
      "Memory use: 4.784198760986328\n",
      "TOTAL REWARD: 113.2 (50 steps)\n",
      "\n",
      "Episode 1 Replay self.q_buffer 710\n",
      "\tAction [0.192 0.567 0.696 0.261 0.097 0.702 0.138 0.853 0.05  0.11 ] Reward 7.3\n",
      "\tAction [0.244 0.519 0.423 0.096 0.506 0.736 0.042 0.998 0.114 0.214] Reward 15.2\n",
      "\tAction [0.15  0.897 0.046 0.05  0.243 0.963 0.114 0.764 0.02  0.926] Reward 10.3\n",
      "\tAction [0.126 0.672 0.849 0.27  0.432 0.989 0.21  0.982 0.262 0.012] Reward 21.3\n",
      "\tAction [0.142 0.977 0.828 0.637 0.12  0.855 0.081 0.933 0.048 0.428] Reward 11.8\n",
      "\tAction [0.64  0.963 0.528 0.092 0.108 0.824 0.154 0.838 0.031 0.526] Reward 7.5\n",
      "\tAction [0.24  0.592 0.418 0.44  0.625 0.907 0.126 0.989 0.336 0.063] Reward 98.9\n",
      "\tAction [0.445 0.919 0.183 0.041 0.375 0.879 0.069 0.981 0.307 0.086] Reward 5.4\n",
      "\tAction [0.665 0.801 0.183 0.199 0.03  0.887 0.045 0.95  0.2   0.318] Reward 2.4\n",
      "\tAction [0.149 0.794 0.543 0.079 0.376 0.892 0.069 0.978 0.263 0.004] Reward 3.2\n",
      "\tAction [0.203 0.963 0.127 0.425 0.078 0.95  0.068 0.524 0.099 0.15 ] Reward 15.6\n",
      "\tAction [0.345 0.299 0.168 0.158 0.04  0.94  0.107 0.985 0.147 0.124] Reward 1.5\n",
      "\tAction [0.204 0.962 0.043 0.145 0.272 0.977 0.024 0.979 0.312 0.715] Reward 4.3\n",
      "\tAction [0.206 0.98  0.268 0.485 0.02  0.643 0.008 0.998 0.297 0.239] Reward 1.4\n",
      "\tAction [0.165 0.941 0.088 0.061 0.073 0.92  0.048 0.878 0.191 0.041] Reward -0.3\n",
      "\tAction [0.507 0.707 0.194 0.189 0.125 0.964 0.014 0.639 0.058 0.089] Reward 4.3\n",
      "\tAction [0.038 0.593 0.171 0.205 0.195 0.493 0.205 0.853 0.532 0.326] Reward 3.4\n",
      "\tAction [0.148 0.613 0.122 0.147 0.022 0.991 0.047 0.958 0.162 0.771] Reward 1.3\n",
      "\tAction [0.129 0.43  0.303 0.013 0.29  0.807 0.292 0.988 0.54  0.383] Reward 3.4\n",
      "\tAction [0.185 0.629 0.897 0.128 0.153 0.889 0.211 0.964 0.132 0.086] Reward 0.3\n",
      "\tAction [0.068 0.927 0.417 0.181 0.261 0.943 0.011 0.915 0.408 0.568] Reward 2.3\n",
      "\tAction [0.067 0.721 0.1   0.012 0.135 0.991 0.018 0.979 0.489 0.217] Reward 1.6\n",
      "\tAction [0.375 0.983 0.052 0.299 0.172 0.92  0.027 0.996 0.835 0.36 ] Reward 9.6\n",
      "\tAction [0.148 0.72  0.006 0.024 0.243 0.772 0.028 0.894 0.618 0.132] Reward -0.4\n",
      "\tAction [0.085 0.815 0.022 0.46  0.336 0.968 0.061 0.972 0.7   0.232] Reward 1.5\n",
      "\tAction [0.234 0.972 0.095 0.109 0.063 0.429 0.06  0.899 0.565 0.367] Reward 0.8\n",
      "\tAction [0.161 0.851 0.208 0.049 0.031 0.872 0.013 0.973 0.634 0.39 ] Reward -0.9\n",
      "\tAction [0.26  0.968 0.28  0.209 0.068 0.818 0.021 0.864 0.467 0.478] Reward 0.9\n",
      "\tAction [0.53  0.9   0.097 0.137 0.796 0.94  0.142 0.979 0.024 0.14 ] Reward 8.1\n",
      "\tAction [0.668 0.928 0.044 0.139 0.848 0.928 0.032 0.357 0.138 0.036] Reward 8.6\n",
      "\tAction [0.098 0.76  0.292 0.049 0.237 0.99  0.003 0.981 0.195 0.207] Reward 0.1\n",
      "\tAction [0.47  0.761 0.024 0.537 0.027 0.841 0.058 0.899 0.036 0.609] Reward 3.3\n",
      "\tAction [0.214 0.73  0.158 0.36  0.094 0.951 0.056 0.988 0.07  0.181] Reward -0.3\n",
      "\tAction [0.481 0.705 0.006 0.094 0.084 0.875 0.028 0.934 0.263 0.086] Reward 0.3\n",
      "\tAction [0.219 0.667 0.212 0.117 0.189 0.735 0.009 0.961 0.067 0.206] Reward 0.1\n",
      "\tAction [0.085 0.687 0.026 0.301 0.285 0.832 0.072 0.966 0.267 0.167] Reward 1.2\n",
      "\tAction [0.089 0.966 0.067 0.043 0.188 0.784 0.064 0.982 0.037 0.041] Reward 0.7\n",
      "\tAction [0.25  0.883 0.148 0.011 0.037 0.814 0.147 0.651 0.114 0.105] Reward 0.3\n",
      "\tAction [0.027 0.986 0.111 0.02  0.074 0.757 0.428 0.855 0.26  0.327] Reward -1.2\n",
      "\tAction [0.078 0.865 0.068 0.703 0.611 0.786 0.003 0.979 0.346 0.193] Reward 8.0\n",
      "\tAction [0.382 0.888 0.063 0.08  0.512 0.874 0.052 0.989 0.395 0.223] Reward -0.7\n",
      "\tAction [0.369 0.972 0.337 0.242 0.025 0.801 0.062 0.956 0.032 0.777] Reward 1.4\n",
      "\tAction [0.112 0.959 0.086 0.083 0.387 0.838 0.052 0.797 0.09  0.211] Reward 0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAction [0.16  0.327 0.035 0.199 0.416 0.969 0.149 0.794 0.101 0.085] Reward 4.5\n",
      "\tAction [0.031 0.74  0.718 0.033 0.316 0.898 0.261 0.946 0.032 0.193] Reward 0.4\n",
      "\tAction [0.078 0.209 0.462 0.017 0.216 0.421 0.086 0.984 0.171 0.045] Reward 6.5\n",
      "\tAction [0.315 0.925 0.719 0.028 0.087 0.882 0.05  0.982 0.662 0.156] Reward -0.9\n",
      "\tAction [0.348 0.389 0.63  0.264 0.125 0.948 0.062 0.923 0.42  0.195] Reward 5.3\n",
      "\tAction [0.1   0.976 0.172 0.066 0.017 0.96  0.181 0.941 0.02  0.435] Reward -0.3\n",
      "\tAction [0.203 0.656 0.751 0.857 0.007 0.956 0.03  0.972 0.078 0.025] Reward 6.4\n",
      "Memory use: 2.693572998046875\n",
      "TOTAL REWARD: 286.0 (50 steps)\n",
      "\n",
      "Episode 2 Replay self.q_buffer 760\n",
      "4034641','4547885','\tAction [0.037 0.976 0.168 0.049 0.304 0.782 0.262 0.917 0.363 0.517] Reward 5.1\n",
      "\tAction [0.38  0.97  0.556 0.154 0.075 0.966 0.055 0.897 0.121 0.851] Reward 2.0\n",
      "\tAction [0.475 0.715 0.278 0.022 0.148 0.682 0.01  0.986 0.012 0.102] Reward -0.1\n",
      "\tAction [0.035 0.927 0.291 0.582 0.054 0.994 0.143 0.811 0.635 0.141] Reward -1.7\n",
      "\tAction [0.54  0.841 0.73  0.22  0.184 0.596 0.038 0.968 0.461 0.577] Reward 5.7\n",
      "\tAction [0.28  0.649 0.785 0.3   0.026 0.723 0.31  0.549 0.073 0.024] Reward 1.5\n",
      "\tAction [0.042 0.524 0.175 0.174 0.342 0.874 0.095 0.85  0.088 0.181] Reward 9.1\n",
      "\tAction [0.145 0.772 0.981 0.557 0.065 0.595 0.02  0.921 0.312 0.548] Reward 4.0\n",
      "\tAction [0.128 0.859 0.293 0.161 0.243 0.553 0.073 0.964 0.361 0.285] Reward -0.1\n",
      "\tAction [0.048 0.342 0.791 0.062 0.301 0.304 0.261 0.98  0.048 0.301] Reward 0.1\n",
      "\tAction [0.435 0.947 0.614 0.478 0.155 0.251 0.511 0.947 0.154 0.065] Reward 3.0\n",
      "\tAction [0.025 0.923 0.451 0.563 0.017 0.599 0.218 0.93  0.434 0.525] Reward -0.6\n",
      "\tAction [0.009 0.693 0.102 0.407 0.305 0.732 0.105 0.791 0.398 0.023] Reward 1.6\n",
      "\tAction [0.083 0.801 0.164 0.305 0.057 0.852 0.024 0.945 0.497 0.447] Reward 1.1\n",
      "\tAction [0.02  0.947 0.513 0.025 0.022 0.8   0.183 0.841 0.443 0.606] Reward -1.1\n",
      "\tAction [0.646 0.97  0.56  0.073 0.115 0.378 0.152 0.986 0.232 0.181] Reward 0.9\n",
      "\tAction [0.112 0.166 0.145 0.14  0.326 0.217 0.068 0.904 0.453 0.245] Reward 4.5\n",
      "\tAction [0.472 0.791 0.334 0.051 0.203 0.345 0.021 0.945 0.07  0.392] Reward -0.5\n",
      "\tAction [0.07  0.774 0.292 0.008 0.149 0.739 0.008 0.817 0.07  0.212] Reward 0.6\n",
      "\tAction [0.055 0.636 0.569 0.242 0.407 0.982 0.008 0.682 0.403 0.034] Reward -0.5\n",
      "\tAction [0.011 0.896 0.319 0.048 0.052 0.894 0.021 0.946 0.122 0.05 ] Reward 0.5\n",
      "\tAction [0.173 0.786 0.248 0.412 0.086 0.858 0.078 0.941 0.316 0.032] Reward -0.8\n",
      "\tAction [0.117 0.738 0.743 0.316 0.036 0.957 0.173 0.418 0.278 0.032] Reward 0.5\n",
      "\tAction [0.11  0.8   0.573 0.134 0.107 0.629 0.002 0.995 0.108 0.053] Reward -0.5\n",
      "\tAction [0.12  0.985 0.364 0.08  0.172 0.501 0.058 0.967 0.047 0.455] Reward -0.1\n",
      "\tAction [0.184 0.788 0.499 0.253 0.053 0.643 0.022 0.966 0.691 0.337] Reward 1.1\n",
      "\tAction [0.691 0.852 0.676 0.501 0.062 0.239 0.088 0.857 0.029 0.539] Reward -1.0\n",
      "\tAction [0.339 0.766 0.19  0.125 0.368 0.841 0.183 0.99  0.021 0.348] Reward -1.0\n",
      "\tAction [0.196 0.963 0.78  0.28  0.019 0.979 0.014 0.944 0.217 0.093] Reward -2.0\n",
      "\tAction [0.13  0.366 0.341 0.376 0.279 0.803 0.144 0.875 0.026 0.266] Reward -1.4\n",
      "\tAction [0.06  0.833 0.668 0.434 0.226 0.952 0.191 0.713 0.106 0.595] Reward 1.2\n",
      "\tAction [0.016 0.915 0.9   0.434 0.171 0.871 0.138 0.954 0.018 0.093] Reward -1.8\n",
      "\tAction [0.172 0.672 0.482 0.073 0.023 0.684 0.313 0.978 0.066 0.008] Reward -0.1\n",
      "\tAction [0.024 0.905 0.517 0.047 0.101 0.913 0.089 0.98  0.148 0.368] Reward 0.7\n",
      "\tAction [0.225 0.968 0.948 0.091 0.018 0.547 0.321 0.987 0.105 0.101] Reward -0.7\n",
      "\tAction [0.278 0.909 0.37  0.184 0.335 0.868 0.143 0.944 0.141 0.369] Reward 0.2\n",
      "\tAction [0.251 0.874 0.136 0.209 0.336 0.979 0.042 0.956 0.095 0.877] Reward 3.6\n",
      "\tAction [0.881 0.896 0.698 0.577 0.035 0.204 0.149 0.866 0.322 0.061] Reward -1.3\n",
      "\tAction [0.02  0.626 0.938 0.246 0.186 0.976 0.015 0.988 0.061 0.418] Reward 0.2\n",
      "\tAction [0.466 0.976 0.275 0.466 0.286 0.398 0.077 0.976 0.038 0.035] Reward 0.2\n",
      "\tAction [0.374 0.748 0.653 0.619 0.006 0.705 0.008 0.962 0.036 0.175] Reward 2.5\n",
      "\tAction [0.183 0.475 0.555 0.101 0.008 0.939 0.029 0.97  0.151 0.246] Reward 0.9\n",
      "\tAction [0.454 0.862 0.409 0.504 0.172 0.956 0.039 0.413 0.052 0.771] Reward 1.6\n",
      "\tAction [0.352 0.986 0.476 0.034 0.495 0.865 0.391 0.987 0.274 0.442] Reward 0.3\n",
      "\tAction [0.557 0.71  0.976 0.041 0.264 0.962 0.229 0.95  0.129 0.111] Reward 1.7\n",
      "\tAction [0.19  0.784 0.843 0.408 0.497 0.614 0.028 0.56  0.028 0.095] Reward 11.6\n",
      "\tAction [0.337 0.955 0.269 0.038 0.151 0.594 0.17  0.911 0.821 0.148] Reward -0.7\n",
      "\tAction [0.035 0.896 0.647 0.245 0.044 0.877 0.049 0.941 0.191 0.12 ] Reward -0.3\n",
      "\tAction [0.277 0.74  0.16  0.014 0.08  0.941 0.054 0.988 0.058 0.471] Reward 0.1\n",
      "\tAction [0.039 0.913 0.166 0.063 0.103 0.826 0.067 0.93  0.088 0.694] Reward 0.2\n",
      "Memory use: 2.766040802001953\n",
      "TOTAL REWARD: 50.0 (50 steps)\n",
      "\n",
      "Episode 3 Replay self.q_buffer 810\n",
      "3574445','4162152','\tAction [0.005 0.747 0.431 0.108 0.119 0.937 0.018 0.385 0.156 0.153] Reward -4.7\n",
      "\tAction [0.082 0.913 0.321 0.02  0.075 0.811 0.016 0.888 0.015 0.301] Reward -2.5\n",
      "\tAction [0.127 0.747 0.081 0.454 0.624 0.851 0.085 0.947 0.086 0.328] Reward 6.4\n",
      "\tAction [0.218 0.715 0.747 0.126 0.524 0.905 0.153 0.713 0.025 0.599] Reward 4.9\n",
      "\tAction [0.039 0.847 0.186 0.032 0.057 0.858 0.496 0.877 0.035 0.604] Reward -0.0\n",
      "\tAction [0.034 0.239 0.331 0.144 0.146 0.158 0.076 0.983 0.169 0.138] Reward 2.3\n",
      "\tAction [0.414 0.702 0.499 0.415 0.221 0.915 0.208 0.958 0.05  0.184] Reward 2.8\n",
      "\tAction [0.011 0.962 0.03  0.106 0.164 0.899 0.085 0.99  0.24  0.124] Reward -1.1\n",
      "\tAction [0.544 0.95  0.819 0.546 0.088 0.367 0.019 0.936 0.094 0.086] Reward -8.1\n",
      "\tAction [0.071 0.741 0.673 0.498 0.313 0.829 0.099 0.982 0.055 0.372] Reward 2.5\n",
      "\tAction [0.801 0.826 0.703 0.221 0.194 0.302 0.032 0.891 0.508 0.26 ] Reward -3.8\n",
      "\tAction [0.733 0.83  0.432 0.101 0.01  0.8   0.012 0.941 0.169 0.242] Reward 0.2\n",
      "\tAction [0.209 0.81  0.933 0.746 0.039 0.264 0.148 0.955 0.011 0.03 ] Reward -1.7\n",
      "\tAction [0.11  0.976 0.487 0.62  0.203 0.967 0.1   0.911 0.032 0.033] Reward -6.2\n",
      "\tAction [0.357 0.98  0.08  0.316 0.44  0.821 0.025 0.957 0.11  0.359] Reward 4.8\n",
      "\tAction [0.194 0.742 0.686 0.14  0.322 0.89  0.033 0.909 0.023 0.024] Reward 0.2\n",
      "\tAction [0.118 0.507 0.226 0.015 0.596 0.821 0.027 0.985 0.066 0.048] Reward 3.2\n",
      "\tAction [0.333 0.638 0.651 0.053 0.257 0.922 0.077 0.365 0.622 0.239] Reward 0.3\n",
      "\tAction [0.005 0.927 0.689 0.414 0.073 0.572 0.234 0.96  0.394 0.262] Reward -2.4\n",
      "\tAction [0.257 0.669 0.518 0.023 0.629 0.767 0.07  0.854 0.06  0.248] Reward 1.0\n",
      "\tAction [0.317 0.727 0.081 0.123 0.278 0.608 0.436 0.978 0.069 0.512] Reward 0.4\n",
      "\tAction [0.374 0.787 0.926 0.199 0.05  0.96  0.01  0.965 0.027 0.097] Reward -1.8\n",
      "\tAction [0.142 0.987 0.143 0.075 0.011 0.865 0.067 0.961 0.076 0.078] Reward -0.5\n",
      "\tAction [0.013 0.865 0.924 0.454 0.216 0.976 0.07  0.925 0.013 0.428] Reward 0.2\n",
      "\tAction [0.062 0.533 0.304 0.299 0.358 0.571 0.109 0.956 0.097 0.09 ] Reward 0.1\n",
      "\tAction [0.066 0.717 0.468 0.07  0.033 0.8   0.248 0.98  0.082 0.221] Reward -1.0\n",
      "\tAction [0.362 0.996 0.794 0.25  0.22  0.94  0.081 0.819 0.282 0.028] Reward -1.8\n",
      "\tAction [0.013 0.868 0.964 0.154 0.682 0.734 0.142 0.95  0.056 0.073] Reward -2.0\n",
      "\tAction [0.206 0.994 0.579 0.137 0.865 0.8   0.037 0.979 0.028 0.055] Reward 1.0\n",
      "\tAction [0.341 0.942 0.375 0.096 0.251 0.878 0.028 0.97  0.02  0.199] Reward -0.1\n",
      "\tAction [0.117 0.9   0.348 0.028 0.168 0.987 0.361 0.763 0.57  0.121] Reward 1.5\n",
      "\tAction [0.033 0.904 0.085 0.055 0.675 0.927 0.89  0.899 0.035 0.028] Reward 3.2\n",
      "\tAction [0.012 0.949 0.425 0.524 0.238 0.903 0.636 0.992 0.05  0.68 ] Reward 2.5\n",
      "\tAction [0.081 0.958 0.123 0.127 0.138 0.913 0.114 0.955 0.263 0.11 ] Reward -0.4\n",
      "\tAction [0.043 0.727 0.309 0.113 0.398 0.421 0.075 0.946 0.081 0.69 ] Reward 0.5\n",
      "\tAction [0.066 0.983 0.951 0.02  0.521 0.991 0.262 0.163 0.119 0.245] Reward 0.7\n",
      "\tAction [0.359 0.843 0.307 0.033 0.232 0.564 0.009 0.936 0.105 0.101] Reward 0.0\n",
      "\tAction [0.276 0.752 0.567 0.258 0.044 0.591 0.037 0.982 0.001 0.053] Reward 0.9\n",
      "\tAction [0.214 0.962 0.54  0.439 0.148 0.295 0.255 0.679 0.024 0.04 ] Reward -0.3\n",
      "\tAction [0.229 0.527 0.715 0.434 0.576 0.884 0.144 0.985 0.428 0.705] Reward 6.4\n",
      "\tAction [0.339 0.939 0.804 0.372 0.15  0.868 0.046 0.938 0.131 0.293] Reward 2.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAction [0.165 0.731 0.575 0.387 0.124 0.952 0.715 0.906 0.236 0.345] Reward 3.0\n",
      "\tAction [0.521 0.913 0.174 0.134 0.027 0.548 0.04  0.738 0.027 0.194] Reward 1.9\n",
      "\tAction [0.152 0.721 0.927 0.051 0.846 0.916 0.179 0.966 0.067 0.238] Reward 0.5\n",
      "\tAction [0.079 0.942 0.676 0.076 0.219 0.928 0.051 0.943 0.018 0.497] Reward 0.1\n",
      "\tAction [0.015 0.983 0.865 0.015 0.361 0.828 0.027 0.843 0.303 0.549] Reward 0.3\n",
      "\tAction [0.296 0.564 0.694 0.325 0.086 0.903 0.123 0.99  0.33  0.041] Reward 0.4\n",
      "\tAction [0.023 0.402 0.794 0.048 0.084 0.816 0.079 0.923 0.013 0.047] Reward 0.3\n",
      "\tAction [0.304 0.797 0.604 0.006 0.033 0.929 0.222 0.971 0.262 0.075] Reward -0.3\n",
      "\tAction [0.179 0.917 0.954 0.069 0.017 0.954 0.006 0.903 0.266 0.094] Reward -0.0\n",
      "Memory use: 2.7316322326660156\n",
      "TOTAL REWARD: 16.1 (50 steps)\n",
      "\n",
      "Episode 4 Replay self.q_buffer 860\n",
      "\tAction [0.737 0.761 0.523 0.045 0.53  0.607 0.038 0.991 0.167 0.543] Reward -10.7\n",
      "\tAction [0.17  0.766 0.7   0.097 0.053 0.902 0.05  0.973 0.08  0.232] Reward -2.7\n",
      "\tAction [0.155 0.868 0.627 0.057 0.122 0.975 0.365 0.864 0.136 0.091] Reward 0.5\n",
      "\tAction [0.122 0.844 0.819 0.269 0.064 0.81  0.052 0.777 0.148 0.159] Reward -0.0\n",
      "\tAction [0.434 0.908 0.275 0.007 0.238 0.74  0.006 0.911 0.326 0.775] Reward -1.5\n",
      "\tAction [0.07  0.746 0.408 0.032 0.083 0.354 0.012 0.998 0.044 0.058] Reward -0.5\n",
      "\tAction [0.118 0.904 0.358 0.141 0.041 0.979 0.398 0.912 0.037 0.193] Reward -4.9\n",
      "\tAction [0.111 0.885 0.755 0.246 0.024 0.753 0.154 0.963 0.579 0.611] Reward 0.2\n",
      "\tAction [0.032 0.455 0.905 0.085 0.105 0.6   0.054 0.964 0.311 0.034] Reward -0.0\n",
      "\tAction [0.127 0.858 0.444 0.662 0.063 0.978 0.019 0.883 0.01  0.341] Reward -2.7\n",
      "\tAction [0.016 0.982 0.794 0.1   0.355 0.941 0.119 0.765 0.112 0.226] Reward 0.1\n",
      "\tAction [0.245 0.962 0.249 0.034 0.136 0.269 0.814 0.871 0.018 0.096] Reward -1.3\n",
      "\tAction [0.132 0.6   0.379 0.106 0.051 0.839 0.062 0.984 0.025 0.201] Reward -1.8\n",
      "\tAction [0.513 0.822 0.848 0.163 0.058 0.932 0.055 0.898 0.057 0.099] Reward -5.2\n",
      "\tAction [0.064 0.898 0.232 0.239 0.403 0.546 0.073 0.372 0.072 0.656] Reward 1.0\n",
      "\tAction [0.599 0.888 0.496 0.095 0.176 0.946 0.106 0.979 0.037 0.244] Reward -1.9\n",
      "\tAction [0.298 0.733 0.779 0.146 0.043 0.828 0.011 0.905 0.319 0.302] Reward -0.9\n",
      "\tAction [0.681 0.706 0.781 0.656 0.163 0.9   0.072 0.948 0.111 0.03 ] Reward -14.0\n",
      "\tAction [0.042 0.981 0.572 0.323 0.175 0.966 0.198 0.969 0.092 0.032] Reward -7.2\n",
      "\tAction [0.063 0.817 0.898 0.107 0.116 0.635 0.022 0.731 0.361 0.162] Reward 0.1\n",
      "\tAction [0.056 0.706 0.454 0.073 0.081 0.961 0.045 0.989 0.044 0.085] Reward -0.2\n",
      "\tAction [0.472 0.775 0.927 0.078 0.137 0.947 0.164 0.967 0.062 0.132] Reward -1.7\n",
      "\tAction [0.131 0.979 0.43  0.449 0.122 0.525 0.007 0.97  0.12  0.037] Reward -6.1\n",
      "\tAction [0.004 0.934 0.313 0.447 0.465 0.135 0.216 0.475 0.139 0.279] Reward 0.9\n",
      "\tAction [0.187 0.97  0.811 0.273 0.437 0.947 0.053 0.994 0.12  0.602] Reward 1.1\n",
      "\tAction [0.083 0.903 0.53  0.065 0.106 0.974 0.165 0.808 0.025 0.029] Reward -2.0\n",
      "\tAction [0.156 0.94  0.988 0.346 0.302 0.934 0.078 0.804 0.045 0.052] Reward 1.1\n",
      "\tAction [0.596 0.807 0.75  0.014 0.231 0.946 0.067 0.957 0.119 0.02 ] Reward -3.0\n",
      "\tAction [0.247 0.781 0.904 0.053 0.135 0.87  0.034 0.988 0.01  0.459] Reward -0.8\n",
      "\tAction [0.606 0.853 0.93  0.016 0.693 0.617 0.019 0.97  0.171 0.052] Reward -2.3\n",
      "\tAction [0.614 0.764 0.525 0.15  0.25  0.831 0.175 0.893 0.188 0.035] Reward -7.0\n",
      "\tAction [0.054 0.751 0.615 0.115 0.275 0.921 0.361 0.981 0.084 0.106] Reward 0.7\n",
      "\tAction [0.087 0.866 0.774 0.124 0.024 0.952 0.084 0.849 0.034 0.029] Reward 0.0\n",
      "\tAction [0.087 0.753 0.912 0.04  0.029 0.907 0.071 0.898 0.046 0.048] Reward -0.7\n",
      "\tAction [0.066 0.838 0.774 0.076 0.418 0.967 0.017 0.972 0.049 0.658] Reward 0.6\n",
      "\tAction [0.061 0.778 0.901 0.018 0.068 0.953 0.163 0.977 0.027 0.021] Reward -0.2\n",
      "\tAction [0.021 0.846 0.127 0.097 0.06  0.861 0.416 0.896 0.105 0.101] Reward 0.1\n",
      "\tAction [0.456 0.926 0.613 0.189 0.087 0.878 0.198 0.99  0.135 0.112] Reward 0.5\n",
      "\tAction [0.078 0.915 0.969 0.046 0.032 0.911 0.335 0.907 0.08  0.093] Reward -0.8\n",
      "\tAction [0.272 0.862 0.372 0.016 0.159 0.782 0.23  0.502 0.179 0.193] Reward 0.2\n",
      "\tAction [0.073 0.783 0.887 0.02  0.035 0.992 0.072 0.977 0.017 0.043] Reward -0.2\n",
      "\tAction [0.153 0.975 0.875 0.038 0.303 0.925 0.079 0.694 0.073 0.053] Reward 0.7\n",
      "\tAction [0.025 0.792 0.916 0.093 0.193 0.944 0.048 0.947 0.174 0.182] Reward -0.2\n",
      "\tAction [0.033 0.879 0.939 0.033 0.084 0.759 0.52  0.793 0.091 0.086] Reward -0.5\n",
      "\tAction [0.16  0.984 0.76  0.243 0.337 0.808 0.02  0.602 0.147 0.195] Reward 1.3\n",
      "\tAction [0.431 0.823 0.789 0.456 0.11  0.953 0.112 0.794 0.028 0.249] Reward -6.7\n",
      "\tAction [0.274 0.737 0.698 0.05  0.237 0.967 0.071 0.905 0.036 0.252] Reward -1.1\n",
      "\tAction [0.435 0.944 0.834 0.239 0.268 0.86  0.086 0.976 0.035 0.038] Reward 0.5\n",
      "\tAction [0.119 0.885 0.457 0.045 0.105 0.888 0.104 0.873 0.011 0.116] Reward -1.0\n",
      "\tAction [0.512 0.904 0.986 0.121 0.143 0.782 0.115 0.967 0.17  0.323] Reward 0.2\n",
      "Memory use: 2.80267333984375\n",
      "TOTAL REWARD: -79.8 (50 steps)\n",
      "\n",
      "Episode 5 Replay self.q_buffer 910\n",
      "\tAction [0.202 0.983 0.879 0.035 0.271 0.897 0.047 0.903 0.016 0.109] Reward -0.3\n",
      "\tAction [0.092 0.856 0.732 0.068 0.115 0.965 0.074 0.936 0.078 0.076] Reward -0.4\n",
      "\tAction [0.009 0.966 0.922 0.483 0.289 0.949 0.193 0.803 0.264 0.231] Reward 0.3\n",
      "\tAction [0.142 0.775 0.973 0.012 0.055 0.86  0.208 0.669 0.143 0.062] Reward -0.5\n",
      "\tAction [0.227 0.8   0.956 0.291 0.673 0.774 0.329 0.993 0.125 0.262] Reward 1.0\n",
      "\tAction [0.225 0.752 0.761 0.105 0.199 0.715 0.182 0.947 0.02  0.082] Reward 0.8\n",
      "\tAction [0.201 0.992 0.605 0.503 0.039 0.734 0.037 0.966 0.058 0.063] Reward -0.2\n",
      "\tAction [0.061 0.813 0.609 0.532 0.287 0.822 0.599 0.898 0.029 0.514] Reward -0.9\n",
      "\tAction [0.118 0.862 0.888 0.142 0.298 0.899 0.056 0.479 0.059 0.551] Reward -0.8\n",
      "\tAction [0.063 0.985 0.979 0.073 0.494 0.799 0.084 0.829 0.013 0.229] Reward -0.2\n",
      "\tAction [0.088 0.65  0.843 0.08  0.123 0.767 0.072 0.859 0.016 0.052] Reward -1.0\n",
      "\tAction [0.07  0.643 0.596 0.034 0.068 0.913 0.111 0.989 0.013 0.098] Reward -1.0\n",
      "\tAction [0.784 0.488 0.924 0.196 0.133 0.938 0.028 0.724 0.568 0.073] Reward -2.9\n",
      "\tAction [0.467 0.891 0.977 0.011 0.247 0.978 0.005 0.966 0.118 0.193] Reward -2.2\n",
      "\tAction [0.161 0.991 0.899 0.331 0.247 0.903 0.029 0.959 0.035 0.184] Reward -0.0\n",
      "\tAction [0.014 0.971 0.85  0.286 0.047 0.572 0.146 0.841 0.002 0.206] Reward -0.6\n",
      "\tAction [0.052 0.954 0.675 0.031 0.584 0.888 0.032 0.68  0.012 0.081] Reward 0.2\n",
      "\tAction [0.205 0.743 0.809 0.134 0.052 0.552 0.024 0.997 0.427 0.251] Reward 0.9\n",
      "\tAction [0.397 0.941 0.456 0.218 0.183 0.783 0.005 0.971 0.125 0.207] Reward 0.1\n",
      "\tAction [0.28  0.158 0.934 0.171 0.398 0.771 0.1   0.911 0.462 0.387] Reward 0.6\n",
      "\tAction [0.359 0.945 0.611 0.012 0.204 0.792 0.413 0.975 0.037 0.158] Reward -0.4\n",
      "\tAction [0.378 0.889 0.874 0.064 0.162 0.75  0.075 0.556 0.064 0.022] Reward -0.9\n",
      "\tAction [0.185 0.725 0.94  0.343 0.246 0.731 0.13  0.528 0.008 0.33 ] Reward 5.0\n",
      "\tAction [0.048 0.949 0.719 0.117 0.018 0.974 0.164 0.767 0.038 0.087] Reward -1.1\n",
      "\tAction [0.085 0.782 0.915 0.373 0.124 0.696 0.086 0.995 0.061 0.096] Reward -0.1\n",
      "\tAction [0.171 0.619 0.795 0.029 0.252 0.839 0.003 0.895 0.108 0.069] Reward 6.7\n",
      "\tAction [0.28  0.869 0.618 0.072 0.07  0.872 0.051 0.962 0.105 0.188] Reward -0.5\n",
      "\tAction [0.151 0.916 0.949 0.052 0.211 0.786 0.02  0.789 0.201 0.07 ] Reward 0.1\n",
      "\tAction [0.077 0.962 0.727 0.094 0.128 0.566 0.317 0.988 0.205 0.199] Reward -0.7\n",
      "\tAction [0.022 0.443 0.87  0.165 0.26  0.55  0.06  0.949 0.29  0.11 ] Reward -0.6\n",
      "\tAction [0.684 0.861 0.821 0.054 0.113 0.902 0.08  0.901 0.079 0.336] Reward -2.1\n",
      "\tAction [0.647 0.682 0.816 0.084 0.151 0.513 0.061 0.957 0.025 0.327] Reward -1.0\n",
      "\tAction [0.049 0.776 0.964 0.215 0.424 0.884 0.002 0.961 0.303 0.04 ] Reward 0.4\n",
      "\tAction [0.177 0.606 0.713 0.344 0.151 0.221 0.197 0.95  0.019 0.272] Reward 1.8\n",
      "\tAction [0.05  0.848 0.768 0.452 0.115 0.899 0.296 0.962 0.289 0.095] Reward -1.0\n",
      "\tAction [0.178 0.653 0.955 0.369 0.122 0.558 0.107 0.64  0.006 0.022] Reward 2.9\n",
      "\tAction [0.597 0.629 0.647 0.037 0.099 0.896 0.081 0.973 0.073 0.01 ] Reward -3.6\n",
      "\tAction [0.022 0.865 0.258 0.317 0.131 0.887 0.073 0.833 0.225 0.342] Reward 0.2\n",
      "\tAction [0.197 0.932 0.956 0.026 0.208 0.693 0.049 0.962 0.103 0.028] Reward -0.1\n",
      "\tAction [0.037 0.969 0.847 0.133 0.03  0.577 0.026 0.702 0.131 0.041] Reward -0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAction [0.039 0.874 0.381 0.101 0.228 0.677 0.021 0.963 0.19  0.052] Reward -1.4\n",
      "\tAction [0.049 0.86  0.777 0.244 0.264 0.574 0.008 0.987 0.002 0.088] Reward -0.2\n",
      "\tAction [0.166 0.968 0.823 0.208 0.367 0.884 0.036 0.982 0.217 0.114] Reward -0.5\n",
      "\tAction [0.22  0.839 0.803 0.059 0.219 0.431 0.036 0.948 0.106 0.025] Reward -0.1\n",
      "\tAction [0.123 0.827 0.796 0.017 0.051 0.91  0.145 0.996 0.233 0.215] Reward -0.2\n",
      "\tAction [0.189 0.909 0.747 0.03  0.012 0.597 0.085 0.936 0.012 0.199] Reward -0.1\n",
      "\tAction [0.387 0.888 0.796 0.063 0.387 0.572 0.132 0.998 0.057 0.341] Reward -1.7\n",
      "\tAction [0.026 0.881 0.923 0.313 0.15  0.813 0.072 0.704 0.007 0.157] Reward 0.2\n",
      "\tAction [0.064 0.883 0.852 0.007 0.157 0.961 0.03  0.761 0.137 0.078] Reward -0.7\n",
      "\tAction [0.125 0.751 0.846 0.278 0.372 0.851 0.091 0.959 0.008 0.216] Reward -0.2\n",
      "Memory use: 2.8651580810546875\n",
      "TOTAL REWARD: -7.8 (50 steps)\n",
      "\n",
      "Episode 6 Replay self.q_buffer 960\n",
      "\tAction [0.066 0.798 0.911 0.577 0.188 0.931 0.032 0.799 0.017 0.054] Reward -1.6\n",
      "\tAction [0.055 0.864 0.89  0.864 0.407 0.745 0.027 0.976 0.172 0.158] Reward -6.7\n",
      "\tAction [0.042 0.643 0.814 0.094 0.019 0.896 0.022 0.908 0.018 0.253] Reward -1.9\n",
      "\tAction [0.226 0.796 0.709 0.035 0.119 0.764 0.01  0.989 0.034 0.025] Reward -1.5\n",
      "\tAction [0.029 0.619 0.968 0.174 0.022 0.995 0.076 0.889 0.121 0.144] Reward -1.2\n",
      "\tAction [0.55  0.908 0.711 0.056 0.348 0.839 0.084 0.955 0.099 0.155] Reward -5.3\n",
      "\tAction [0.289 0.573 0.949 0.606 0.286 0.938 0.388 0.968 0.098 0.056] Reward 15.1\n",
      "\tAction [0.039 0.946 0.943 0.376 0.054 0.945 0.042 0.543 0.575 0.034] Reward -0.4\n",
      "\tAction [0.154 0.945 0.824 0.049 0.251 0.939 0.035 0.759 0.094 0.079] Reward 0.0\n",
      "\tAction [0.066 0.816 0.956 0.728 0.009 0.975 0.008 0.95  0.083 0.285] Reward -0.9\n",
      "\tAction [0.138 0.762 0.896 0.06  0.126 0.859 0.079 0.889 0.017 0.112] Reward -0.0\n",
      "\tAction [0.216 0.812 0.944 0.162 0.378 0.557 0.004 0.845 0.193 0.081] Reward 0.1\n",
      "\tAction [0.091 0.953 0.949 0.455 0.045 0.789 0.026 0.813 0.003 0.458] Reward -0.1\n",
      "\tAction [0.056 0.786 0.986 0.475 0.46  0.866 0.346 0.907 0.419 0.08 ] Reward 1.0\n",
      "\tAction [0.303 0.842 0.899 0.141 0.021 0.205 0.125 0.96  0.054 0.032] Reward -0.7\n",
      "\tAction [0.306 0.963 0.966 0.027 0.162 0.865 0.061 0.943 0.328 0.315] Reward -0.3\n",
      "\tAction [0.033 0.815 0.675 0.137 0.324 0.773 0.105 0.94  0.011 0.035] Reward 0.2\n",
      "\tAction [0.707 0.658 0.333 0.378 0.151 0.961 0.18  0.995 0.128 0.337] Reward -0.9\n",
      "\tAction [0.059 0.688 0.107 0.089 0.093 0.536 0.035 0.981 0.023 0.126] Reward -1.8\n",
      "\tAction [0.138 0.56  0.998 0.416 0.055 0.875 0.658 0.905 0.04  0.624] Reward 2.0\n",
      "\tAction [0.242 0.667 0.919 0.13  0.322 0.8   0.439 0.875 0.034 0.147] Reward 1.6\n",
      "\tAction [0.394 0.837 0.603 0.009 0.253 0.568 0.024 0.734 0.029 0.356] Reward 0.3\n",
      "\tAction [0.107 0.76  0.491 0.299 0.064 0.846 0.044 0.995 0.148 0.127] Reward -0.4\n",
      "\tAction [0.367 0.503 0.97  0.268 0.335 0.841 0.042 0.557 0.077 0.109] Reward 8.3\n",
      "\tAction [0.476 0.713 0.988 0.445 0.079 0.918 0.024 0.954 0.191 0.178] Reward 0.3\n",
      "\tAction [0.254 0.96  0.883 0.564 0.104 0.632 0.244 0.96  0.046 0.074] Reward -0.0\n",
      "\tAction [0.114 0.851 0.634 0.268 0.002 0.989 0.035 0.963 0.232 0.27 ] Reward -0.3\n",
      "\tAction [0.054 0.908 0.824 0.157 0.045 0.843 0.087 0.718 0.054 0.064] Reward 0.1\n",
      "\tAction [0.055 0.303 0.985 0.029 0.491 0.667 0.074 0.939 0.052 0.559] Reward 0.1\n",
      "\tAction [0.084 0.995 0.878 0.06  0.226 0.923 0.055 0.99  0.125 0.545] Reward -0.7\n",
      "\tAction [0.198 0.941 0.81  0.085 0.034 0.867 0.119 0.881 0.132 0.049] Reward -0.2\n",
      "\tAction [0.698 0.94  0.78  0.01  0.119 0.493 0.042 0.668 0.059 0.242] Reward -0.9\n",
      "\tAction [0.011 0.815 0.577 0.395 0.442 0.905 0.007 0.973 0.017 0.075] Reward 1.1\n",
      "\tAction [0.053 0.885 0.784 0.127 0.521 0.736 0.085 0.969 0.11  0.118] Reward -0.0\n",
      "\tAction [0.297 0.951 0.739 0.029 0.115 0.976 0.192 0.948 0.24  0.053] Reward -0.2\n",
      "\tAction [0.397 0.827 0.515 0.03  0.045 0.973 0.065 0.831 0.052 0.49 ] Reward -2.3\n",
      "\tAction [0.059 0.949 0.968 0.117 0.055 0.951 0.01  0.993 0.739 0.017] Reward 0.2\n",
      "\tAction [0.119 0.97  0.518 0.145 0.204 0.875 0.182 0.8   0.309 0.556] Reward 0.1\n",
      "\tAction [0.109 0.628 0.966 0.016 0.004 0.857 0.105 0.717 0.009 0.203] Reward -0.4\n",
      "\tAction [0.139 0.944 0.841 0.354 0.352 0.972 0.023 0.866 0.476 0.036] Reward 0.2\n",
      "\tAction [0.071 0.945 0.887 0.204 0.485 0.889 0.094 0.932 0.056 0.009] Reward -0.1\n",
      "\tAction [0.01  0.925 0.952 0.208 0.079 0.446 0.025 0.958 0.016 0.188] Reward -0.3\n",
      "\tAction [0.172 0.73  0.85  0.044 0.118 0.99  0.016 0.933 0.066 0.513] Reward -0.2\n",
      "\tAction [0.276 0.926 0.704 0.029 0.022 0.71  0.102 0.913 0.569 0.046] Reward -0.5\n",
      "\tAction [0.523 0.976 0.618 0.05  0.53  0.885 0.102 0.912 0.036 0.046] Reward -1.4\n",
      "\tAction [0.231 0.744 0.573 0.189 0.711 0.186 0.039 0.971 0.108 0.594] Reward 3.6\n",
      "\tAction [0.719 0.874 0.944 0.072 0.211 0.985 0.05  0.854 0.081 0.295] Reward -3.0\n",
      "\tAction [0.037 0.959 0.986 0.107 0.299 0.84  0.04  0.602 0.053 0.08 ] Reward 0.2\n",
      "\tAction [0.036 0.824 0.833 0.25  0.13  0.525 0.013 0.994 0.045 0.224] Reward -0.1\n",
      "\tAction [0.063 0.966 0.973 0.446 0.048 0.938 0.136 0.968 0.02  0.286] Reward -0.2\n",
      "Memory use: 2.8994522094726562\n",
      "TOTAL REWARD: -0.0 (50 steps)\n",
      "\n",
      "Episode 7 Replay self.q_buffer 1010\n",
      "5050963','\tAction [0.31  0.819 0.406 0.378 0.061 0.952 0.001 0.985 0.077 0.21 ] Reward -1.2\n",
      "\tAction [0.411 0.532 0.601 0.17  0.005 0.822 0.028 0.968 0.058 0.166] Reward 5.9\n",
      "\tAction [0.114 0.878 0.977 0.126 0.53  0.997 0.269 0.91  0.221 0.059] Reward 0.3\n",
      "\tAction [0.15  0.566 0.803 0.107 0.798 0.951 0.004 0.434 0.019 0.789] Reward 22.3\n",
      "\tAction [0.056 0.9   0.905 0.093 0.038 0.796 0.034 0.954 0.222 0.152] Reward 0.4\n",
      "\tAction [0.32  0.751 0.548 0.118 0.332 0.763 0.007 0.742 0.058 0.055] Reward 10.8\n",
      "\tAction [0.322 0.909 0.977 0.245 0.053 0.73  0.09  0.93  0.062 0.102] Reward -1.6\n",
      "\tAction [0.371 0.953 0.942 0.053 0.036 0.685 0.13  0.901 0.48  0.241] Reward -2.5\n",
      "\tAction [0.051 0.994 0.859 0.044 0.032 0.66  0.225 0.508 0.083 0.511] Reward -0.2\n",
      "\tAction [0.061 0.897 0.942 0.408 0.103 0.73  0.211 0.992 0.049 0.037] Reward 0.3\n",
      "\tAction [0.219 0.661 0.719 0.472 0.161 0.806 0.017 0.902 0.011 0.082] Reward 14.6\n",
      "\tAction [0.787 0.968 0.782 0.132 0.02  0.96  0.076 0.966 0.008 0.096] Reward -2.8\n",
      "\tAction [0.05  0.977 0.801 0.067 0.126 0.929 0.632 0.945 0.047 0.101] Reward -1.5\n",
      "\tAction [0.035 0.99  0.956 0.06  0.031 0.995 0.049 0.982 0.276 0.254] Reward 0.8\n",
      "\tAction [0.437 0.694 0.571 0.338 0.041 0.832 0.575 0.739 0.051 0.62 ] Reward -0.6\n",
      "\tAction [0.076 0.981 0.994 0.635 0.159 0.635 0.061 0.851 0.389 0.466] Reward -0.1\n",
      "\tAction [0.731 0.934 0.568 0.038 0.039 0.959 0.008 0.919 0.003 0.157] Reward -2.6\n",
      "\tAction [0.047 0.985 0.595 0.747 0.019 0.933 0.017 0.918 0.004 0.273] Reward -3.6\n",
      "\tAction [0.028 0.332 0.94  0.028 0.097 0.99  0.101 0.962 0.032 0.178] Reward -2.6\n",
      "\tAction [0.441 0.876 0.821 0.429 0.034 0.508 0.148 0.864 0.045 0.024] Reward 0.0\n",
      "\tAction [0.381 0.558 0.894 0.504 0.045 0.979 0.036 0.936 0.043 0.559] Reward 13.5\n",
      "\tAction [0.179 0.949 0.79  0.078 0.447 0.953 0.366 0.886 0.028 0.289] Reward 0.7\n",
      "\tAction [0.13  0.65  0.956 0.091 0.136 0.986 0.305 0.944 0.036 0.2  ] Reward -0.7\n",
      "\tAction [0.159 0.98  0.531 0.016 0.15  0.794 0.054 0.838 0.074 0.261] Reward 0.1\n",
      "\tAction [0.221 0.931 0.912 0.55  0.13  0.829 0.106 0.931 0.042 0.069] Reward -2.8\n",
      "\tAction [0.063 0.94  0.847 0.151 0.351 0.968 0.096 0.864 0.001 0.061] Reward -0.1\n",
      "\tAction [0.44  0.892 0.573 0.072 0.237 0.921 0.106 0.913 0.144 0.085] Reward 0.2\n",
      "\tAction [0.819 0.968 0.963 0.364 0.338 0.838 0.077 0.783 0.227 0.056] Reward 1.7\n",
      "\tAction [0.295 0.978 0.735 0.164 0.048 0.87  0.355 0.848 0.136 0.496] Reward -1.4\n",
      "\tAction [0.062 0.918 0.708 0.195 0.222 0.778 0.095 0.933 0.155 0.023] Reward 0.5\n",
      "\tAction [0.092 0.613 0.718 0.033 0.066 0.724 0.098 0.993 0.017 0.619] Reward -0.7\n",
      "\tAction [0.356 0.906 0.928 0.048 0.303 0.845 0.049 0.94  0.012 0.347] Reward 0.1\n",
      "\tAction [0.1   0.966 0.836 0.232 0.418 0.949 0.331 0.939 0.01  0.121] Reward 0.2\n",
      "\tAction [0.07  0.685 0.834 0.807 0.123 0.9   0.016 0.967 0.025 0.341] Reward 2.0\n",
      "\tAction [0.03  0.911 0.64  0.006 0.009 0.748 0.029 0.973 0.125 0.147] Reward -0.1\n",
      "\tAction [0.239 0.952 0.829 0.049 0.044 0.858 0.061 0.948 0.273 0.582] Reward 0.0\n",
      "\tAction [0.832 0.632 0.932 0.024 0.059 0.882 0.049 0.793 0.077 0.112] Reward -1.1\n",
      "\tAction [0.277 0.927 0.995 0.28  0.144 0.764 0.004 0.978 0.07  0.091] Reward -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAction [0.15  0.985 0.906 0.227 0.062 0.86  0.006 0.973 0.216 0.031] Reward 0.6\n",
      "\tAction [0.295 0.785 0.93  0.086 0.057 0.93  0.025 0.927 0.146 0.089] Reward 1.0\n",
      "\tAction [0.174 0.985 0.896 0.217 0.293 0.92  0.167 0.939 0.037 0.039] Reward 0.3\n",
      "\tAction [0.07  0.847 0.688 0.045 0.034 0.789 0.236 0.967 0.159 0.24 ] Reward -0.2\n",
      "\tAction [0.027 0.982 0.941 0.402 0.528 0.523 0.015 0.625 0.225 0.055] Reward 0.2\n",
      "\tAction [0.12  0.827 0.883 0.064 0.056 0.914 0.11  0.965 0.16  0.044] Reward -0.6\n",
      "\tAction [0.083 0.496 0.474 0.072 0.139 0.461 0.044 0.862 0.011 0.039] Reward -1.6\n",
      "\tAction [0.422 0.864 0.483 0.044 0.134 0.904 0.04  0.993 0.149 0.102] Reward -2.2\n",
      "\tAction [0.157 0.529 0.948 0.21  0.024 0.904 0.118 0.936 0.259 0.059] Reward 1.9\n",
      "\tAction [0.085 0.951 0.7   0.074 0.178 0.476 0.018 0.98  0.035 0.085] Reward 0.6\n",
      "\tAction [0.353 0.77  0.974 0.238 0.073 0.903 0.084 0.526 0.105 0.172] Reward 1.2\n",
      "\tAction [0.046 0.939 0.973 0.157 0.053 0.716 0.079 0.877 0.032 0.057] Reward -0.4\n",
      "Memory use: 2.9818382263183594\n",
      "TOTAL REWARD: 48.0 (50 steps)\n",
      "\n",
      "Episode 8 Replay self.q_buffer 1060\n",
      "4365804','\tAction [0.045 0.95  0.817 0.259 0.408 0.513 0.048 0.691 0.016 0.108] Reward -0.1\n",
      "\tAction [0.024 0.697 0.836 0.113 0.089 0.905 0.329 0.914 0.021 0.07 ] Reward -1.9\n",
      "\tAction [0.072 0.494 0.799 0.06  0.023 0.781 0.057 0.933 0.02  0.27 ] Reward -0.1\n",
      "\tAction [0.252 0.87  0.933 0.149 0.662 0.857 0.251 0.821 0.056 0.111] Reward 1.9\n",
      "\tAction [0.053 0.784 0.941 0.032 0.844 0.413 0.071 0.993 0.049 0.127] Reward 0.1\n",
      "\tAction [0.045 0.651 0.889 0.015 0.298 0.759 0.024 0.967 0.218 0.122] Reward -2.6\n",
      "\tAction [0.02  0.831 0.942 0.054 0.045 0.799 0.092 0.956 0.035 0.342] Reward 0.0\n",
      "\tAction [0.174 0.933 0.929 0.092 0.216 0.932 0.009 0.818 0.094 0.156] Reward -2.6\n",
      "\tAction [0.243 0.98  0.982 0.041 0.095 0.761 0.04  0.864 0.185 0.469] Reward -1.2\n",
      "\tAction [0.046 0.919 0.937 0.152 0.024 0.959 0.041 0.666 0.128 0.343] Reward -1.3\n",
      "\tAction [0.008 0.084 0.765 0.114 0.299 0.973 0.01  0.923 0.105 0.46 ] Reward -0.2\n",
      "\tAction [0.029 0.932 0.796 0.101 0.024 0.86  0.034 0.878 0.177 0.184] Reward -1.2\n",
      "\tAction [0.236 0.916 0.801 0.621 0.319 0.978 0.053 0.957 0.083 0.34 ] Reward 0.1\n",
      "\tAction [0.054 0.965 0.735 0.099 0.022 0.693 0.031 0.81  0.025 0.473] Reward -0.9\n",
      "\tAction [0.342 0.375 0.46  0.54  0.122 0.896 0.117 0.764 0.076 0.061] Reward 6.6\n",
      "\tAction [0.054 0.682 0.587 0.239 0.146 0.946 0.184 0.748 0.039 0.571] Reward -3.5\n",
      "\tAction [0.32  0.707 0.866 0.048 0.059 0.722 0.107 0.984 0.099 0.088] Reward -0.1\n",
      "\tAction [0.334 0.749 0.93  0.432 0.568 0.943 0.111 0.977 0.213 0.02 ] Reward 3.3\n",
      "\tAction [0.418 0.661 0.536 0.365 0.169 0.974 0.11  0.942 0.172 0.393] Reward 0.0\n",
      "\tAction [0.088 0.899 0.719 0.197 0.015 0.945 0.031 0.771 0.029 0.152] Reward -2.1\n",
      "\tAction [0.454 0.938 0.647 0.032 0.195 0.799 0.145 0.864 0.393 0.449] Reward -0.9\n",
      "\tAction [0.336 0.295 0.823 0.27  0.097 0.919 0.514 0.946 0.03  0.062] Reward 4.2\n",
      "\tAction [0.081 0.9   0.884 0.232 0.099 0.897 0.054 0.968 0.046 0.12 ] Reward -1.6\n",
      "\tAction [0.32  0.94  0.695 0.252 0.275 0.913 0.012 0.835 0.057 0.17 ] Reward 0.2\n",
      "\tAction [0.007 0.949 0.453 0.12  0.086 0.863 0.021 0.976 0.035 0.039] Reward -2.0\n",
      "\tAction [0.106 0.85  0.731 0.329 0.256 0.68  0.073 0.976 0.287 0.079] Reward 1.7\n",
      "\tAction [0.011 0.793 0.924 0.087 0.046 0.603 0.006 0.878 0.125 0.344] Reward -0.2\n",
      "\tAction [0.226 0.958 0.78  0.242 0.043 0.962 0.007 0.79  0.328 0.089] Reward -1.6\n",
      "\tAction [0.145 0.941 0.244 0.051 0.145 0.913 0.243 0.97  0.124 0.505] Reward -1.9\n",
      "\tAction [0.134 0.899 0.894 0.19  0.017 0.577 0.212 0.673 0.32  0.309] Reward -1.0\n",
      "\tAction [0.217 0.506 0.918 0.202 0.229 0.801 0.07  0.76  0.02  0.28 ] Reward 3.2\n",
      "\tAction [0.195 0.788 0.819 0.047 0.043 0.766 0.341 0.562 0.364 0.045] Reward 0.5\n",
      "\tAction [0.187 0.871 0.734 0.051 0.341 0.955 0.04  0.76  0.006 0.233] Reward 0.5\n",
      "\tAction [0.185 0.498 0.62  0.082 0.006 0.994 0.073 0.995 0.022 0.336] Reward -0.3\n",
      "\tAction [0.221 0.904 0.561 0.316 0.311 0.446 0.049 0.968 0.047 0.718] Reward 0.9\n",
      "\tAction [0.311 0.52  0.981 0.327 0.162 0.98  0.087 0.973 0.031 0.138] Reward 4.6\n",
      "\tAction [0.094 0.973 0.479 0.075 0.638 0.417 0.017 0.855 0.009 0.112] Reward 0.7\n",
      "\tAction [0.677 0.715 0.901 0.024 0.031 0.715 0.069 0.98  0.166 0.129] Reward -0.2\n",
      "\tAction [0.136 0.966 0.989 0.675 0.79  0.867 0.081 0.984 0.228 0.265] Reward 1.6\n",
      "\tAction [0.134 0.727 0.596 0.027 0.09  0.929 0.01  0.995 0.022 0.354] Reward 0.2\n",
      "\tAction [0.254 0.844 0.715 0.265 0.049 0.979 0.019 0.933 0.044 0.435] Reward -0.1\n",
      "\tAction [0.041 0.562 0.67  0.582 0.397 0.872 0.423 0.982 0.028 0.271] Reward 5.5\n",
      "\tAction [0.234 0.497 0.585 0.068 0.256 0.37  0.197 0.844 0.029 0.208] Reward 0.7\n",
      "\tAction [0.064 0.711 0.905 0.132 0.041 0.416 0.023 0.928 0.025 0.01 ] Reward -0.1\n",
      "\tAction [0.043 0.725 0.78  0.398 0.047 0.689 0.074 0.815 0.    0.349] Reward -0.7\n",
      "\tAction [0.036 0.527 0.967 0.027 0.147 0.795 0.015 0.865 0.097 0.082] Reward -0.5\n",
      "\tAction [0.027 0.893 0.848 0.016 0.015 0.831 0.046 0.822 0.384 0.104] Reward -0.2\n",
      "\tAction [0.114 0.824 0.934 0.553 0.205 0.872 0.123 0.738 0.025 0.165] Reward 0.5\n",
      "\tAction [0.046 0.653 0.849 0.143 0.032 0.688 0.153 0.911 0.062 0.211] Reward -0.8\n",
      "\tAction [0.369 0.91  0.92  0.017 0.252 0.75  0.313 0.982 0.064 0.061] Reward -0.1\n",
      "Memory use: 3.024505615234375\n",
      "TOTAL REWARD: 7.3 (50 steps)\n",
      "\n",
      "Episode 9 Replay self.q_buffer 1110\n",
      "4348572','\tAction [0.066 0.567 0.973 0.326 0.271 0.81  0.046 0.92  0.059 0.36 ] Reward 3.7\n",
      "\tAction [0.321 0.941 0.786 0.107 0.076 0.955 0.009 0.903 0.017 0.029] Reward -1.0\n",
      "\tAction [0.237 0.775 0.822 0.079 0.056 0.732 0.339 0.949 0.043 0.627] Reward -0.1\n",
      "\tAction [0.41  0.924 0.984 0.082 0.14  0.865 0.036 0.805 0.033 0.186] Reward -1.4\n",
      "\tAction [0.086 0.818 0.867 0.212 0.183 0.98  0.007 0.982 0.209 0.023] Reward -0.1\n",
      "\tAction [0.396 0.87  0.819 0.659 0.264 0.514 0.006 0.912 0.066 0.104] Reward -0.3\n",
      "\tAction [0.034 0.862 0.971 0.324 0.089 0.78  0.018 0.992 0.026 0.177] Reward -0.5\n",
      "\tAction [0.071 0.165 0.881 0.041 0.167 0.96  0.017 0.981 0.196 0.416] Reward 0.6\n",
      "\tAction [0.026 0.837 0.894 0.179 0.311 0.847 0.064 0.835 0.723 0.028] Reward -0.1\n",
      "\tAction [0.176 0.668 0.909 0.263 0.187 0.843 0.136 0.99  0.135 0.261] Reward 0.4\n",
      "\tAction [0.059 0.88  0.593 0.093 0.22  0.951 0.095 0.916 0.133 0.056] Reward 0.0\n",
      "\tAction [0.028 0.843 0.936 0.135 0.14  0.852 0.127 0.937 0.159 0.251] Reward -0.3\n",
      "\tAction [0.066 0.701 0.897 0.449 0.171 0.913 0.075 0.811 0.048 0.01 ] Reward 0.1\n",
      "\tAction [0.049 0.764 0.778 0.209 0.165 0.9   0.032 0.97  0.055 0.013] Reward -0.3\n",
      "\tAction [0.248 0.865 0.982 0.034 0.054 0.985 0.039 0.952 0.089 0.395] Reward -0.9\n",
      "\tAction [0.234 0.755 0.959 0.219 0.073 0.944 0.18  0.758 0.051 0.096] Reward -0.0\n",
      "\tAction [0.097 0.27  0.915 0.026 0.025 0.968 0.038 0.741 0.005 0.167] Reward -0.3\n",
      "\tAction [0.275 0.924 0.956 0.658 0.345 0.585 0.037 0.992 0.003 0.192] Reward 1.0\n",
      "\tAction [0.234 0.924 0.566 0.129 0.075 0.834 0.141 0.692 0.25  0.3  ] Reward -0.4\n",
      "\tAction [0.303 0.8   0.809 0.31  0.007 0.873 0.168 0.926 0.026 0.035] Reward -0.2\n",
      "\tAction [0.177 0.921 0.98  0.098 0.117 0.928 0.656 0.986 0.406 0.404] Reward 0.1\n",
      "\tAction [0.104 0.644 0.944 0.139 0.131 0.352 0.02  0.918 0.036 0.019] Reward -0.3\n",
      "\tAction [0.023 0.78  0.923 0.019 0.088 0.909 0.158 0.941 0.032 0.04 ] Reward -0.2\n",
      "\tAction [0.035 0.82  0.439 0.282 0.084 0.79  0.03  0.948 0.109 0.159] Reward -0.8\n",
      "\tAction [0.254 0.774 0.741 0.464 0.066 0.727 0.068 0.973 0.02  0.024] Reward 0.4\n",
      "\tAction [0.062 0.556 0.976 0.597 0.357 0.84  0.022 0.963 0.008 0.012] Reward 14.3\n",
      "\tAction [0.205 0.924 0.157 0.08  0.053 0.944 0.195 0.991 0.025 0.359] Reward 0.3\n",
      "\tAction [0.028 0.953 0.852 0.365 0.036 0.899 0.068 0.95  0.789 0.035] Reward -0.2\n",
      "\tAction [0.144 0.982 0.874 0.272 0.187 0.952 0.416 0.837 0.022 0.131] Reward 0.0\n",
      "\tAction [0.5   0.97  0.979 0.57  0.158 0.799 0.058 0.765 0.158 0.027] Reward -0.3\n",
      "\tAction [0.11  0.467 0.97  0.095 0.043 0.94  0.507 0.879 0.218 0.175] Reward -0.3\n",
      "\tAction [0.028 0.982 0.81  0.091 0.208 0.381 0.093 0.651 0.021 0.258] Reward -0.1\n",
      "\tAction [0.052 0.195 0.905 0.142 0.312 0.372 0.013 0.932 0.023 0.172] Reward -0.2\n",
      "\tAction [0.124 0.908 0.514 0.121 0.02  0.981 0.127 0.758 0.019 0.05 ] Reward -0.0\n",
      "\tAction [0.061 0.783 0.73  0.071 0.616 0.956 0.003 0.982 0.017 0.021] Reward 0.1\n",
      "\tAction [0.429 0.926 0.467 0.203 0.449 0.582 0.043 0.966 0.067 0.272] Reward -0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAction [0.229 0.962 0.737 0.417 0.718 0.558 0.115 0.97  0.101 0.841] Reward 0.2\n",
      "\tAction [0.262 0.818 0.776 0.01  0.011 0.903 0.172 0.989 0.044 0.414] Reward 0.1\n",
      "\tAction [0.244 0.927 0.918 0.087 0.25  0.903 0.318 0.924 0.431 0.676] Reward -0.1\n",
      "\tAction [0.04  0.497 0.877 0.152 0.068 0.909 0.096 0.901 0.159 0.079] Reward -0.7\n",
      "\tAction [0.031 0.7   0.987 0.685 0.143 0.982 0.063 0.997 0.162 0.053] Reward -0.2\n",
      "\tAction [0.418 0.771 0.959 0.825 0.008 0.697 0.138 0.98  0.011 0.015] Reward -0.1\n",
      "\tAction [0.061 0.914 0.794 0.385 0.25  0.578 0.049 0.84  0.076 0.3  ] Reward -0.2\n",
      "\tAction [0.19  0.945 0.586 0.277 0.122 0.662 0.137 0.933 0.032 0.038] Reward 0.1\n",
      "\tAction [0.079 0.846 0.867 0.086 0.172 0.807 0.282 0.952 0.023 0.241] Reward -0.3\n",
      "\tAction [0.583 0.606 0.944 0.439 0.32  0.687 0.096 0.897 0.152 0.108] Reward -0.9\n",
      "\tAction [0.177 0.899 0.977 0.301 0.049 0.845 0.163 0.908 0.284 0.099] Reward -0.2\n",
      "\tAction [0.277 0.966 0.855 0.077 0.18  0.778 0.217 0.991 0.04  0.052] Reward -0.1\n",
      "\tAction [0.025 0.84  0.612 0.103 0.555 0.948 0.002 0.966 0.078 0.375] Reward 0.1\n",
      "\tAction [0.156 0.958 0.636 0.797 0.149 0.792 0.062 0.953 0.186 0.275] Reward 0.0\n",
      "Memory use: 3.104583740234375\n",
      "TOTAL REWARD: 10.3 (50 steps)\n",
      "\n",
      "Episode 10 Replay self.q_buffer 1160\n",
      "4750242','\tAction [0.067 0.968 0.794 0.303 0.066 0.993 0.042 0.941 0.296 0.028] Reward 0.4\n",
      "\tAction [0.116 0.766 0.344 0.043 0.015 0.91  0.049 0.932 0.101 0.16 ] Reward -1.4\n",
      "\tAction [0.148 0.901 0.928 0.107 0.34  0.929 0.05  0.831 0.012 0.076] Reward -0.4\n",
      "\tAction [0.026 0.764 0.954 0.391 0.342 0.918 0.034 0.805 0.008 0.272] Reward 2.0\n",
      "\tAction [0.151 0.938 0.676 0.113 0.095 0.701 0.153 0.971 0.058 0.471] Reward -0.5\n",
      "\tAction [0.117 0.859 0.88  0.692 0.007 0.946 0.011 0.948 0.308 0.101] Reward 0.6\n",
      "\tAction [0.038 0.496 0.474 0.15  0.009 0.38  0.577 0.979 0.051 0.052] Reward -0.4\n",
      "\tAction [0.126 0.935 0.669 0.042 0.067 0.695 0.027 0.882 0.204 0.235] Reward -0.1\n",
      "\tAction [0.42  0.957 0.921 0.406 0.14  0.973 0.047 0.881 0.243 0.077] Reward -5.7\n",
      "\tAction [0.44  0.784 0.694 0.027 0.033 0.242 0.466 0.836 0.398 0.158] Reward -0.9\n",
      "\tAction [0.034 0.942 0.747 0.47  0.184 0.977 0.012 0.752 0.042 0.362] Reward -1.1\n",
      "\tAction [0.205 0.847 0.806 0.056 0.09  0.788 0.051 0.607 0.074 0.137] Reward 0.1\n",
      "\tAction [0.071 0.991 0.936 0.063 0.154 0.799 0.172 0.987 0.122 0.308] Reward 0.1\n",
      "\tAction [0.203 0.941 0.95  0.027 0.15  0.862 0.018 0.902 0.033 0.069] Reward -0.3\n",
      "\tAction [0.48  0.773 0.989 0.699 0.426 0.962 0.21  0.854 0.012 0.049] Reward 2.1\n",
      "\tAction [0.583 0.838 0.928 0.38  0.017 0.925 0.149 0.985 0.085 0.384] Reward -3.6\n",
      "\tAction [0.16  0.962 0.525 0.138 0.134 0.948 0.419 0.883 0.178 0.028] Reward -0.7\n",
      "\tAction [0.186 0.7   0.901 0.06  0.643 0.934 0.207 0.674 0.01  0.016] Reward 1.1\n",
      "\tAction [0.072 0.904 0.979 0.262 0.132 0.851 0.048 0.964 0.045 0.07 ] Reward -0.0\n",
      "\tAction [0.226 0.955 0.838 0.597 0.067 0.562 0.028 0.966 0.16  0.881] Reward -0.2\n",
      "\tAction [0.426 0.964 0.505 0.012 0.014 0.926 0.181 0.983 0.059 0.146] Reward -1.6\n",
      "\tAction [0.586 0.686 0.914 0.008 0.109 0.805 0.04  0.262 0.006 0.274] Reward -1.2\n",
      "\tAction [0.204 0.98  0.663 0.021 0.514 0.764 0.105 0.976 0.078 0.152] Reward 0.3\n",
      "\tAction [0.066 0.671 0.687 0.385 0.143 0.827 0.016 0.937 0.336 0.548] Reward 3.0\n",
      "\tAction [0.05  0.964 0.971 0.069 0.284 0.848 0.12  0.949 0.268 0.035] Reward -0.1\n",
      "\tAction [0.454 0.884 0.838 0.136 0.14  0.96  0.389 0.953 0.093 0.061] Reward -3.0\n",
      "\tAction [0.066 0.809 0.581 0.015 0.112 0.8   0.045 0.977 0.023 0.148] Reward -0.4\n",
      "\tAction [0.387 0.682 0.636 0.096 0.011 0.519 0.037 0.89  0.022 0.236] Reward 0.6\n",
      "\tAction [0.277 0.822 0.498 0.309 0.108 0.991 0.082 0.874 0.233 0.179] Reward -3.5\n",
      "\tAction [0.213 0.952 0.902 0.033 0.142 0.716 0.093 0.99  0.167 0.055] Reward -0.2\n",
      "\tAction [0.077 0.913 0.738 0.149 0.7   0.821 0.011 0.976 0.03  0.141] Reward 0.5\n",
      "\tAction [0.043 0.874 0.521 0.187 0.637 0.86  0.055 0.958 0.037 0.238] Reward -1.6\n",
      "\tAction [0.389 0.984 0.991 0.154 0.166 0.992 0.018 0.999 0.068 0.038] Reward -0.5\n",
      "\tAction [0.22  0.444 0.681 0.069 0.028 0.955 0.012 0.968 0.053 0.23 ] Reward 2.0\n",
      "\tAction [0.015 0.98  0.979 0.087 0.085 0.115 0.115 0.947 0.193 0.17 ] Reward 0.1\n",
      "\tAction [0.134 0.862 0.662 0.388 0.024 0.932 0.05  0.97  0.059 0.08 ] Reward -1.1\n",
      "\tAction [0.647 0.932 0.902 0.051 0.214 0.636 0.031 0.681 0.535 0.801] Reward -1.0\n",
      "\tAction [0.184 0.882 0.819 0.323 0.106 0.813 0.074 0.861 0.072 0.032] Reward -0.8\n",
      "\tAction [0.196 0.907 0.816 0.292 0.371 0.903 0.102 0.965 0.025 0.034] Reward 0.8\n",
      "\tAction [0.597 0.662 0.96  0.201 0.091 0.971 0.064 0.938 0.082 0.057] Reward -1.0\n",
      "\tAction [0.367 0.95  0.878 0.124 0.094 0.97  0.026 0.898 0.007 0.138] Reward -0.4\n",
      "\tAction [0.102 0.873 0.575 0.038 0.415 0.743 0.191 0.989 0.045 0.157] Reward -0.7\n",
      "\tAction [0.176 0.715 0.849 0.207 0.05  0.354 0.109 0.995 0.084 0.23 ] Reward 0.6\n",
      "\tAction [0.122 0.981 0.802 0.057 0.246 0.916 0.023 0.991 0.107 0.538] Reward -0.3\n",
      "\tAction [0.048 0.992 0.972 0.527 0.06  0.948 0.005 0.937 0.148 0.026] Reward 0.0\n",
      "\tAction [0.633 0.854 0.828 0.041 0.48  0.677 0.078 0.99  0.01  0.319] Reward -2.3\n",
      "\tAction [0.033 0.848 0.587 0.097 0.522 0.717 0.189 0.968 0.081 0.108] Reward -0.2\n",
      "\tAction [0.094 0.836 0.423 0.277 0.376 0.984 0.058 0.745 0.022 0.223] Reward 0.2\n",
      "\tAction [0.171 0.796 0.965 0.42  0.083 0.907 0.068 0.954 0.013 0.425] Reward -0.2\n",
      "\tAction [0.064 0.939 0.854 0.075 0.29  0.99  0.007 0.972 0.03  0.147] Reward -0.1\n",
      "Memory use: 3.1707916259765625\n",
      "TOTAL REWARD: -21.2 (50 steps)\n",
      "\n",
      "Episode 11 Replay self.q_buffer 1210\n",
      "670064','4382013','5013931','\tAction [0.188 0.777 0.887 0.071 0.072 0.939 0.012 0.882 0.02  0.379] Reward -1.7\n",
      "\tAction [0.108 0.942 0.911 0.112 0.149 0.976 0.053 0.412 0.069 0.18 ] Reward -0.3\n",
      "\tAction [0.053 0.904 0.761 0.269 0.128 0.829 0.024 0.886 0.008 0.396] Reward 0.3\n",
      "\tAction [0.052 0.932 0.679 0.426 0.221 0.823 0.32  0.953 0.25  0.076] Reward -0.5\n",
      "\tAction [0.059 0.984 0.904 0.085 0.014 0.961 0.008 0.893 0.066 0.101] Reward -0.1\n",
      "\tAction [0.032 0.915 0.924 0.014 0.478 0.73  0.021 0.991 0.111 0.168] Reward -2.1\n",
      "\tAction [0.13  0.804 0.817 0.364 0.005 0.969 0.021 0.957 0.026 0.034] Reward 2.0\n",
      "\tAction [0.021 0.916 0.912 0.263 0.502 0.911 0.03  0.487 0.023 0.29 ] Reward 0.5\n",
      "\tAction [0.824 0.897 0.83  0.028 0.031 0.927 0.012 0.882 0.078 0.113] Reward -1.0\n",
      "\tAction [0.106 0.704 0.971 0.275 0.087 0.785 0.086 0.964 0.015 0.12 ] Reward 0.5\n",
      "\tAction [0.131 0.958 0.903 0.238 0.012 0.953 0.045 0.894 0.205 0.165] Reward -0.1\n",
      "\tAction [0.215 0.561 0.692 0.067 0.04  0.91  0.008 0.704 0.141 0.034] Reward 5.2\n",
      "\tAction [0.019 0.876 0.428 0.495 0.19  0.506 0.543 0.975 0.073 0.663] Reward -0.2\n",
      "\tAction [0.135 0.889 0.573 0.018 0.098 0.988 0.024 0.905 0.121 0.132] Reward -0.2\n",
      "\tAction [0.463 0.577 0.81  0.267 0.409 0.925 0.195 0.912 0.024 0.086] Reward 12.2\n",
      "\tAction [0.019 0.96  0.869 0.192 0.483 0.861 0.172 0.915 0.095 0.047] Reward 0.1\n",
      "\tAction [0.046 0.968 0.915 0.343 0.014 0.878 0.044 0.862 0.067 0.331] Reward -0.4\n",
      "\tAction [0.226 0.876 0.951 0.291 0.497 0.754 0.104 0.898 0.089 0.009] Reward 1.3\n",
      "\tAction [0.021 0.832 0.762 0.107 0.196 0.391 0.066 0.94  0.139 0.495] Reward -0.4\n",
      "\tAction [0.483 0.753 0.812 0.266 0.198 0.721 0.04  0.977 0.059 0.069] Reward 0.0\n",
      "\tAction [0.356 0.449 0.951 0.022 0.381 0.885 0.116 0.977 0.062 0.084] Reward 11.9\n",
      "\tAction [0.022 0.886 0.744 0.477 0.255 0.847 0.158 0.793 0.09  0.14 ] Reward 0.1\n",
      "\tAction [0.03  0.99  0.846 0.397 0.44  0.761 0.013 0.639 0.009 0.583] Reward -0.0\n",
      "\tAction [0.18  0.991 0.969 0.583 0.034 0.86  0.188 0.876 0.097 0.188] Reward -1.6\n",
      "\tAction [0.081 0.953 0.972 0.393 0.218 0.725 0.044 0.99  0.128 0.062] Reward -0.0\n",
      "\tAction [0.056 0.564 0.876 0.228 0.295 0.654 0.056 0.866 0.025 0.314] Reward -0.0\n",
      "\tAction [0.047 0.628 0.931 0.247 0.03  0.852 0.344 0.955 0.063 0.126] Reward 0.1\n",
      "\tAction [0.062 0.838 0.955 0.111 0.07  0.78  0.099 0.964 0.103 0.041] Reward -0.2\n",
      "\tAction [0.302 0.952 0.896 0.434 0.084 0.919 0.233 0.833 0.115 0.158] Reward -1.6\n",
      "\tAction [0.182 0.747 0.98  0.121 0.034 0.91  0.015 0.99  0.06  0.021] Reward 0.4\n",
      "\tAction [0.096 0.951 0.77  0.355 0.025 0.831 0.361 0.942 0.245 0.123] Reward -0.2\n",
      "\tAction [0.126 0.914 0.907 0.449 0.169 0.807 0.007 0.848 0.094 0.074] Reward -0.6\n",
      "\tAction [0.523 0.983 0.741 0.419 0.04  0.884 0.049 0.983 0.023 0.203] Reward -0.8\n",
      "\tAction [0.219 0.989 0.867 0.263 0.29  0.954 0.019 0.995 0.049 0.016] Reward -1.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAction [0.499 0.352 0.896 0.923 0.091 0.795 0.02  0.984 0.296 0.004] Reward 12.7\n",
      "\tAction [0.59  0.958 0.962 0.153 0.282 0.935 0.046 0.919 0.232 0.283] Reward -1.7\n",
      "\tAction [0.192 0.857 0.947 0.285 0.129 0.991 0.104 0.877 0.01  0.143] Reward -1.1\n",
      "\tAction [0.161 0.569 0.9   0.4   0.064 0.698 0.01  0.979 0.084 0.052] Reward 2.5\n",
      "\tAction [0.256 0.929 0.819 0.578 0.102 0.58  0.044 0.981 0.07  0.094] Reward -1.6\n",
      "\tAction [0.267 0.789 0.923 0.013 0.124 0.951 0.107 0.73  0.012 0.128] Reward -0.1\n",
      "\tAction [0.138 0.96  0.766 0.029 0.005 0.458 0.038 0.959 0.011 0.239] Reward 0.0\n",
      "\tAction [0.065 0.793 0.865 0.045 0.089 0.9   0.014 0.956 0.005 0.056] Reward -0.1\n",
      "\tAction [0.031 0.832 0.63  0.125 0.318 0.91  0.012 0.928 0.025 0.018] Reward -1.0\n",
      "\tAction [0.037 0.953 0.993 0.122 0.029 0.975 0.372 0.972 0.091 0.085] Reward 0.0\n",
      "\tAction [0.145 0.709 0.871 0.017 0.1   0.72  0.038 0.852 0.021 0.274] Reward -0.3\n",
      "\tAction [0.14  0.498 0.785 0.62  0.047 0.673 0.05  0.877 0.186 0.109] Reward 7.1\n",
      "\tAction [0.253 0.959 0.756 0.103 0.061 0.995 0.43  0.897 0.04  0.062] Reward -0.1\n",
      "\tAction [0.144 0.919 0.678 0.702 0.043 0.528 0.038 0.944 0.186 0.362] Reward -2.2\n",
      "\tAction [0.031 0.94  0.851 0.065 0.102 0.85  0.02  0.969 0.075 0.058] Reward -0.5\n",
      "\tAction [0.032 0.925 0.741 0.126 0.083 0.98  0.059 0.903 0.049 0.113] Reward 0.2\n",
      "Memory use: 3.218231201171875\n",
      "TOTAL REWARD: 35.0 (50 steps)\n",
      "\n",
      "Episode 12 Replay self.q_buffer 1260\n",
      "5399031','\tAction [0.144 0.625 0.946 0.268 0.055 0.946 0.027 0.737 0.173 0.075] Reward 5.0\n",
      "\tAction [0.07  0.83  0.399 0.069 0.372 0.735 0.094 0.963 0.189 0.016] Reward -0.6\n",
      "\tAction [0.045 0.428 0.495 0.024 0.185 0.971 0.05  0.881 0.092 0.122] Reward 0.9\n",
      "\tAction [0.115 0.472 0.264 0.29  0.08  0.852 0.084 0.959 0.029 0.212] Reward 6.8\n",
      "\tAction [0.451 0.815 0.892 0.089 0.532 0.911 0.056 0.941 0.065 0.015] Reward -0.2\n",
      "\tAction [0.031 0.978 0.352 0.169 0.113 0.978 0.346 0.782 0.034 0.044] Reward -0.4\n",
      "\tAction [0.204 0.925 0.94  0.031 0.241 0.795 0.264 0.949 0.181 0.294] Reward -1.0\n",
      "\tAction [0.147 0.741 0.797 0.132 0.817 0.487 0.01  0.847 0.188 0.173] Reward 3.6\n",
      "\tAction [0.129 0.932 0.788 0.109 0.242 0.966 0.023 0.975 0.035 0.244] Reward -0.2\n",
      "\tAction [0.096 0.942 0.911 0.833 0.006 0.981 0.059 0.99  0.021 0.061] Reward 0.0\n",
      "\tAction [0.483 0.973 0.456 0.07  0.54  0.475 0.008 0.901 0.3   0.05 ] Reward -0.5\n",
      "\tAction [0.049 0.78  0.696 0.113 0.25  0.664 0.091 0.481 0.122 0.177] Reward -1.2\n",
      "\tAction [0.229 0.567 0.962 0.035 0.164 0.839 0.118 0.95  0.147 0.048] Reward 2.5\n",
      "\tAction [0.075 0.924 0.847 0.648 0.014 0.738 0.026 0.908 0.203 0.111] Reward -0.2\n",
      "\tAction [0.12  0.421 0.715 0.135 0.034 0.902 0.324 0.975 0.008 0.034] Reward 1.1\n",
      "\tAction [0.285 0.945 0.875 0.783 0.369 0.745 0.215 0.984 0.225 0.346] Reward 0.1\n",
      "\tAction [0.323 0.846 0.849 0.158 0.104 0.657 0.083 0.969 0.121 0.034] Reward -0.8\n",
      "\tAction [0.316 0.88  0.877 0.003 0.127 0.524 0.392 0.976 0.016 0.053] Reward -0.2\n",
      "\tAction [0.02  0.984 0.491 0.044 0.591 0.919 0.088 0.927 0.096 0.199] Reward -0.4\n",
      "\tAction [0.026 0.598 0.95  0.156 0.029 0.884 0.166 0.867 0.047 0.092] Reward -0.3\n",
      "\tAction [0.123 0.988 0.884 0.333 0.167 0.933 0.006 0.981 0.038 0.121] Reward -0.3\n",
      "\tAction [0.354 0.558 0.94  0.071 0.005 0.934 0.007 0.785 0.025 0.045] Reward 3.2\n",
      "\tAction [0.14  0.766 0.99  0.023 0.037 0.407 0.023 0.941 0.047 0.136] Reward -0.3\n",
      "\tAction [0.009 0.733 0.748 0.08  0.059 0.683 0.293 0.815 0.051 0.438] Reward 0.2\n",
      "\tAction [0.188 0.845 0.978 0.012 0.422 0.775 0.043 0.947 0.025 0.357] Reward -0.1\n",
      "\tAction [0.073 0.773 0.9   0.302 0.119 0.957 0.061 0.854 0.015 0.115] Reward 1.1\n",
      "\tAction [0.141 0.817 0.58  0.084 0.162 0.849 0.004 0.96  0.348 0.099] Reward -0.7\n",
      "\tAction [0.32  0.952 0.605 0.175 0.335 0.974 0.031 0.922 0.276 0.163] Reward -0.8\n",
      "\tAction [0.012 0.605 0.923 0.175 0.045 0.962 0.096 0.954 0.012 0.506] Reward -0.3\n",
      "\tAction [0.023 0.977 0.946 0.246 0.148 0.794 0.015 0.61  0.095 0.052] Reward -0.3\n",
      "\tAction [0.111 0.775 0.685 0.291 0.068 0.456 0.066 0.961 0.086 0.099] Reward 0.3\n",
      "\tAction [0.015 0.937 0.906 0.027 0.522 0.867 0.016 0.787 0.478 0.139] Reward -0.3\n",
      "\tAction [0.081 0.666 0.857 0.054 0.37  0.954 0.135 0.809 0.158 0.31 ] Reward -0.7\n",
      "\tAction [0.06  0.202 0.905 0.072 0.178 0.928 0.098 0.974 0.094 0.097] Reward -0.1\n",
      "\tAction [0.062 0.93  0.825 0.244 0.206 0.788 0.15  0.86  0.005 0.094] Reward 0.1\n",
      "\tAction [0.525 0.982 0.254 0.112 0.284 0.975 0.02  0.95  0.028 0.173] Reward -0.4\n",
      "\tAction [0.035 0.995 0.471 0.136 0.066 0.591 0.126 0.836 0.105 0.446] Reward -0.0\n",
      "\tAction [0.014 0.702 0.944 0.265 0.1   0.926 0.035 0.974 0.046 0.338] Reward -0.0\n",
      "\tAction [0.267 0.779 0.949 0.047 0.178 0.351 0.05  0.808 0.1   0.062] Reward 0.1\n",
      "\tAction [0.03  0.816 0.857 0.215 0.042 0.886 0.033 0.982 0.129 0.588] Reward -0.4\n",
      "\tAction [0.028 0.711 0.928 0.467 0.238 0.981 0.177 0.966 0.072 0.147] Reward 4.3\n",
      "\tAction [0.064 0.942 0.879 0.547 0.168 0.971 0.045 0.98  0.227 0.44 ] Reward -0.3\n",
      "\tAction [0.549 0.435 0.928 0.007 0.271 0.958 0.13  0.988 0.144 0.012] Reward 2.6\n",
      "\tAction [0.062 0.857 0.801 0.152 0.04  0.792 0.064 0.897 0.087 0.056] Reward -0.3\n",
      "\tAction [0.158 0.846 0.804 0.016 0.317 0.829 0.039 0.912 0.012 0.46 ] Reward 0.3\n",
      "\tAction [0.071 0.782 0.866 0.285 0.075 0.977 0.051 0.954 0.036 0.025] Reward -0.4\n",
      "\tAction [0.023 0.961 0.963 0.03  0.01  0.929 0.362 0.881 0.027 0.017] Reward -0.2\n",
      "\tAction [0.252 0.87  0.994 0.33  0.009 0.956 0.058 0.878 0.063 0.156] Reward -0.4\n",
      "\tAction [0.062 0.737 0.874 0.052 0.076 0.971 0.075 0.963 0.115 0.217] Reward -0.1\n",
      "\tAction [0.222 0.704 0.802 0.124 0.164 0.98  0.241 0.931 0.143 0.351] Reward 2.2\n",
      "Memory use: 3.28924560546875\n",
      "TOTAL REWARD: 22.1 (50 steps)\n",
      "\n",
      "Episode 13 Replay self.q_buffer 1310\n",
      "\tAction [0.305 0.971 0.931 0.015 0.061 0.318 0.078 0.932 0.13  0.013] Reward -1.0\n",
      "\tAction [0.164 0.863 0.859 0.449 0.145 0.428 0.091 0.978 0.208 0.049] Reward -0.9\n",
      "\tAction [0.142 0.893 0.691 0.678 0.068 0.709 0.047 0.957 0.036 0.132] Reward -3.2\n",
      "\tAction [0.01  0.937 0.946 0.21  0.093 0.87  0.067 0.923 0.419 0.306] Reward 0.2\n",
      "\tAction [0.104 0.854 0.802 0.39  0.091 0.865 0.126 0.991 0.097 0.06 ] Reward -0.1\n",
      "\tAction [0.253 0.854 0.876 0.309 0.402 0.768 0.317 0.937 0.082 0.351] Reward 1.2\n",
      "\tAction [0.322 0.911 0.934 0.047 0.071 0.849 0.029 0.873 0.181 0.166] Reward -0.4\n",
      "\tAction [0.017 0.966 0.533 0.174 0.258 0.986 0.509 0.848 0.044 0.068] Reward 0.3\n",
      "\tAction [0.394 0.967 0.912 0.027 0.146 0.905 0.052 0.942 0.048 0.087] Reward -0.1\n",
      "\tAction [0.076 0.799 0.944 0.023 0.172 0.681 0.093 0.433 0.058 0.098] Reward -0.1\n",
      "\tAction [0.076 0.979 0.898 0.135 0.452 0.853 0.112 0.936 0.05  0.099] Reward 0.1\n",
      "\tAction [0.413 0.82  0.701 0.111 0.257 0.895 0.06  0.969 0.14  0.478] Reward -4.8\n",
      "\tAction [0.194 0.894 0.844 0.132 0.59  0.77  0.037 0.962 0.132 0.082] Reward 2.9\n",
      "\tAction [0.145 0.907 0.916 0.025 0.173 0.976 0.131 0.904 0.344 0.22 ] Reward -0.0\n",
      "\tAction [0.174 0.976 0.701 0.408 0.324 0.821 0.05  0.996 0.201 0.047] Reward 0.4\n",
      "\tAction [0.182 0.882 0.928 0.446 0.09  0.719 0.169 0.997 0.029 0.05 ] Reward -0.6\n",
      "\tAction [0.074 0.909 0.797 0.394 0.07  0.981 0.017 0.973 0.051 0.145] Reward 0.0\n",
      "\tAction [0.285 0.772 0.967 0.496 0.066 0.86  0.035 0.849 0.119 0.344] Reward -3.3\n",
      "\tAction [0.159 0.937 0.975 0.295 0.279 0.876 0.015 0.959 0.049 0.042] Reward -0.0\n",
      "\tAction [0.133 0.97  0.486 0.029 0.252 0.693 0.007 0.949 0.022 0.038] Reward 0.0\n",
      "\tAction [0.108 0.802 0.952 0.19  0.023 0.964 0.085 0.899 0.03  0.396] Reward -0.2\n",
      "\tAction [0.064 0.862 0.67  0.186 0.058 0.958 0.01  0.94  0.018 0.047] Reward -0.2\n",
      "\tAction [0.218 0.364 0.86  0.032 0.024 0.795 0.041 0.998 0.047 0.524] Reward 2.0\n",
      "\tAction [0.097 0.941 0.837 0.046 0.098 0.968 0.08  0.992 0.041 0.113] Reward 0.1\n",
      "\tAction [0.297 0.669 0.752 0.168 0.196 0.63  0.039 0.349 0.094 0.793] Reward 3.4\n",
      "\tAction [0.189 0.614 0.964 0.098 0.075 0.973 0.045 0.9   0.197 0.176] Reward 1.5\n",
      "\tAction [0.509 0.631 0.932 0.039 0.261 0.424 0.034 0.698 0.152 0.065] Reward 0.2\n",
      "\tAction [0.692 0.988 0.799 0.233 0.172 0.731 0.086 0.776 0.094 0.077] Reward -0.4\n",
      "\tAction [0.037 0.489 0.908 0.078 0.317 0.983 0.149 0.995 0.062 0.054] Reward 0.5\n",
      "\tAction [0.198 0.948 0.967 0.045 0.018 0.99  0.01  0.879 0.061 0.092] Reward -0.2\n",
      "\tAction [0.082 0.944 0.3   0.088 0.124 0.707 0.079 0.992 0.242 0.096] Reward 0.0\n",
      "\tAction [0.037 0.978 0.923 0.105 0.058 0.971 0.011 0.965 0.022 0.06 ] Reward -0.2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tAction [0.256 0.868 0.911 0.112 0.066 0.82  0.166 0.987 0.008 0.206] Reward -0.3\n",
      "\tAction [0.062 0.969 0.962 0.083 0.043 0.523 0.028 0.953 0.194 0.056] Reward 0.1\n",
      "\tAction [0.189 0.89  0.967 0.217 0.103 0.875 0.046 0.957 0.011 0.093] Reward 0.0\n",
      "\tAction [0.13  0.866 0.905 0.186 0.087 0.783 0.164 0.968 0.057 0.153] Reward -0.1\n",
      "\tAction [0.272 0.844 0.875 0.076 0.222 0.873 0.013 0.854 0.019 0.253] Reward -0.6\n",
      "\tAction [0.022 0.897 0.296 0.016 0.262 0.977 0.017 0.943 0.011 0.174] Reward 0.1\n",
      "\tAction [0.312 0.604 0.976 0.021 0.747 0.954 0.06  0.633 0.085 0.261] Reward 8.0\n",
      "\tAction [0.014 0.962 0.822 0.381 0.217 0.923 0.049 0.952 0.029 0.274] Reward -0.1\n",
      "\tAction [0.473 0.475 0.745 0.046 0.025 0.997 0.078 0.974 0.16  0.071] Reward 0.1\n",
      "\tAction [0.003 0.695 0.832 0.058 0.345 0.845 0.014 0.745 0.32  0.078] Reward 0.2\n",
      "\tAction [0.145 0.997 0.954 0.227 0.505 0.633 0.126 0.727 0.106 0.064] Reward 0.1\n",
      "\tAction [0.145 0.809 0.93  0.139 0.036 0.965 0.097 0.959 0.018 0.059] Reward 0.2\n",
      "\tAction [0.221 0.82  0.949 0.137 0.142 0.907 0.257 0.904 0.2   0.142] Reward -0.5\n",
      "\tAction [0.03  0.984 0.747 0.003 0.136 0.873 0.089 0.911 0.06  0.046] Reward 0.1\n",
      "\tAction [0.28  0.859 0.894 0.029 0.122 0.992 0.042 0.99  0.011 0.018] Reward -0.5\n",
      "\tAction [0.195 0.824 0.777 0.091 0.35  0.323 0.077 0.931 0.085 0.192] Reward 0.3\n",
      "\tAction [0.046 0.649 0.929 0.142 0.135 0.926 0.024 0.954 0.039 0.693] Reward 0.5\n",
      "\tAction [0.115 0.774 0.66  0.435 0.134 0.252 0.119 0.641 0.141 0.233] Reward 0.2\n",
      "Memory use: 3.3569412231445312\n",
      "TOTAL REWARD: 5.1 (50 steps)\n",
      "\n",
      "Episode 14 Replay self.q_buffer 1360\n",
      "\tAction [0.192 0.704 0.98  0.144 0.463 0.791 0.004 0.936 0.02  0.266] Reward 1.3\n",
      "\tAction [0.109 0.981 0.247 0.114 0.267 0.961 0.107 0.994 0.035 0.134] Reward -0.1\n",
      "\tAction [0.154 0.618 0.733 0.163 0.145 0.903 0.133 0.918 0.016 0.255] Reward 10.2\n",
      "\tAction [0.108 0.521 0.965 0.148 0.26  0.451 0.065 0.873 0.021 0.583] Reward -0.2\n",
      "\tAction [0.048 0.845 0.519 0.142 0.332 0.675 0.068 0.98  0.053 0.134] Reward 0.1\n",
      "\tAction [0.183 0.8   0.726 0.214 0.087 0.907 0.003 0.904 0.025 0.028] Reward -0.1\n",
      "\tAction [0.071 0.587 0.826 0.151 0.189 0.88  0.108 0.986 0.243 0.109] Reward 1.1\n",
      "\tAction [0.175 0.878 0.764 0.198 0.214 0.995 0.015 0.98  0.054 0.197] Reward -0.3\n",
      "\tAction [0.29  0.85  0.359 0.156 0.05  0.875 0.092 0.958 0.072 0.049] Reward -0.9\n",
      "\tAction [0.106 0.894 0.644 0.088 0.202 0.471 0.767 0.988 0.008 0.153] Reward -0.1\n",
      "\tAction [0.199 0.911 0.737 0.416 0.57  0.916 0.128 0.867 0.205 0.076] Reward -1.1\n",
      "\tAction [0.037 0.846 0.814 0.103 0.067 0.954 0.068 0.907 0.004 0.25 ] Reward 0.0\n",
      "\tAction [0.153 0.492 0.964 0.175 0.013 0.945 0.002 0.976 0.026 0.092] Reward 2.9\n",
      "\tAction [0.281 0.893 0.976 0.022 0.052 0.839 0.009 0.92  0.022 0.025] Reward -0.2\n",
      "\tAction [0.338 0.957 0.699 0.168 0.354 0.879 0.08  0.938 0.109 0.053] Reward -1.3\n",
      "\tAction [0.034 0.82  0.854 0.132 0.066 0.102 0.155 0.596 0.09  0.047] Reward -0.3\n",
      "\tAction [0.399 0.959 0.936 0.037 0.317 0.768 0.034 0.255 0.097 0.039] Reward -0.6\n",
      "\tAction [0.144 0.52  0.973 0.196 0.039 0.93  0.033 0.993 0.106 0.101] Reward 1.7\n",
      "\tAction [0.093 0.708 0.81  0.195 0.047 0.947 0.217 0.991 0.032 0.056] Reward 0.2\n",
      "\tAction [0.343 0.95  0.78  0.066 0.059 0.906 0.071 0.987 0.16  0.032] Reward -0.5\n",
      "\tAction [0.312 0.979 0.984 0.148 0.154 0.491 0.014 0.733 0.063 0.251] Reward -0.2\n",
      "\tAction [0.097 0.836 0.903 0.061 0.04  0.898 0.001 0.961 0.2   0.306] Reward -0.3\n",
      "\tAction [0.354 0.479 0.786 0.213 0.015 0.282 0.068 0.971 0.089 0.632] Reward 2.8\n",
      "\tAction [0.199 0.991 0.943 0.114 0.05  0.803 0.071 0.751 0.102 0.16 ] Reward 0.2\n",
      "\tAction [0.143 0.965 0.952 0.25  0.286 0.997 0.221 0.826 0.035 0.146] Reward -0.1\n",
      "\tAction [0.289 0.961 0.773 0.035 0.071 0.968 0.023 0.995 0.099 0.008] Reward -0.6\n",
      "\tAction [0.044 0.876 0.736 0.735 0.079 0.137 0.031 0.841 0.154 0.393] Reward -0.0\n",
      "\tAction [0.159 0.925 0.82  0.164 0.133 0.723 0.018 0.902 0.02  0.037] Reward -0.0\n",
      "\tAction [0.191 0.951 0.294 0.05  0.158 0.558 0.48  0.818 0.232 0.175] Reward -0.4\n",
      "\tAction [0.42  0.968 0.9   0.055 0.217 0.774 0.034 0.973 0.104 0.063] Reward -0.7\n",
      "\tAction [0.152 0.977 0.664 0.058 0.028 0.768 0.008 0.85  0.321 0.021] Reward -0.2\n",
      "\tAction [0.071 0.903 0.895 0.233 0.065 0.803 0.031 0.996 0.082 0.035] Reward -0.1\n",
      "\tAction [0.045 0.855 0.8   0.128 0.273 0.584 0.155 0.982 0.04  0.213] Reward -0.3\n",
      "\tAction [0.241 0.929 0.865 0.039 0.27  0.873 0.462 0.963 0.121 0.324] Reward 0.2\n",
      "\tAction [0.12  0.964 0.783 0.105 0.92  0.801 0.404 0.951 0.169 0.091] Reward 0.3\n",
      "\tAction [0.29  0.946 0.997 0.03  0.153 0.761 0.145 0.959 0.087 0.409] Reward 0.1\n",
      "\tAction [0.138 0.956 0.88  0.228 0.103 0.852 0.041 0.919 0.289 0.318] Reward -0.3\n",
      "\tAction [0.069 0.916 0.544 0.554 0.064 0.868 0.327 0.756 0.093 0.11 ] Reward 0.1\n",
      "\tAction [0.121 0.592 0.865 0.211 0.017 0.883 0.049 0.958 0.054 0.153] Reward 0.8\n",
      "\tAction [0.187 0.917 0.889 0.358 0.468 0.876 0.044 0.925 0.306 0.13 ] Reward -0.3\n",
      "\tAction [0.09  0.958 0.687 0.13  0.101 0.902 0.018 0.956 0.101 0.097] Reward -0.1\n",
      "\tAction [0.161 0.76  0.909 0.127 0.5   0.348 0.071 0.874 0.02  0.051] Reward 0.1\n",
      "\tAction [0.058 0.892 0.665 0.125 0.458 0.972 0.085 0.988 0.091 0.086] Reward 0.0\n",
      "\tAction [0.043 0.942 0.817 0.605 0.028 0.262 0.043 0.973 0.014 0.282] Reward -0.0\n",
      "\tAction [0.169 0.869 0.563 0.036 0.01  0.955 0.018 0.971 0.122 0.251] Reward -0.3\n",
      "\tAction [0.016 0.928 0.706 0.039 0.503 0.995 0.013 0.958 0.087 0.31 ] Reward -0.0\n",
      "\tAction [0.2   0.981 0.974 0.375 0.32  0.761 0.008 0.551 0.003 0.045] Reward -0.1\n",
      "\tAction [0.091 0.843 0.303 0.182 0.36  0.979 0.28  0.988 0.068 0.137] Reward -0.3\n",
      "\tAction [0.16  0.984 0.64  0.064 0.034 0.708 0.178 0.734 0.281 0.637] Reward -0.2\n",
      "\tAction [0.067 0.932 0.951 0.037 0.183 0.755 0.047 0.972 0.12  0.115] Reward 0.1\n",
      "Memory use: 3.3961830139160156\n",
      "TOTAL REWARD: 12.2 (50 steps)\n",
      "\n",
      "Episode 15 Replay self.q_buffer 1410\n",
      "\tAction [0.019 0.933 0.926 0.46  0.056 0.887 0.078 0.949 0.016 0.102] Reward -0.2\n",
      "\tAction [0.245 0.995 0.731 0.141 0.44  0.905 0.047 0.953 0.102 0.117] Reward -0.0\n",
      "\tAction [0.04  0.693 0.874 0.343 0.017 0.85  0.019 0.964 0.053 0.081] Reward 0.2\n",
      "\tAction [0.439 0.993 0.917 0.045 0.231 0.763 0.054 0.974 0.229 0.273] Reward -0.2\n"
     ]
    }
   ],
   "source": [
    "importlib.reload(qmain)\n",
    "Q = qmain.CRSNet(.9)\n",
    "Q.train(dqn_generator, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T16:24:15.920842Z",
     "start_time": "2018-05-29T16:23:41.342074Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.save_models()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T14:48:55.729060Z",
     "start_time": "2018-05-29T14:48:55.724043Z"
    }
   },
   "outputs": [],
   "source": [
    "shutil.rmtree(C.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T14:48:56.767254Z",
     "start_time": "2018-05-29T14:48:56.763264Z"
    }
   },
   "outputs": [],
   "source": [
    "os.makedirs(C.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T19:44:49.300614Z",
     "start_time": "2018-05-29T19:44:49.284657Z"
    }
   },
   "outputs": [],
   "source": [
    "cropI = np.random.normal(size=(1,*C.dims,3))\n",
    "\n",
    "crop_true_seg = np.random.normal(size=(1,*C.dims,3))\n",
    "true_cls = np.random.uniform(size=(1,3))\n",
    "\n",
    "y_true = np.random.normal(size=(1,*C.dims,3))\n",
    "y_pred = np.random.normal(size=(1,*C.dims,4))\n",
    "\n",
    "loss_layer = train_model.layers[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T19:44:57.204848Z",
     "start_time": "2018-05-29T19:44:54.411315Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.05015051\n",
      "Memory use: 2.0087203979492188\n",
      "-0.050445613\n",
      "Memory use: 2.0087203979492188\n",
      "-0.050510425\n",
      "Memory use: 2.008716583251953\n",
      "-0.049581293\n",
      "Memory use: 2.008716583251953\n",
      "-0.049884304\n",
      "Memory use: 2.008716583251953\n",
      "-0.048125546\n",
      "Memory use: 2.008716583251953\n",
      "-0.049017023\n",
      "Memory use: 2.008716583251953\n",
      "-0.04843785\n",
      "Memory use: 2.0087203979492188\n",
      "-0.050015423\n",
      "Memory use: 2.0087203979492188\n",
      "-0.049520925\n",
      "Memory use: 2.0087203979492188\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10):\n",
    "    print(loss_layer.get_loss(0, y_true, y_pred, C.loss_weights[1:]))\n",
    "    memory();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T18:18:34.280289Z",
     "start_time": "2018-05-29T18:18:18.388368Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(tn)\n",
    "for _ in range(10):\n",
    "    log_vars = [K.get_value(x)[0] for x in train_model.layers[-1].log_vars]\n",
    "    loss = np.sum(np.exp(-log_vars[0]) * K.get_value(tn.hetero_cls_loss(y_true, y_pred)) + log_vars[0], -1)\n",
    "    print(np.mean(loss))\n",
    "    memory();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T16:55:26.869123Z",
     "start_time": "2018-05-29T16:55:26.846162Z"
    }
   },
   "outputs": [],
   "source": [
    "A = prediction_model.predict(cropI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-29T17:00:15.134130Z",
     "start_time": "2018-05-29T17:00:14.176132Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for _ in range(10):\n",
    "    train_model.train_on_batch([cropI, crop_true_seg, true_cls], None)\n",
    "    A=prediction_model.predict(cropI);\n",
    "    print(A[1])\n",
    "    memory();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T18:11:58.749023Z",
     "start_time": "2018-05-28T18:11:47.096324Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(qmain)\n",
    "Q = qmain.CRSNet(1.)\n",
    "\n",
    "replay_conf = {'size': 10000,\n",
    "        'learn_start': 100,\n",
    "        'partition_num': 100,\n",
    "        'total_step': 10000,\n",
    "        'batch_size': 4}\n",
    "BATCH_SIZE = replay_conf[\"batch_size\"]\n",
    "\n",
    "Q.load_models(replay_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T18:12:16.001164Z",
     "start_time": "2018-05-28T18:12:15.992160Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.critic.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T18:12:17.440415Z",
     "start_time": "2018-05-28T18:12:17.431412Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.actor.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T18:12:19.496073Z",
     "start_time": "2018-05-28T18:12:19.480144Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Q.env.pred_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T14:54:39.886003Z",
     "start_time": "2018-05-28T14:54:26.248967Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(qmain)\n",
    "Q = qmain.CRSNet(.5)\n",
    "img, true_seg, true_cls = next(dqn_generator)\n",
    "seg, seg_var, cls = Q.run(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T14:54:41.512682Z",
     "start_time": "2018-05-28T14:54:41.378015Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis.draw_slices(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T14:56:11.067887Z",
     "start_time": "2018-05-28T14:56:10.964966Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis.draw_slices(true_seg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-28T14:55:59.697285Z",
     "start_time": "2018-05-28T14:55:59.556932Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vis.draw_slices(seg, normalize=[0,.05])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T19:51:04.504974Z",
     "start_time": "2018-05-26T19:51:01.162486Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = M.fit_generator(gen, steps_per_epoch=2, epochs=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prediction_model = get_prediction_model()\n",
    "trainable_model = get_trainable_model(prediction_model)\n",
    "trainable_model.compile(optimizer='adam', loss=None)\n",
    "assert len(trainable_model.layers[-1].trainable_weights) == 2  # two log_vars, one for each output\n",
    "assert len(trainable_model.losses) == 1\n",
    "hist = trainable_model.fit([X, Y1, Y2], nb_epoch=nb_epoch, batch_size=batch_size, verbose=0)\n",
    "\n",
    "[np.exp(K.get_value(log_var[0]))**0.5 for log_var in trainable_model.layers[-1].log_vars]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-26T01:08:37.572870Z",
     "start_time": "2018-05-26T01:08:37.552924Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config('etiology')\n",
    "T = config.Hyperparams()\n",
    "T.get_best_hyperparams()\n",
    "T.epochs = 30\n",
    "T.steps_per_epoch = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-05-21T19:36:55.197Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(cbuild)\n",
    "importlib.reload(dqna)\n",
    "#dqn_generator = cbuild._train_gen_dqn([])\n",
    "agent = dqna.train_dqn(dqn_generator, agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "agent.cls_model.fit(cls_generator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T03:15:14.525488Z",
     "start_time": "2018-05-21T03:15:14.521478Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "T.padding = ['same', 'same']\n",
    "T.pool_sizes = [(2,2,2),(2,2,2)]\n",
    "T.f = [64,64,64,64,64,64,64]\n",
    "T.skip_con = True\n",
    "T.epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-19T21:34:45.693522Z",
     "start_time": "2018-05-19T21:34:45.678482Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "drm.dcm2npy_batch(acc_nums=[\"E100113043\"])\n",
    "#vm.reset_accnum('E105464882')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-29T19:22:14.662640Z",
     "start_time": "2018-04-29T19:22:14.527783Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vm.plot_check(2, \"E102088195\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T04:11:27.139653Z",
     "start_time": "2018-05-02T04:11:26.670940Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vm.xref_dirs_with_excel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-02T13:53:09.930757Z",
     "start_time": "2018-05-02T04:11:28.356875Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crun.run_fixed_hyperparams([C], hyperparams=T)#C_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-22T15:18:23.881939Z",
     "start_time": "2018-05-22T15:18:20.054251Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(cbuild)\n",
    "model = cbuild.build_cnn_hyperparams(T)\n",
    "model.summary(line_length=120)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-21T21:56:12.245374Z",
     "start_time": "2018-05-21T21:33:39.982268Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(cbuild)\n",
    "#Z_reader = ['E103312835_1','12823036_0','12569915_0','E102093118_0','E102782525_0','12799652_0','E100894274_0','12874178_3','E100314676_0','12842070_0','13092836_2','12239783_0','12783467_0','13092966_0','E100962970_0','E100183257_1','E102634440_0','E106182827_0','12582632_0','E100121654_0','E100407633_0','E105310461_0','12788616_0','E101225606_0','12678910_1','E101083458_1','12324408_0','13031955_0','E101415263_0','E103192914_0','12888679_2','E106096969_0','E100192709_1','13112385_1','E100718398_0','12207268_0','E105244287_0','E102095465_0','E102613189_0','12961059_0','11907521_0','E105311123_0','12552705_0','E100610622_0','12975280_0','E105918926_0','E103020139_1','E101069048_1','E105427046_0','13028374_0','E100262351_0','12302576_0','12451831_0','E102929168_0','E100383453_0','E105344747_0','12569826_0','E100168661_0','12530153_0','E104697262_0']\n",
    "X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4)#, Z_test_fixed=Z_reader)\n",
    "Z_test, Z_train_orig = Z\n",
    "X_train_orig, Y_train_orig = train_orig\n",
    "hist = model.fit_generator(train_generator, steps_per_epoch=T.steps_per_epoch, epochs=T.epochs, validation_data=[X_test, Y_test])#, callbacks=[T.early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-12T15:42:30.363207Z",
     "start_time": "2018-05-12T15:42:29.920558Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save(join(C.model_dir, \"model_.hdf5\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T22:46:55.730496Z",
     "start_time": "2018-05-10T22:46:55.703429Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.predict(X_train_orig[20:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-10T22:44:57.324854Z",
     "start_time": "2018-05-10T22:44:57.319840Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_train_orig[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hist = model.fit_generator(train_generator, steps_per_epoch=T.steps_per_epoch, epochs=T.epochs, validation_data=[X_test, Y_test])#, callbacks=[T.early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plot_with_bbox(fn_list[2], cls_mapping[wrong_guesses[2]])\n",
    "Y_pred = model.predict(X_test)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_test])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "#save_output(Z_test, y_pred, y_true)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "#y_true_simp, y_pred_simp, _ = cnna.merge_classes(y_true, y_pred)\n",
    "#print(accuracy_score(y_true_simp, y_pred_simp))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
