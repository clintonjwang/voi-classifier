{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.models\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from keras import backend as K\n",
    "\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import config\n",
    "import csv\n",
    "import niftiutils.helper_fxns as hf\n",
    "import importlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "C = config.Config()\n",
    "T = config.Hyperparams()\n",
    "T.get_best_hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cbuild.run_fixed_hyperparams([C], hyperparams=T)#C_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C_list = [config.Config(), config.Config()]\n",
    "C_list[0].dims = [36,36,12]\n",
    "C_list[1].dims = [24,24,12]\n",
    "C_list[0].artif_dir = \"E:\\\\imgs\\\\artif_imgs_3612\\\\\"\n",
    "C_list[1].artif_dir = \"E:\\\\imgs\\\\artif_imgs_2412\\\\\"\n",
    "C_list[0].aug_dir = \"E:\\\\imgs\\\\aug_imgs_3612_cropint\\\\\"\n",
    "C_list[1].aug_dir = \"E:\\\\imgs\\\\aug_imgs_2412_cropint\\\\\"\n",
    "C_list[0].orig_dir = \"E:\\\\imgs\\\\orig_imgs_3612_cropint\\\\\"\n",
    "C_list[1].orig_dir = \"E:\\\\imgs\\\\orig_imgs_2412_cropint\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('\\n'.join(Z_test_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_test_full = ['E103312835_1','12823036_0','12569915_0','E102093118_0','E102782525_0','12799652_0','E100894274_0','12874178_3','E100314676_0','12842070_0','13092836_2','12239783_0','12783467_0','13092966_0','E100962970_0','E100183257_1','E102634440_0','E106182827_0','12582632_0','E100121654_0','E100407633_0','E105310461_0','12788616_0','E101225606_0','12678910_1','E101083458_1','12324408_0','13031955_0','E101415263_0','E103192914_0','12888679_2','E106096969_0','E100192709_1','13112385_1','E100718398_0','12207268_0','E105244287_0','E102095465_0','E102613189_0','12961059_0','11907521_0','E105311123_0','12552705_0','E100610622_0','12975280_0','E105918926_0','E103020139_1','E101069048_1','E105427046_0','13028374_0','E100262351_0','12302576_0','12451831_0','E102929168_0','E100383453_0','E105344747_0','12569826_0','E100168661_0','12530153_0','E104697262_0']\n",
    "Z_test_full = [z+\".npy\" for z in Z_test_full]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = cbuild.build_cnn_hyperparams(T)\n",
    "#model.summary(line_length=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4, n_art=0, Z_test=Z_test_full)\n",
    "Z_test, Z_train_orig = Z\n",
    "X_train_orig, Y_train_orig = train_orig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#from keras.utils import multi_gpu_model\n",
    "#model = multi_gpu_model(model, gpus=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "750/750 [==============================] - 111s 148ms/step - loss: 0.6111 - acc: 0.8169 - val_loss: 0.7647 - val_acc: 0.7500\n",
      "Epoch 2/30\n",
      "750/750 [==============================] - 82s 109ms/step - loss: 0.2904 - acc: 0.9212 - val_loss: 0.6988 - val_acc: 0.8000\n",
      "Epoch 3/30\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.1872 - acc: 0.9481 - val_loss: 0.6160 - val_acc: 0.8000\n",
      "Epoch 4/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.1371 - acc: 0.9607 - val_loss: 0.6716 - val_acc: 0.8000\n",
      "Epoch 5/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.1031 - acc: 0.9719 - val_loss: 0.5841 - val_acc: 0.8167\n",
      "Epoch 6/30\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.0858 - acc: 0.9743 - val_loss: 0.6505 - val_acc: 0.8500\n",
      "Epoch 7/30\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.0709 - acc: 0.9795 - val_loss: 0.4510 - val_acc: 0.8833\n",
      "Epoch 8/30\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.0559 - acc: 0.9837 - val_loss: 0.5422 - val_acc: 0.8500\n",
      "Epoch 9/30\n",
      "750/750 [==============================] - ETA: 0s - loss: 0.0531 - acc: 0.984 - 79s 106ms/step - loss: 0.0530 - acc: 0.9846 - val_loss: 0.6177 - val_acc: 0.8500\n",
      "Epoch 10/30\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.0435 - acc: 0.9869 - val_loss: 0.5698 - val_acc: 0.8833\n",
      "Epoch 11/30\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.0366 - acc: 0.9894 - val_loss: 0.7872 - val_acc: 0.7667\n",
      "Epoch 12/30\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.0334 - acc: 0.9902 - val_loss: 0.6796 - val_acc: 0.8667\n",
      "Epoch 13/30\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.0300 - acc: 0.9915 - val_loss: 0.8395 - val_acc: 0.8000\n",
      "Epoch 14/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0275 - acc: 0.9917 - val_loss: 0.5707 - val_acc: 0.8500\n",
      "Epoch 15/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0275 - acc: 0.9917 - val_loss: 0.5660 - val_acc: 0.8667\n",
      "Epoch 16/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0245 - acc: 0.9929 - val_loss: 0.6485 - val_acc: 0.8000\n",
      "Epoch 17/30\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.0228 - acc: 0.9938 - val_loss: 0.7847 - val_acc: 0.8167\n",
      "Epoch 18/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0216 - acc: 0.9930 - val_loss: 0.5751 - val_acc: 0.8500\n",
      "Epoch 19/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0160 - acc: 0.9956 - val_loss: 0.7377 - val_acc: 0.8500\n",
      "Epoch 20/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0219 - acc: 0.9933 - val_loss: 0.5478 - val_acc: 0.9000\n",
      "Epoch 21/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0169 - acc: 0.9950 - val_loss: 0.7808 - val_acc: 0.8833\n",
      "Epoch 22/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0140 - acc: 0.9961 - val_loss: 0.7217 - val_acc: 0.8167\n",
      "Epoch 23/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0149 - acc: 0.9956 - val_loss: 0.6270 - val_acc: 0.8333\n",
      "Epoch 24/30\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.0149 - acc: 0.9958 - val_loss: 0.5216 - val_acc: 0.8833\n",
      "Epoch 25/30\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.0130 - acc: 0.9963 - val_loss: 0.5484 - val_acc: 0.8833\n",
      "Epoch 26/30\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.0161 - acc: 0.9956 - val_loss: 0.7234 - val_acc: 0.8667\n",
      "Epoch 27/30\n",
      "750/750 [==============================] - 80s 106ms/step - loss: 0.0121 - acc: 0.9966 - val_loss: 0.5864 - val_acc: 0.9000\n",
      "Epoch 28/30\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.0128 - acc: 0.9961 - val_loss: 0.5669 - val_acc: 0.8333\n",
      "Epoch 29/30\n",
      "750/750 [==============================] - 79s 106ms/step - loss: 0.0098 - acc: 0.9972 - val_loss: 0.5838 - val_acc: 0.8500\n",
      "Epoch 30/30\n",
      "750/750 [==============================] - 79s 105ms/step - loss: 0.0104 - acc: 0.9973 - val_loss: 0.5861 - val_acc: 0.8833\n"
     ]
    }
   ],
   "source": [
    "hist = model.fit_generator(train_generator, steps_per_epoch=T.steps_per_epoch, epochs=T.epochs, validation_data=[X_test, Y_test])#, callbacks=[T.early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('E:\\\\models\\\\model_reader.hdf5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import dr_methods as drm\n",
    "voi_df_art, voi_df_ven, voi_df_eq = drm.get_voi_dfs()\n",
    "small_voi_df = pd.read_csv(C.small_voi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9\n",
      "0.916666666667\n"
     ]
    }
   ],
   "source": [
    "#plot_with_bbox(fn_list[2], cls_mapping[wrong_guesses[2]])\n",
    "Y_pred = model.predict(X_test)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_test])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "#save_output(Z_test, y_pred, y_true)\n",
    "\n",
    "print(accuracy_score(y_true, y_pred))\n",
    "y_true_simp, y_pred_simp, _ = cbuild.merge_classes(y_true, y_pred)\n",
    "print(accuracy_score(y_true_simp, y_pred_simp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "acc6cls = []\n",
    "acc3cls = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(19):\n",
    "    model_num = 306+i\n",
    "    X_test, Y_test, train_generator, num_samples, train_orig, Z = cbuild.get_cnn_data(n=4, n_art=0, Z_test=df[df['model_num'] == model_num][\"z_test\"])\n",
    "    X_train_orig, Y_train_orig = train_orig\n",
    "    model = keras.models.load_model(os.path.join(C.model_dir, \"models_%d.hdf5\" % model_num)) #models_305\n",
    "    \n",
    "    Y_pred = model.predict(X_train_orig)\n",
    "    y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_train_orig])\n",
    "    y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "    acc6cls.append(accuracy_score(y_true, y_pred))\n",
    "\n",
    "    y_true_simp, y_pred_simp, _ = cbuild.merge_classes(y_true, y_pred)\n",
    "\n",
    "    acc3cls.append(accuracy_score(y_true_simp, y_pred_simp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\t0\t0\t0\t0\t0\n",
      "0\t8\t3\t0\t0\t0\n",
      "2\t1\t5\t1\t0\t0\n",
      "0\t0\t2\t9\t0\t0\n",
      "0\t1\t0\t0\t10\t1\n",
      "1\t0\t0\t0\t0\t9\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(cm)):\n",
    "    print('\\t'.join(cm[:,i].astype(str)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"Z:\\\\Inter-reader study\\\\Answer key.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "y_true = df[\"Class\"].values\n",
    "y_pred = df[\"R1\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_true, y_pred, labels=C.classes_to_include)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_list = fn_list + list(Z_test[~np.equal(y_pred, y_true)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_list = fn_list + list(Z_train_orig[~np.equal(y_pred, y_true)])\n",
    "fn_list = fn_list + list(Z_train_orig[~np.equal(y_pred, y_true)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fn_x_list = [x+\"(\"+str(voi_df_art[voi_df_art[\"id\"] == x[:-4]][\"x1\"].values[0])+\")\" for x in fn_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unique, counts = np.unique(fn_x_list, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dict(zip(unique, counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Y_pred = model.predict(X_train_orig)\n",
    "y_true = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_train_orig])\n",
    "y_pred = np.array([max(enumerate(x), key=operator.itemgetter(1))[0] for x in Y_pred])\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "#cbuild.save_output(Z_train_orig, y_pred, y_true)#, save_dir=C.output_img_dir+\"\\\\training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305\n",
    "#cnna.visualize_layer(model, 'conv3d_148')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_filter_outputs.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_dense_outputs = cbuild.build_pretrain_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "filter_results = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "\n",
    "filter_avgs = np.concatenate([filter_results[cls] for cls in C.classes_to_include], axis=0)\n",
    "filter_avgs = np.mean(filter_avgs, axis=0)\n",
    "\n",
    "filter_cls_avg_unscaled = {cls: np.mean(filter_results[cls], axis=0) for cls in C.classes_to_include}\n",
    "filter_cls_avg_scaled = {cls: np.mean(filter_results[cls], axis=0) / filter_avgs for cls in C.classes_to_include}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters.csv', 'w', newline='') as csvfile:\n",
    "    header = ['filter_num'] + C.classes_to_include\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [filter_cls_avg_scaled[cls][f_num] for cls in C.classes_to_include])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(hf.flatten([list(feature_sheet[\"evidence1\"+cls].dropna().values) for cls in C.classes_to_include]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_sheet = pd.read_excel(C.xls_name, \"Descriptions\")\n",
    "\n",
    "features_by_cls = {}\n",
    "feat_count = {}\n",
    "for cls in C.classes_to_include:\n",
    "    features_by_cls[cls] = list(feature_sheet[\"evidence1\"+cls].dropna().values)\n",
    "    features_by_cls[cls] = features_by_cls[cls] + list(feature_sheet[\"evidence2\"+cls].dropna().values)\n",
    "#all_features = list(set([f for cls in features for f in features[cls]]))\n",
    "\n",
    "feat_count = dict(zip(*np.unique([f for cls in features_by_cls for f in features_by_cls[cls]], return_counts=True)))\n",
    "all_features = list(feat_count.keys())\n",
    "#for cls in C.classes_to_include:\n",
    "#    features_by_cls[cls] = list(set(features_by_cls[cls]))\n",
    "\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "continuous enhancing rim (5)\n",
      "progressive or concentric enhancement (5)\n",
      "regular spherical hypointense mass (5)\n"
     ]
    }
   ],
   "source": [
    "z = dict(zip(*np.unique(features_by_cls['colorectal'], return_counts=True)))\n",
    "for k in z:\n",
    "    print(k, \" (\",z[k],\")\", sep=\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'arterial enhancement': 10,\n",
       " 'central scar': 5,\n",
       " 'continuous enhancing rim': 10,\n",
       " 'delayed isointensity': 10,\n",
       " 'heterogeneous': 10,\n",
       " 'hyperintense mass on delayed phase': 10,\n",
       " 'hypointense without enhancement': 10,\n",
       " 'infiltrative': 10,\n",
       " 'lobulated margins': 10,\n",
       " 'nodular or discontinuous enhancement': 10,\n",
       " 'progressive centripetal filling': 10,\n",
       " 'progressive or concentric enhancement': 10,\n",
       " 'regular spherical hypointense mass': 10,\n",
       " 'thin well-defined walls': 10,\n",
       " 'venous washout': 10}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for col in [c for c in feature_sheet.columns if c.startswith(\"evidence\")]:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_features_by_cls = {cls: {} for cls in features_by_cls}\n",
    "Z_features = {}\n",
    "\n",
    "for cls in C.classes_to_include:\n",
    "    for f in features_by_cls[cls]:\n",
    "        if f not in Z_features:\n",
    "            Z_features[f] = []\n",
    "            \n",
    "        Z_features_by_cls[cls][f] = [x+\".npy\" for x in feature_sheet[feature_sheet[\"evidence1\"+cls] == f][cls].values]\n",
    "        Z_features[f] += [x+\".npy\" for x in feature_sheet[feature_sheet[\"evidence1\"+cls] == f][cls].values]\n",
    "        if feature_sheet[\"evidence2\"+cls].dropna().size > 0:\n",
    "            Z_features_by_cls[cls][f] = Z_features_by_cls[cls][f] + [x+\".npy\" for x in feature_sheet[feature_sheet[\"evidence2\"+cls] == f][cls].values]\n",
    "            Z_features[f] += [x+\".npy\" for x in feature_sheet[feature_sheet[\"evidence2\"+cls] == f][cls].values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_filters = {f:np.empty([0,100]) for f in all_features}#{cls: {} for cls in features}\n",
    "feature_filters_scaled = {}#{cls: {} for cls in features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs\n",
    "        \n",
    "    feature_filters[f] = np.mean(feature_filters[f], axis=0)# / filter_avgs\n",
    "\n",
    "    ff = feature_filters[f]\n",
    "    feature_filters[f] = np.where(ff > np.mean(ff), ff, 0)\n",
    "\n",
    "    #ff = feature_filters[cls][f]\n",
    "    #feature_filters[cls][f] = np.where(ff > 1*filter_cls_avg_unscaled[cls], ff, 0)\n",
    "\n",
    "    ff = feature_filters[f]\n",
    "    feature_filters[f] = np.where(ff > 1.3*filter_avgs, ff, 0)\n",
    "\n",
    "    feature_filters_scaled[f] = feature_filters[f] / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 678,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 679,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_features_cls = header[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_test_full = ['E106097391_0.npy', 'E104978772_1.npy', '12900535_0.npy', 'E100150242_0.npy', 'E105490014_0.npy', 'E103147618_0.npy', 'E103510187_0.npy', 'E104657225_0.npy', 'E100551966_0.npy', 'E101388602_0.npy', 'E100215900_8.npy', 'E100215900_7.npy', 'E104045692_0.npy', '13104521_0.npy', 'E100383453_0.npy', '12943286_0.npy', '12271995_0.npy', 'E102315724_0.npy', 'E104949189_0.npy', 'E100511083_1.npy', 'E101579471_0.npy', '13018986_1.npy', '13203550_8.npy', '13112385_0.npy', '12712463_0.npy', '12361082_0.npy', '13028374_0.npy', 'E103985934_1.npy', 'E100529980_0.npy', '12042703_3.npy', '12961059_0.npy', 'E105724706_2.npy', 'E100592424_2.npy', 'E103104254_0.npy', 'E104546069_0.npy', 'E101665217_1.npy', '12090000_0.npy', 'E100592424_1.npy', '12961059_1.npy', 'E105474285_0.npy', '12502068_1.npy', 'E100814791_0.npy', 'E102613189_0.npy', 'E105427046_0.npy', 'E102881031_1.npy', 'E102929168_0.npy', 'E102310482_0.npy', 'E102095465_0.npy', 'E101811299_0.npy', 'E104737273_0.npy', '12890053_0.npy', 'E100168661_1.npy', '12637865_0.npy', 'E100168661_2.npy', '12239783_0.npy', '12707781_0.npy', '12706568_1.npy', '12823036_0.npy', '12404081_0.npy', '12365693_1.npy']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_full))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_full))] for cls in C.classes_to_include}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filters_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)\n",
    "    #filters_test[cls] = np.apply_along_axis(lambda x: x / filter_avgs, 1, filters_test[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_evidence_strength(feature_filters, pred_filters):\n",
    "    \"\"\"A good pred_filter has high values for all the key (non-zero) features of feature_filter.\n",
    "    These values should be unscaled.\n",
    "    Returns average percentage of the mean value of the key filters (capped at 100%)\"\"\"\n",
    "    \n",
    "    strength = 0\n",
    "    num_key_filters = sum(feature_filters > 0)\n",
    "    \n",
    "    for i in range(len(pred_filters)):\n",
    "        t = feature_filters[i]\n",
    "        p = pred_filters[i]\n",
    "        \n",
    "        if t == 0:\n",
    "            continue\n",
    "            \n",
    "        strength += min(p/t, 1.1)#t*p / filter_avgs[i]**.7\n",
    "    return (strength / num_key_filters / 1.1)**.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['13112385_0.npy', '12361082_0.npy', 'E101579471_0.npy',\n",
       "       '12042703_3.npy', 'E103985934_1.npy', '13028374_0.npy',\n",
       "       'E100529980_0.npy', '12712463_0.npy', '13018986_1.npy',\n",
       "       '13203550_8.npy'],\n",
       "      dtype='<U16')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Z_test[\"cyst\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "#x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected features:\n",
      "- continuous enhancing rim/['hcc', 'colorectal'] - 77%\n",
      "- progressive or concentric enhancement/['cholangio', 'colorectal'] - 73%\n",
      "- regular spherical hypointense mass/['colorectal', 'cyst'] - 72%\n",
      "- venous washout/['hcc'] - 66%\n",
      "- hypointense without enhancement/['cyst'] - 61%\n"
     ]
    }
   ],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "max_strength = 0\n",
    "\n",
    "#cls = true_cls\n",
    "\"\"\"for f in features[cls]:\n",
    "    evidence[f] = get_evidence_strength(feature_filters[cls][f], filters_test[cls][img_num])\n",
    "    max_strength = max(max_strength, evidence[f])\n",
    "\"\"\"\n",
    "#for cls in C.classes_to_include:\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAACFCAYAAABsdIKzAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e5ecc3bef0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for true_cls in C.classes_to_include:\n",
    "    for img_num in range(len(filters_test[true_cls])):\n",
    "        z = Z_test[true_cls][img_num]\n",
    "        x = np.expand_dims(x_test[true_cls][img_num], axis=0)\n",
    "        evidence = {}\n",
    "        max_strength = 0\n",
    "        cls = true_cls\n",
    "        \n",
    "        output[z] = [true_cls]\n",
    "        \n",
    "        preds = model.predict(x, verbose=False)[0]\n",
    "        for pred_cls, pred_conf in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:2]:\n",
    "            output[z] = output[z] + [pred_cls, pred_conf]\n",
    "        \n",
    "        for f in all_features:\n",
    "            evidence[f + \"/\" + str(cls_features[f])] = get_evidence_strength(feature_filters[f], filters_test[true_cls][img_num])\n",
    "\n",
    "        for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True):\n",
    "            output[z] = output[z] + [f, int(strength*100)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred4.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_conf1', 'pred_cls2', 'pred_conf2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_full)):\n",
    "        writer.writerow([Z_test_full[z_num]] + [output[Z_test_full[z_num]][0] in output[Z_test_full[z_num]][5], \\\n",
    "                        output[Z_test_full[z_num]][0] in output[Z_test_full[z_num]][7]] + output[Z_test_full[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
