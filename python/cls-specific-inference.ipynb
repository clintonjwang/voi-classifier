{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.models\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from keras import backend as K\n",
    "\n",
    "import argparse\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import csv\n",
    "import niftiutils.helper_fxns as hf\n",
    "import importlib\n",
    "import inference_methods as im\n",
    "import itertools\n",
    "from math import sqrt, log, pi, exp\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange, vectorize, guvectorize\n",
    "from numpy import matmul, diag\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config()\n",
    "T = config.Hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_length(vector):\n",
    "    return sqrt(np.sum(vector**2))\n",
    "\n",
    "def squash_per_dim(vector, eps = 10**-10):\n",
    "    for i,x in enumerate(vector):\n",
    "        vector[i] *= x / (1 + x**2)\n",
    "    return vector\n",
    "\n",
    "def squash(vector, eps = 10**-10):\n",
    "    s_squared_norm = np.sum(vector**2)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / sqrt(s_squared_norm + eps)\n",
    "    return scale * vector\n",
    "\n",
    "def vec_distance(u, v):\n",
    "    return sqrt(np.sum((u - v)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305\n",
    "model_dense_outputs = cbuild.build_pretrain_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final_outputs = cbuild.build_pretrain_model(model, last_layer=\"pre-softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[-3].get_weights()[0]\n",
    "bias = model.layers[-3].get_weights()[1]\n",
    "\n",
    "#np.dot(filter_results[0], W) * eff_mult + eff_bias\n",
    "\n",
    "gamma, beta, mu, var = model.layers[-2].get_weights()\n",
    "\n",
    "eff_bias = (np.zeros(6) + bias - mu) / var**.5 * gamma + beta\n",
    "eff_mult = (np.ones(6) + bias - mu) / var**.5 * gamma + beta - eff_bias\n",
    "\n",
    "W_eff = W * eff_mult# + eff_bias\n",
    "\n",
    "#(np.dot(filter_results[0], W) + bias - mu) / var**.5 * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_vector(vector, feature_vectors):\n",
    "    best_feature = -1\n",
    "    for f_ix, feature_vec in enumerate(feature_vectors):\n",
    "        if np.dot(vector, feature_vec) < threshold:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_filter_results = (filter_results - filter_avgs) / filter_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.amin(filter_results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vectors = np.empty((num_features, num_units))\n",
    "feature_relevance = np.empty(num_features)\n",
    "for f_ix in range(num_features):\n",
    "    feature_vectors[f_ix, :] = np.mean(feature_filters[all_features[f_ix]], axis=0)\n",
    "    feature_evidence[f_ix] = np.dot(feature_vectors[f_ix], W_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squashed_filter_results = (filter_results / filter_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- feature_vectors * unit_relevance should be maximized\n",
    "\n",
    "- np.dot(feature_vectors[i], feature_vectors[j]) should be minimized, OR\n",
    "- vec_distance(feature_vectors[i], feature_vectors[j]) should be maximized for all pairs i,j\n",
    "\n",
    "- features \"turn on / off\" specific units; try to minimize the number of units impacted by a given feature\n",
    "\n",
    "- p(z|x) > .75 for all x manually annotated by z\n",
    "\n",
    "===\n",
    "- show % of evidence explained (fraction of sum of contributing units that are captured by features that turn those units on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_relevance = np.empty(num_units)\n",
    "for u_ix in range(num_units):\n",
    "    unit_relevance[u_ix] = np.amax(W_eff[u_ix]) - np.amin(W_eff[u_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_results = filter_results*unit_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_length = np.mean(np.apply_along_axis(get_length, 1, filter_results*unit_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239091267665856"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(squash(filter_results[2] * unit_relevance * 2/avg_length)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "filters_by_cls = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "filter_results = np.concatenate([filters_by_cls[cls] for cls in C.classes_to_include], axis=0)\n",
    "\n",
    "filter_avgs = np.mean(filter_results, axis=0)\n",
    "filter_stds = np.std(filter_results, axis=0)\n",
    "\n",
    "#filter_cls_avg_unscaled = {cls: np.mean(filter_results[cls], axis=0) for cls in C.classes_to_include}\n",
    "#filter_cls_avg_scaled = {cls: np.mean(filter_results[cls], axis=0) / filter_avgs for cls in C.classes_to_include}\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "feat_count.pop(\"central scar\")\n",
    "all_features = list(feat_count.keys())\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = dict(zip(*np.unique(features_by_cls['colorectal'], return_counts=True)))\n",
    "for k in z:\n",
    "    print(k, \" (\",z[k],\")\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "feat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_features = cnna.get_annotated_files(features_by_cls)\n",
    "Z_features.pop(\"central scar\")\n",
    "\n",
    "feature_filter_means = {}\n",
    "feature_filter_stds = {}\n",
    "feature_filters_scaled = {}#{cls: {} for cls in features}\n",
    "feature_filters = {f:np.empty([0,100]) for f in all_features}\n",
    "\n",
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs\n",
    "        \n",
    "    feature_filters[f] = (feature_filters[f] - filter_avgs) / filter_stds\n",
    "    \n",
    "    feature_filter_means[f] = np.mean(feature_filters[f], axis=0)# / filter_avgs\n",
    "    feature_filter_stds[f] = np.std(feature_filters[f], axis=0)# / filter_avgs\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > np.mean(ff), ff, 0)\n",
    "\n",
    "    #ff = feature_filters[cls][f]\n",
    "    #feature_filters[cls][f] = np.where(ff > 1*filter_cls_avg_unscaled[cls], ff, 0)\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > 1.3*filter_avgs, ff, 0)\n",
    "\n",
    "    feature_filters_scaled[f] = feature_filter_means[f] / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(all_features) # number of features\n",
    "num_units = 100 # number of dense units\n",
    "\n",
    "num_annotations = 8\n",
    "Z_test = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "indices_f = [orig_data_dict[cls][1] for cls in C.classes_to_include]\n",
    "indices_f = hf.flatten(indices_f)\n",
    "\n",
    "fixed_indices = np.empty([num_features, num_annotations])\n",
    "for f_ix,f in enumerate(all_features):\n",
    "    fixed_indices[f_ix, :] = np.where(np.isin(indices_f, random.sample(set(Z_features[f]), num_annotations)))[0]\n",
    "fixed_indices = fixed_indices.astype(int)\n",
    "\n",
    "test_indices = np.where(np.isin(indices_f, Z_test))[0]\n",
    "\n",
    "z_states = np.array([z for z in itertools.product([0,1], repeat=num_features) if sum(z) <= 4 and sum(z) >= 2])\n",
    "num_states = len(z_states)\n",
    "num_imgs = filter_results.shape[0]\n",
    "z_states_bool = [tuple([bool(x) for x in z]) for z in z_states]\n",
    "\n",
    "X = filter_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM_loop(k, alpha=.8, beta=.8, **kwargs):\n",
    "    if len(kwargs) == 0:\n",
    "        W = np.empty((num_features, num_units))\n",
    "        for f_ix in range(num_features):\n",
    "            W[f_ix] = feature_filter_means[all_features[f_ix]]*5\n",
    "        mu = np.ones(num_units)\n",
    "        sigma = 5\n",
    "        a = 5\n",
    "        b = 3\n",
    "        theta = scipy.random.normal(size=(num_features, num_features))\n",
    "    else:\n",
    "        mu = kwargs['mu']\n",
    "        sigma = kwargs['sigma']\n",
    "        W = kwargs['W']\n",
    "        a = kwargs['a']\n",
    "        b = kwargs['b']\n",
    "        theta = kwargs['theta']\n",
    "    \n",
    "    print(\"Initializing probabilities...\")\n",
    "    U = im.get_squashed_X(X, a, b)\n",
    "    p_z = im.get_p_z(z_states, theta)\n",
    "    s_states = im.get_s_states(z_states, W, p_z)\n",
    "    p_x_z = im.get_all_p_x_z(mu, sigma, s_states, U, fixed_indices, z_states)\n",
    "    p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "    \n",
    "    print(\"Running EM:\")\n",
    "    for jj in range(k):\n",
    "        print(\"   Iteration\", jj, end=\"...\")\n",
    "        theta = im.update_thetas(np.sum(p_z_x, axis=0), z_states_bool, theta, alpha)\n",
    "\n",
    "        p_z = im.get_p_z(z_states, theta)\n",
    "        s_states = im.get_s_states(z_states, W, p_z)\n",
    "        p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "\n",
    "        print(\"Updating W...\", end=\"\")\n",
    "        W = im.update_W(mu, W, z_states, U, p_z_x, alpha)\n",
    "        \n",
    "        print(\"Updating mu, sigma, a, b...\", end=\"\")\n",
    "        mu = im.update_mus(mu, s_states, U, p_z_x, beta)\n",
    "        sigma = im.update_sigma(mu, sigma, s_states, U, p_z_x)\n",
    "        a, b = im.update_ab(mu, a, b, sigma, s_states, X, p_z_x, alpha)\n",
    "\n",
    "        if jj < k-1:\n",
    "            print(\"Updating probabilities...\")\n",
    "            U = im.get_squashed_X(X, a, b)\n",
    "            p_x_z = im.get_all_p_x_z(mu, sigma, s_states, U, fixed_indices, z_states)\n",
    "            p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "            \n",
    "    return W, theta, mu, sigma, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5114559762018067 0.37116036916138695\n",
      "-23.477571361505852 35.70796897554486\n",
      "-0.37341581275389374 3.609967541902764\n",
      "1.4366602990173498\n",
      "0.10039143798782463 0.5960327919588387\n"
     ]
    }
   ],
   "source": [
    "params = {'mu': mu,\n",
    "'sigma': sigma,\n",
    "'a': a,\n",
    "'b': b,\n",
    "'W': W,\n",
    "'theta': theta}\n",
    "\n",
    "print(np.amin(mu), np.amax(mu))\n",
    "print(np.amin(W), np.amax(W))\n",
    "print(np.amin(theta), np.amax(theta))\n",
    "print(sigma)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 523,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 524,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing probabilities...\n",
      "Running EM:\n",
      "   Iteration 0...Updating W...19.39.59.79.99.Updating mu, sigma, a..."
     ]
    }
   ],
   "source": [
    "W, theta, mu, sigma, a, b = EM_loop(1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U = im.get_squashed_X(X, a, b)\n",
    "p_z = im.get_p_z(z_states, theta)\n",
    "s_states = im.get_s_states(z_states, W, p_z)\n",
    "p_x_z = im.get_all_p_x_z(mu, sigma, s_states, U, fixed_indices, z_states)\n",
    "p_z_x = im.get_all_p_z_x(p_x_z, p_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for _ in range(num_states*10):\n",
    "    np.nan_to_num(0)\n",
    "    #pass\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 531,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for test_ix in test_indices[5:]:\n",
    "    p_zi_x = np.zeros([num_features])\n",
    "    for f_ix in range(num_features):\n",
    "        state_ixs = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "        p_zi_x[f_ix] = sum(p_z_x[test_ix, state_ixs])\n",
    "        #for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "        #    output[z] = output[z] + [f, strength]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 532,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hyperintense mass on delayed phase 50%\n",
      "lobulated margins 50%\n",
      "regular spherical hypointense mass 42%\n",
      "thin well-defined walls 33%\n",
      "arterial enhancement 33%\n",
      "progressive centripetal filling 31%\n"
     ]
    }
   ],
   "source": [
    "for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "    if strength<0.3:\n",
    "        break\n",
    "    print(\"%s %d%%\" % (all_features[f], strength*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 533,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADPCAYAAAD21NURAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nGZIkSV37H8F9c8ZjL/GYAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e982262940>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for cls in C.classes_to_include:\n",
    "    x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == indices_f[test_ix])]\n",
    "    if len(x_test_quick) > 0:\n",
    "        break\n",
    "hf.draw_slices(x_test_quick[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@njit\n",
    "def update_stdevs(mu, m, sigma, s, z_states, filter_results, p_z_x):\n",
    "    sigma_est = np.empty([num_features, num_units])\n",
    "    s_est = np.empty([num_units])\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "\n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in range(num_states):\n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - \\\n",
    "                       np.dot(mu[:, u_ix], z_states_bool[state_ix]) - m[u_ix])**2 for img_ix in range(num_imgs)]) / 2\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "        temp = scipy.optimize.minimize(\\\n",
    "                    lambda Var: sum([a_i[state_ix] / (sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) + c_i[state_ix] * log(sqrt(sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) * sqrt(2*pi)) for state_ix in range(num_states)]),\n",
    "                      np.concatenate([sigma[:, u_ix]**2, [s[u_ix]**2]]), \n",
    "                      bounds=tuple(itertools.repeat((.001, 1000),num_features+1)))\n",
    "\n",
    "        sigma_est[:, u_ix] = [sqrt(i) for i in temp['x'][:-1]]\n",
    "        s_est[u_ix] = sqrt(temp['x'][-1])\n",
    "    \n",
    "    return sigma_est, s_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([a_i[state_ix]/(x+var_adj[state_ix])**2 - \\\n",
    "                                         c_i[state_ix]/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "#state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "#p_z_x does not need to be rescaled because the factor cancels out\n",
    "var_adj = np.zeros([num_states])\n",
    "a_i = np.zeros([num_states])\n",
    "c_i = np.zeros([num_states])\n",
    "\n",
    "for u_ix in range(num_units):\n",
    "    for state_ix in range(num_states):\n",
    "        mean = np.dot(mu[:, u_ix], z_states[state_ix]) + m[u_ix]\n",
    "        a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "        c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "    temp = scipy.optimize.linearmixing(\\\n",
    "                lambda Var: sum([a_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2)**2 - \\\n",
    "                 c_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2) for state_ix in range(num_states)]), \\\n",
    "                  np.concatenate([sigma[:, u_ix], [s[u_ix]]]), verbose=True, maxiter=10000)\n",
    "    \n",
    "    #sigma_est[:, u_ix]\n",
    "    if u_ix % 20 == 0:\n",
    "        print(u_ix, time.time()-t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([(a_i[state_ix]/(x+var_adj[state_ix]) - \\\n",
    "                                         c_i[state_ix])/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_test_features = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares approach\n",
    "Theta = np.array([feature_filter_means[f] for f in all_features])\n",
    "Theta = np.transpose(Theta, (1,0))\n",
    "\n",
    "filters_test = {}\n",
    "features_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)\n",
    "    filters_test[cls] = (filters_test[cls] - filter_avgs) / filter_stds\n",
    "    features_test[cls] = np.linalg.lstsq(Theta, np.transpose(filters_test[cls], (1,0)))[0]\n",
    "    #filters_test[cls] = np.apply_along_axis(lambda x: x / filter_avgs, 1, filters_test[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covar_to_corr(covar):\n",
    "    A = np.diag(np.diag(covar)**(-0.5))\n",
    "    return np.matmul(np.matmul(A, covar), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = covar_to_corr(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(corr - np.linalg.pinv(np.diag(np.diag(np.linalg.pinv(corr)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cutoff_eigenval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.random.normal(size=filter_results.shape)\n",
    "dummy_fa = FactorAnalysis()\n",
    "dummy_fa.fit(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.linalg.eigvals(fa.get_covariance()) > np.mean(np.linalg.eigvals(dummy_fa.get_covariance())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=10,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.fit(filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89081373261106078"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=3\n",
    "fa.get_covariance()[x,x] - fa.noise_variance_[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = cnna.get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))\n",
    "\n",
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "for cls in C.classes_to_include:\n",
    "    for img_num in range(len(filters_test[cls])):\n",
    "        z = Z_test[cls][img_num]\n",
    "        x = np.expand_dims(x_test[cls][img_num], axis=0)\n",
    "        evidence = {}\n",
    "        \n",
    "        output[z] = [cls]\n",
    "        \n",
    "        preds = model.predict(x, verbose=False)[0]\n",
    "        for pred_cls, pred_conf in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:2]:\n",
    "            output[z] = output[z] + [pred_cls]\n",
    "        \n",
    "        #for f in all_features:\n",
    "        #    evidence[f + \"/\" + str(cls_features[f])] = get_evidence_strength(feature_filters[f], filters_test[cls][img_num])\n",
    "        \n",
    "        for i in range(len(all_features)):\n",
    "            evidence[all_features[i] + \"/\" + str(cls_features[all_features[i]])] = features_test[cls][i, img_num]\n",
    "        \n",
    "        f1='infiltrative'\n",
    "        f2='lobulated margins'\n",
    "        if evidence[f1 + \"/\" + str(cls_features[f1])] < evidence[f2 + \"/\" + str(cls_features[f2])]:\n",
    "            evidence.pop(f1 + \"/\" + str(cls_features[f1]))\n",
    "        else:\n",
    "            evidence.pop(f2 + \"/\" + str(cls_features[f2]))\n",
    "        \n",
    "        for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True):\n",
    "            output[z] = output[z] + [f, strength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred5.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_cls2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_features)):\n",
    "        writer.writerow([Z_test_features[z_num]] + [output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][3], \\\n",
    "                        output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][5]] + output[Z_test_features[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:\\\\filters_pred3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "for _,row in df.iterrows():\n",
    "    if row[\"pred_cls1\"] in row[\"feature_1\"]:\n",
    "        agree += 1\n",
    "print(agree/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
