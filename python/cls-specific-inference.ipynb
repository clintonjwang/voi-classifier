{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.models\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from keras import backend as K\n",
    "\n",
    "import argparse\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import csv\n",
    "import niftiutils.helper_fxns as hf\n",
    "import importlib\n",
    "import inference_methods as im\n",
    "import itertools\n",
    "from math import sqrt, log, pi, exp\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange, vectorize, guvectorize\n",
    "from numpy import matmul, diag\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config()\n",
    "T = config.Hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_length(vector):\n",
    "    return sqrt(np.sum(vector**2))\n",
    "\n",
    "def squash_per_dim(vector, eps = 10**-10):\n",
    "    for i,x in enumerate(vector):\n",
    "        vector[i] *= x / (1 + x**2)\n",
    "    return vector\n",
    "\n",
    "def squash(vector, eps = 10**-10):\n",
    "    s_squared_norm = np.sum(vector**2)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / sqrt(s_squared_norm + eps)\n",
    "    return scale * vector\n",
    "\n",
    "def vec_distance(u, v):\n",
    "    return sqrt(np.sum((u - v)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305\n",
    "model_dense_outputs = cbuild.build_pretrain_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final_outputs = cbuild.build_pretrain_model(model, last_layer=\"pre-softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[-3].get_weights()[0]\n",
    "bias = model.layers[-3].get_weights()[1]\n",
    "\n",
    "#np.dot(filter_results[0], W) * eff_mult + eff_bias\n",
    "\n",
    "gamma, beta, mu, var = model.layers[-2].get_weights()\n",
    "\n",
    "eff_bias = (np.zeros(6) + bias - mu) / var**.5 * gamma + beta\n",
    "eff_mult = (np.ones(6) + bias - mu) / var**.5 * gamma + beta - eff_bias\n",
    "\n",
    "W_eff = W * eff_mult# + eff_bias\n",
    "\n",
    "#(np.dot(filter_results[0], W) + bias - mu) / var**.5 * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_vector(vector, feature_vectors):\n",
    "    best_feature = -1\n",
    "    for f_ix, feature_vec in enumerate(feature_vectors):\n",
    "        if np.dot(vector, feature_vec) < threshold:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_filter_results = (filter_results - filter_avgs) / filter_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.amin(filter_results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vectors = np.empty((num_features, num_units))\n",
    "feature_relevance = np.empty(num_features)\n",
    "for f_ix in range(num_features):\n",
    "    feature_vectors[f_ix, :] = np.mean(feature_filters[all_features[f_ix]], axis=0)\n",
    "    feature_evidence[f_ix] = np.dot(feature_vectors[f_ix], W_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squashed_filter_results = (filter_results / filter_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- feature_vectors * unit_relevance should be maximized\n",
    "\n",
    "- np.dot(feature_vectors[i], feature_vectors[j]) should be minimized, OR\n",
    "- vec_distance(feature_vectors[i], feature_vectors[j]) should be maximized for all pairs i,j\n",
    "\n",
    "- features \"turn on / off\" specific units; try to minimize the number of units impacted by a given feature\n",
    "\n",
    "- p(z|x) > .75 for all x manually annotated by z\n",
    "\n",
    "===\n",
    "- show % of evidence explained (fraction of sum of contributing units that are captured by features that turn those units on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_relevance = np.empty(num_units)\n",
    "for u_ix in range(num_units):\n",
    "    unit_relevance[u_ix] = np.amax(W_eff[u_ix]) - np.amin(W_eff[u_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_results = filter_results*unit_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_length = np.mean(np.apply_along_axis(get_length, 1, filter_results*unit_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239091267665856"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(squash(filter_results[2] * unit_relevance * 2/avg_length)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "filters_by_cls = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "filter_results = np.concatenate([filters_by_cls[cls] for cls in C.classes_to_include], axis=0)\n",
    "\n",
    "filter_avgs = np.mean(filter_results, axis=0)\n",
    "filter_stds = np.std(filter_results, axis=0)\n",
    "\n",
    "#filter_cls_avg_unscaled = {cls: np.mean(filter_results[cls], axis=0) for cls in C.classes_to_include}\n",
    "#filter_cls_avg_scaled = {cls: np.mean(filter_results[cls], axis=0) / filter_avgs for cls in C.classes_to_include}\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "feat_count.pop(\"central scar\")\n",
    "all_features = list(feat_count.keys())\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = dict(zip(*np.unique(features_by_cls['colorectal'], return_counts=True)))\n",
    "for k in z:\n",
    "    print(k, \" (\",z[k],\")\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "feat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_features = cnna.get_annotated_files(features_by_cls)\n",
    "Z_features.pop(\"central scar\")\n",
    "\n",
    "feature_filter_means = {}\n",
    "feature_filter_stds = {}\n",
    "feature_filters_scaled = {}#{cls: {} for cls in features}\n",
    "feature_filters = {f:np.empty([0,100]) for f in all_features}\n",
    "\n",
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs\n",
    "        \n",
    "    feature_filters[f] = (feature_filters[f] - filter_avgs) / filter_stds\n",
    "    \n",
    "    feature_filter_means[f] = np.mean(feature_filters[f], axis=0)# / filter_avgs\n",
    "    feature_filter_stds[f] = np.std(feature_filters[f], axis=0)# / filter_avgs\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > np.mean(ff), ff, 0)\n",
    "\n",
    "    #ff = feature_filters[cls][f]\n",
    "    #feature_filters[cls][f] = np.where(ff > 1*filter_cls_avg_unscaled[cls], ff, 0)\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > 1.3*filter_avgs, ff, 0)\n",
    "\n",
    "    feature_filters_scaled[f] = feature_filter_means[f] / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(all_features) # number of features\n",
    "num_units = 100 # number of dense units\n",
    "\n",
    "num_annotations = 8\n",
    "Z_test = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "indices_f = [orig_data_dict[cls][1] for cls in C.classes_to_include]\n",
    "indices_f = hf.flatten(indices_f)\n",
    "\n",
    "fixed_indices = np.empty([num_features, num_annotations])\n",
    "for f_ix,f in enumerate(all_features):\n",
    "    fixed_indices[f_ix, :] = np.where(np.isin(indices_f, random.sample(set(Z_features[f]), num_annotations)))[0]\n",
    "fixed_indices = fixed_indices.astype(int)\n",
    "\n",
    "test_indices = np.where(np.isin(indices_f, Z_test))[0]\n",
    "\n",
    "z_states = np.array([z for z in itertools.product([0,1], repeat=num_features) if sum(z) <= 4 and sum(z) >= 2])\n",
    "num_states = len(z_states)\n",
    "num_imgs = filter_results.shape[0]\n",
    "z_states_bool = [tuple([bool(x) for x in z]) for z in z_states]\n",
    "\n",
    "X = filter_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 458,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM_loop(k, alpha=.8, beta=.8, **kwargs):\n",
    "    if len(kwargs) == 0:\n",
    "        W = np.empty((num_features, num_units))\n",
    "        for f_ix in range(num_features):\n",
    "            W[f_ix] = feature_filter_means[all_features[f_ix]]*5\n",
    "        mu = np.ones(num_units)\n",
    "        sigma = 5\n",
    "        a = 5\n",
    "        b = 3\n",
    "        theta = scipy.random.normal(size=(num_features, num_features))\n",
    "    else:\n",
    "        mu = kwargs['mu']\n",
    "        sigma = kwargs['sigma']\n",
    "        W = kwargs['W']\n",
    "        a = kwargs['a']\n",
    "        b = kwargs['b']\n",
    "        theta = kwargs['theta']\n",
    "    \n",
    "    print(\"Initializing probabilities...\")\n",
    "    U = im.get_squashed_X(X, a, b)\n",
    "    p_z = im.get_p_z(z_states, theta)\n",
    "    s_states = im.get_s_states(z_states, W, p_z)\n",
    "    p_x_z = im.get_all_p_x_z(mu, sigma, s_states, U, fixed_indices, z_states)\n",
    "    p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "    \n",
    "    print(\"Running EM:\")\n",
    "    for jj in range(k):\n",
    "        print(\"   Iteration\", jj, end=\"...\")\n",
    "        theta = im.update_thetas(np.sum(p_z_x, axis=0), z_states_bool, theta, alpha)\n",
    "\n",
    "        p_z = im.get_p_z(z_states, theta)\n",
    "        s_states = im.get_s_states(z_states, W, p_z)\n",
    "        p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "\n",
    "        print(\"Updating W...\", end=\"\")\n",
    "        W = im.update_W(mu, W, z_states, U, p_z_x, fixed_indices, alpha)\n",
    "        \n",
    "        print(\"Updating mu, sigma, a, b...\", end=\"\")\n",
    "        mu = im.update_mus(mu, s_states, U, p_z_x, beta)\n",
    "        sigma = im.update_sigma(mu, sigma, s_states, U, p_z_x)\n",
    "        a, b = im.update_ab(mu, a, b, sigma, s_states, X, p_z_x, alpha)\n",
    "\n",
    "        if jj < k-1:\n",
    "            print(\"Updating probabilities...\")\n",
    "            U = im.get_squashed_X(X, a, b)\n",
    "            p_x_z = im.get_all_p_x_z(mu, sigma, s_states, U, fixed_indices, z_states)\n",
    "            p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "            \n",
    "    return W, theta, mu, sigma, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.5081943693658567 0.406778526706645\n",
      "-4656.812675023782 2685.931452122116\n",
      "-2.772420050528194 2.4864205927611613\n",
      "2.2390123722107806\n",
      "0.4522379730259878 1.0995914246743232\n"
     ]
    }
   ],
   "source": [
    "params = {'mu': mu,\n",
    "'sigma': sigma,\n",
    "'a': a,\n",
    "'b': b,\n",
    "'W': W,\n",
    "'theta': theta}\n",
    "\n",
    "print(np.amin(mu), np.amax(mu))\n",
    "print(np.amin(W), np.amax(W))\n",
    "print(np.amin(theta), np.amax(theta))\n",
    "print(sigma)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)\n",
    "im.W_opt_func(np.ravel(W), a_i, b_i, c_i, z_states, W, fixed_indices, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)\n",
    "#W = im.update_W(mu, W, z_states, U, p_z_x, fixed_indices, .5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing probabilities...\n",
      "Running EM:\n",
      "   Iteration 0...Updating W..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating mu, sigma, a, b...Updating probabilities...\n",
      "   Iteration 1...Updating W..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating mu, sigma, a, b..."
     ]
    }
   ],
   "source": [
    "W, theta, mu, sigma, a, b = EM_loop(2, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U = im.get_squashed_X(X, a, b)\n",
    "p_z = im.get_p_z(z_states, theta)\n",
    "s_states = im.get_s_states(z_states, W, p_z)\n",
    "p_x_z = im.get_all_p_x_z(mu, sigma, s_states, U, fixed_indices, z_states)\n",
    "p_z_x = im.get_all_p_z_x(p_x_z, p_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90258907, 0.71103892, 0.83980306, ..., 0.63327332, 0.75203248,\n",
       "        0.83408646],\n",
       "       [0.89236645, 0.70221545, 0.84021332, ..., 0.61368347, 0.74269648,\n",
       "        0.82401343],\n",
       "       [0.90030987, 0.7125053 , 0.84443901, ..., 0.62957205, 0.76503742,\n",
       "        0.84298065],\n",
       "       ...,\n",
       "       [0.88828572, 0.70134657, 0.85327451, ..., 0.62657499, 0.75126848,\n",
       "        0.83175554],\n",
       "       [0.88742008, 0.6986848 , 0.85106994, ..., 0.61511456, 0.74837652,\n",
       "        0.83094499],\n",
       "       [0.89227868, 0.67837364, 0.84718689, ..., 0.62329841, 0.74032896,\n",
       "        0.83429364]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p_x_z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for _ in range(num_states*10):\n",
    "    np.nan_to_num(0)\n",
    "    #pass\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nodular or discontinuous enhancement 92%\n",
      "hyperintense mass on delayed phase 67%\n",
      "lobulated margins 62%\n",
      "progressive or concentric enhancement 60%\n",
      "infiltrative 56%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADPCAYAAAD21NURAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e4d6085518>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ix = 10\n",
    "#for test_ix in test_indices[5:]:\n",
    "p_zi_x = np.zeros([num_features])\n",
    "for f_ix in range(num_features):\n",
    "    state_ixs = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    p_zi_x[f_ix] = sum(p_z_x[test_ix, state_ixs])\n",
    "    #for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "    #    output[z] = output[z] + [f, strength]\n",
    "#break\n",
    "\n",
    "for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "    if strength<0.3:\n",
    "        break\n",
    "    print(\"%s %d%%\" % (all_features[f], strength*100))\n",
    "\n",
    "for cls in C.classes_to_include:\n",
    "    x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == indices_f[test_ix])]\n",
    "    if len(x_test_quick) > 0:\n",
    "        break\n",
    "hf.draw_slices(x_test_quick[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@njit\n",
    "def update_stdevs(mu, m, sigma, s, z_states, filter_results, p_z_x):\n",
    "    sigma_est = np.empty([num_features, num_units])\n",
    "    s_est = np.empty([num_units])\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "\n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in range(num_states):\n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - \\\n",
    "                       np.dot(mu[:, u_ix], z_states_bool[state_ix]) - m[u_ix])**2 for img_ix in range(num_imgs)]) / 2\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "        temp = scipy.optimize.minimize(\\\n",
    "                    lambda Var: sum([a_i[state_ix] / (sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) + c_i[state_ix] * log(sqrt(sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) * sqrt(2*pi)) for state_ix in range(num_states)]),\n",
    "                      np.concatenate([sigma[:, u_ix]**2, [s[u_ix]**2]]), \n",
    "                      bounds=tuple(itertools.repeat((.001, 1000),num_features+1)))\n",
    "\n",
    "        sigma_est[:, u_ix] = [sqrt(i) for i in temp['x'][:-1]]\n",
    "        s_est[u_ix] = sqrt(temp['x'][-1])\n",
    "    \n",
    "    return sigma_est, s_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([a_i[state_ix]/(x+var_adj[state_ix])**2 - \\\n",
    "                                         c_i[state_ix]/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "#state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "#p_z_x does not need to be rescaled because the factor cancels out\n",
    "var_adj = np.zeros([num_states])\n",
    "a_i = np.zeros([num_states])\n",
    "c_i = np.zeros([num_states])\n",
    "\n",
    "for u_ix in range(num_units):\n",
    "    for state_ix in range(num_states):\n",
    "        mean = np.dot(mu[:, u_ix], z_states[state_ix]) + m[u_ix]\n",
    "        a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "        c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "    temp = scipy.optimize.linearmixing(\\\n",
    "                lambda Var: sum([a_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2)**2 - \\\n",
    "                 c_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2) for state_ix in range(num_states)]), \\\n",
    "                  np.concatenate([sigma[:, u_ix], [s[u_ix]]]), verbose=True, maxiter=10000)\n",
    "    \n",
    "    #sigma_est[:, u_ix]\n",
    "    if u_ix % 20 == 0:\n",
    "        print(u_ix, time.time()-t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([(a_i[state_ix]/(x+var_adj[state_ix]) - \\\n",
    "                                         c_i[state_ix])/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_test_features = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares approach\n",
    "Theta = np.array([feature_filter_means[f] for f in all_features])\n",
    "Theta = np.transpose(Theta, (1,0))\n",
    "\n",
    "filters_test = {}\n",
    "features_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)\n",
    "    filters_test[cls] = (filters_test[cls] - filter_avgs) / filter_stds\n",
    "    features_test[cls] = np.linalg.lstsq(Theta, np.transpose(filters_test[cls], (1,0)))[0]\n",
    "    #filters_test[cls] = np.apply_along_axis(lambda x: x / filter_avgs, 1, filters_test[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covar_to_corr(covar):\n",
    "    A = np.diag(np.diag(covar)**(-0.5))\n",
    "    return np.matmul(np.matmul(A, covar), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = covar_to_corr(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(corr - np.linalg.pinv(np.diag(np.diag(np.linalg.pinv(corr)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cutoff_eigenval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.random.normal(size=filter_results.shape)\n",
    "dummy_fa = FactorAnalysis()\n",
    "dummy_fa.fit(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.linalg.eigvals(fa.get_covariance()) > np.mean(np.linalg.eigvals(dummy_fa.get_covariance())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=10,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.fit(filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89081373261106078"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=3\n",
    "fa.get_covariance()[x,x] - fa.noise_variance_[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = cnna.get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))\n",
    "\n",
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "for cls in C.classes_to_include:\n",
    "    for img_num in range(len(filters_test[cls])):\n",
    "        z = Z_test[cls][img_num]\n",
    "        x = np.expand_dims(x_test[cls][img_num], axis=0)\n",
    "        evidence = {}\n",
    "        \n",
    "        output[z] = [cls]\n",
    "        \n",
    "        preds = model.predict(x, verbose=False)[0]\n",
    "        for pred_cls, pred_conf in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:2]:\n",
    "            output[z] = output[z] + [pred_cls]\n",
    "        \n",
    "        #for f in all_features:\n",
    "        #    evidence[f + \"/\" + str(cls_features[f])] = get_evidence_strength(feature_filters[f], filters_test[cls][img_num])\n",
    "        \n",
    "        for i in range(len(all_features)):\n",
    "            evidence[all_features[i] + \"/\" + str(cls_features[all_features[i]])] = features_test[cls][i, img_num]\n",
    "        \n",
    "        f1='infiltrative'\n",
    "        f2='lobulated margins'\n",
    "        if evidence[f1 + \"/\" + str(cls_features[f1])] < evidence[f2 + \"/\" + str(cls_features[f2])]:\n",
    "            evidence.pop(f1 + \"/\" + str(cls_features[f1]))\n",
    "        else:\n",
    "            evidence.pop(f2 + \"/\" + str(cls_features[f2]))\n",
    "        \n",
    "        for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True):\n",
    "            output[z] = output[z] + [f, strength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred5.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_cls2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_features)):\n",
    "        writer.writerow([Z_test_features[z_num]] + [output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][3], \\\n",
    "                        output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][5]] + output[Z_test_features[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:\\\\filters_pred3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "for _,row in df.iterrows():\n",
    "    if row[\"pred_cls1\"] in row[\"feature_1\"]:\n",
    "        agree += 1\n",
    "print(agree/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
