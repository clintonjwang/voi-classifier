{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "import keras.models\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from keras import backend as K\n",
    "\n",
    "import argparse\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import csv\n",
    "import niftiutils.helper_fxns as hf\n",
    "import importlib\n",
    "import inference_methods as im\n",
    "import itertools\n",
    "from math import sqrt, log, pi, exp\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange, vectorize, guvectorize\n",
    "from numpy import matmul, diag\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config()\n",
    "T = config.Hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_length(vector):\n",
    "    return sqrt(np.sum(vector**2))\n",
    "\n",
    "def squash_per_dim(vector, eps = 10**-10):\n",
    "    for i,x in enumerate(vector):\n",
    "        vector[i] *= x / (1 + x**2)\n",
    "    return vector\n",
    "\n",
    "def squash(vector, eps = 10**-10):\n",
    "    s_squared_norm = np.sum(vector**2)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / sqrt(s_squared_norm + eps)\n",
    "    return scale * vector\n",
    "\n",
    "def vec_distance(u, v):\n",
    "    return sqrt(np.sum((u - v)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305\n",
    "model_dense_outputs = cbuild.build_pretrain_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final_outputs = cbuild.build_pretrain_model(model, last_layer=\"pre-softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[-3].get_weights()[0]\n",
    "bias = model.layers[-3].get_weights()[1]\n",
    "\n",
    "#np.dot(filter_results[0], W) * eff_mult + eff_bias\n",
    "\n",
    "gamma, beta, mu, var = model.layers[-2].get_weights()\n",
    "\n",
    "eff_bias = (np.zeros(6) + bias - mu) / var**.5 * gamma + beta\n",
    "eff_mult = (np.ones(6) + bias - mu) / var**.5 * gamma + beta - eff_bias\n",
    "\n",
    "W_eff = W * eff_mult# + eff_bias\n",
    "\n",
    "#(np.dot(filter_results[0], W) + bias - mu) / var**.5 * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_vector(vector, feature_vectors):\n",
    "    best_feature = -1\n",
    "    for f_ix, feature_vec in enumerate(feature_vectors):\n",
    "        if np.dot(vector, feature_vec) < threshold:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_filter_results = (filter_results - filter_avgs) / filter_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.amin(filter_results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vectors = np.empty((num_features, num_units))\n",
    "feature_relevance = np.empty(num_features)\n",
    "for f_ix in range(num_features):\n",
    "    feature_vectors[f_ix, :] = np.mean(feature_filters[all_features[f_ix]], axis=0)\n",
    "    feature_evidence[f_ix] = np.dot(feature_vectors[f_ix], W_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squashed_filter_results = (filter_results / filter_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- feature_vectors * unit_relevance should be maximized\n",
    "\n",
    "- np.dot(feature_vectors[i], feature_vectors[j]) should be minimized, OR\n",
    "- vec_distance(feature_vectors[i], feature_vectors[j]) should be maximized for all pairs i,j\n",
    "\n",
    "- features \"turn on / off\" specific units; try to minimize the number of units impacted by a given feature\n",
    "\n",
    "- p(z|x) > .75 for all x manually annotated by z\n",
    "\n",
    "===\n",
    "- show % of evidence explained (fraction of sum of contributing units that are captured by features that turn those units on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_relevance = np.empty(num_units)\n",
    "for u_ix in range(num_units):\n",
    "    unit_relevance[u_ix] = np.amax(W_eff[u_ix]) - np.amin(W_eff[u_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_results = filter_results*unit_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_length = np.mean(np.apply_along_axis(get_length, 1, filter_results*unit_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239091267665856"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(squash(filter_results[2] * unit_relevance * 2/avg_length)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "filters_by_cls = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "filter_results = np.concatenate([filters_by_cls[cls] for cls in C.classes_to_include], axis=0)\n",
    "\n",
    "filter_avgs = np.mean(filter_results, axis=0)\n",
    "filter_stds = np.std(filter_results, axis=0)\n",
    "\n",
    "#filter_cls_avg_unscaled = {cls: np.mean(filter_results[cls], axis=0) for cls in C.classes_to_include}\n",
    "#filter_cls_avg_scaled = {cls: np.mean(filter_results[cls], axis=0) / filter_avgs for cls in C.classes_to_include}\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "feat_count.pop(\"central scar\")\n",
    "all_features = list(feat_count.keys())\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = dict(zip(*np.unique(features_by_cls['colorectal'], return_counts=True)))\n",
    "for k in z:\n",
    "    print(k, \" (\",z[k],\")\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "feat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_features = cnna.get_annotated_files(features_by_cls)\n",
    "Z_features.pop(\"central scar\")\n",
    "\n",
    "feature_filter_means = {}\n",
    "feature_filter_stds = {}\n",
    "feature_filters_scaled = {}#{cls: {} for cls in features}\n",
    "feature_filters = {f:np.empty([0,100]) for f in all_features}\n",
    "\n",
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs\n",
    "        \n",
    "    feature_filters[f] = (feature_filters[f] - filter_avgs) / filter_stds\n",
    "    \n",
    "    feature_filter_means[f] = np.mean(feature_filters[f], axis=0)# / filter_avgs\n",
    "    feature_filter_stds[f] = np.std(feature_filters[f], axis=0)# / filter_avgs\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > np.mean(ff), ff, 0)\n",
    "\n",
    "    #ff = feature_filters[cls][f]\n",
    "    #feature_filters[cls][f] = np.where(ff > 1*filter_cls_avg_unscaled[cls], ff, 0)\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > 1.3*filter_avgs, ff, 0)\n",
    "\n",
    "    feature_filters_scaled[f] = feature_filter_means[f] / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(all_features) # number of features\n",
    "num_units = 100 # number of dense units\n",
    "\n",
    "num_annotations = 8\n",
    "Z_test = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "indices_f = [orig_data_dict[cls][1] for cls in C.classes_to_include]\n",
    "indices_f = hf.flatten(indices_f)\n",
    "\n",
    "fixed_indices = np.empty([num_features, num_annotations])\n",
    "for f_ix,f in enumerate(all_features):\n",
    "    fixed_indices[f_ix, :] = np.where(np.isin(indices_f, random.sample(set(Z_features[f]), num_annotations)))[0]\n",
    "fixed_indices = fixed_indices.astype(int)\n",
    "\n",
    "test_indices = np.where(np.isin(indices_f, Z_test))[0]\n",
    "\n",
    "z_states = np.array([z for z in itertools.product([0,1], repeat=num_features) if sum(z) <= 4 and sum(z) >= 2])\n",
    "num_states = len(z_states)\n",
    "num_imgs = filter_results.shape[0]\n",
    "z_states_bool = [tuple([bool(x) for x in z]) for z in z_states]\n",
    "\n",
    "X = filter_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM_loop(k, alpha=.8, beta=.8, **kwargs):\n",
    "    if len(kwargs) == 0:\n",
    "        W = np.empty((num_features, num_units))\n",
    "        for f_ix in range(num_features):\n",
    "            W[f_ix] = feature_filter_means[all_features[f_ix]]*5\n",
    "        mu = np.ones(num_units)\n",
    "        sigma = 5\n",
    "        a = 5\n",
    "        theta = scipy.random.normal(size=(num_features, num_features))\n",
    "    else:\n",
    "        mu = kwargs['mu']\n",
    "        sigma = kwargs['sigma']\n",
    "        W = kwargs['W']\n",
    "        a = kwargs['a']\n",
    "        theta = kwargs['theta']\n",
    "    \n",
    "    print(\"Initializing probabilities...\")\n",
    "    p_z = im.get_p_z(z_states, theta)\n",
    "    s_states = im.get_s_states(z_states, W, p_z)\n",
    "    p_x_z = im.get_all_p_x_z(mu, sigma, a, s_states, X, fixed_indices, z_states)\n",
    "    p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "    \n",
    "    print(\"Running EM:\")\n",
    "    for jj in range(k):\n",
    "        print(\"   Iteration\", jj, end=\"...\")\n",
    "        theta = im.update_thetas(np.sum(p_z_x, axis=0), z_states_bool, theta, alpha)\n",
    "\n",
    "        p_z = im.get_p_z(z_states, theta)\n",
    "        p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "\n",
    "        print(\"Updating W...\", end=\"\")\n",
    "        U = im.get_squashed_X(X, a)\n",
    "        W = im.update_W(mu, a, W, z_states, U, p_z_x, alpha)\n",
    "        \n",
    "        print(\"Updating mu, sigma, a...\", end=\"\")\n",
    "        mu = im.update_mus(mu, a, W, sigma, s_states, X, p_z_x, fixed_indices, beta)\n",
    "        sigma = im.update_sigma(mu, a, sigma, s_states, X, p_z_x)\n",
    "        a = im.update_a(mu, a, sigma, s_states, X, p_z_x, alpha)\n",
    "\n",
    "        if jj < k-1:\n",
    "            print(\"Updating probabilities...\")\n",
    "            s_states = im.get_s_states(z_states, W, p_z)\n",
    "            p_x_z = im.get_all_p_x_z(mu, sigma, a, s_states, X, fixed_indices, z_states)\n",
    "            p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "            \n",
    "    return W, theta, mu, sigma, a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan nan\n",
      "-46.7117992338079 2.999305426343363\n",
      "nan nan\n",
      "nan\n",
      "9.99999466978039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\numpy\\core\\_methods.py:29: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_minimum(a, axis, None, out, keepdims)\n",
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    }
   ],
   "source": [
    "params = {'mu': mu,\n",
    "'sigma': sigma,\n",
    "'a': a,\n",
    "'W': W,\n",
    "'theta': theta}\n",
    "\n",
    "print(np.amin(mu), np.amax(mu))\n",
    "print(np.amin(W), np.amax(W))\n",
    "print(np.amin(theta), np.amax(theta))\n",
    "print(sigma)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing probabilities...\n",
      "Running EM:\n",
      "   Iteration 0...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 1...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 2...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 3...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 4...Updating W..."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Clinton\\AppData\\Local\\conda\\conda\\envs\\old-keras\\lib\\site-packages\\numpy\\core\\_methods.py:26: RuntimeWarning: invalid value encountered in reduce\n",
      "  return umr_maximum(a, axis, None, out, keepdims)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 5...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 6...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 7...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 8...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 9...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 10...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 11...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 12...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 13...Updating W...19.39.59.79.99.Updating mu, sigma, a...Updating probabilities...\n",
      "   Iteration 14...Updating W...19.39.59.79.99.Updating mu, sigma, a..."
     ]
    }
   ],
   "source": [
    "W, theta, mu, sigma, a = EM_loop(1, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#importlib.reload(im)\n",
    "p_z = im.get_p_z(z_states, theta)\n",
    "s_states = im.get_s_states(z_states, W, p_z)\n",
    "p_x_z = im.get_all_p_x_z(mu, sigma, a, s_states, X, fixed_indices, z_states)\n",
    "p_z_x = im.get_all_p_z_x(p_x_z, p_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for _ in range(num_states*10):\n",
    "    np.nan_to_num(0)\n",
    "    #pass\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for test_ix in test_indices[60:]:\n",
    "    p_zi_x = np.zeros([num_features])\n",
    "    for f_ix in range(num_features):\n",
    "        state_ixs = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "        p_zi_x[f_ix] = sum(p_z_x[test_ix, state_ixs])\n",
    "        #for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "        #    output[z] = output[z] + [f, strength]\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hypointense without enhancement 99%\n",
      "thin well-defined walls 99%\n",
      "regular spherical hypointense mass 99%\n",
      "arterial enhancement 99%\n"
     ]
    }
   ],
   "source": [
    "for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "    if strength<0.3:\n",
    "        break\n",
    "    print(\"%s %d%%\" % (all_features[f], strength*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADPCAYAAAD21NURAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSZKkrtkMSZKkrv0nTINCRGy1gnQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e94f75bda0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for cls in C.classes_to_include:\n",
    "    x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == indices_f[test_ix])]\n",
    "    if len(x_test_quick) > 0:\n",
    "        break\n",
    "hf.draw_slices(x_test_quick[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@njit\n",
    "def update_stdevs(mu, m, sigma, s, z_states, filter_results, p_z_x):\n",
    "    sigma_est = np.empty([num_features, num_units])\n",
    "    s_est = np.empty([num_units])\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "\n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in range(num_states):\n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - \\\n",
    "                       np.dot(mu[:, u_ix], z_states_bool[state_ix]) - m[u_ix])**2 for img_ix in range(num_imgs)]) / 2\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "        temp = scipy.optimize.minimize(\\\n",
    "                    lambda Var: sum([a_i[state_ix] / (sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) + c_i[state_ix] * log(sqrt(sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) * sqrt(2*pi)) for state_ix in range(num_states)]),\n",
    "                      np.concatenate([sigma[:, u_ix]**2, [s[u_ix]**2]]), \n",
    "                      bounds=tuple(itertools.repeat((.001, 1000),num_features+1)))\n",
    "\n",
    "        sigma_est[:, u_ix] = [sqrt(i) for i in temp['x'][:-1]]\n",
    "        s_est[u_ix] = sqrt(temp['x'][-1])\n",
    "    \n",
    "    return sigma_est, s_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([a_i[state_ix]/(x+var_adj[state_ix])**2 - \\\n",
    "                                         c_i[state_ix]/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "#state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "#p_z_x does not need to be rescaled because the factor cancels out\n",
    "var_adj = np.zeros([num_states])\n",
    "a_i = np.zeros([num_states])\n",
    "c_i = np.zeros([num_states])\n",
    "\n",
    "for u_ix in range(num_units):\n",
    "    for state_ix in range(num_states):\n",
    "        mean = np.dot(mu[:, u_ix], z_states[state_ix]) + m[u_ix]\n",
    "        a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "        c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "    temp = scipy.optimize.linearmixing(\\\n",
    "                lambda Var: sum([a_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2)**2 - \\\n",
    "                 c_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2) for state_ix in range(num_states)]), \\\n",
    "                  np.concatenate([sigma[:, u_ix], [s[u_ix]]]), verbose=True, maxiter=10000)\n",
    "    \n",
    "    #sigma_est[:, u_ix]\n",
    "    if u_ix % 20 == 0:\n",
    "        print(u_ix, time.time()-t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([(a_i[state_ix]/(x+var_adj[state_ix]) - \\\n",
    "                                         c_i[state_ix])/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_test_features = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares approach\n",
    "Theta = np.array([feature_filter_means[f] for f in all_features])\n",
    "Theta = np.transpose(Theta, (1,0))\n",
    "\n",
    "filters_test = {}\n",
    "features_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)\n",
    "    filters_test[cls] = (filters_test[cls] - filter_avgs) / filter_stds\n",
    "    features_test[cls] = np.linalg.lstsq(Theta, np.transpose(filters_test[cls], (1,0)))[0]\n",
    "    #filters_test[cls] = np.apply_along_axis(lambda x: x / filter_avgs, 1, filters_test[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covar_to_corr(covar):\n",
    "    A = np.diag(np.diag(covar)**(-0.5))\n",
    "    return np.matmul(np.matmul(A, covar), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = covar_to_corr(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(corr - np.linalg.pinv(np.diag(np.diag(np.linalg.pinv(corr)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cutoff_eigenval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.random.normal(size=filter_results.shape)\n",
    "dummy_fa = FactorAnalysis()\n",
    "dummy_fa.fit(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.linalg.eigvals(fa.get_covariance()) > np.mean(np.linalg.eigvals(dummy_fa.get_covariance())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=10,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.fit(filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89081373261106078"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=3\n",
    "fa.get_covariance()[x,x] - fa.noise_variance_[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = cnna.get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))\n",
    "\n",
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "for cls in C.classes_to_include:\n",
    "    for img_num in range(len(filters_test[cls])):\n",
    "        z = Z_test[cls][img_num]\n",
    "        x = np.expand_dims(x_test[cls][img_num], axis=0)\n",
    "        evidence = {}\n",
    "        \n",
    "        output[z] = [cls]\n",
    "        \n",
    "        preds = model.predict(x, verbose=False)[0]\n",
    "        for pred_cls, pred_conf in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:2]:\n",
    "            output[z] = output[z] + [pred_cls]\n",
    "        \n",
    "        #for f in all_features:\n",
    "        #    evidence[f + \"/\" + str(cls_features[f])] = get_evidence_strength(feature_filters[f], filters_test[cls][img_num])\n",
    "        \n",
    "        for i in range(len(all_features)):\n",
    "            evidence[all_features[i] + \"/\" + str(cls_features[all_features[i]])] = features_test[cls][i, img_num]\n",
    "        \n",
    "        f1='infiltrative'\n",
    "        f2='lobulated margins'\n",
    "        if evidence[f1 + \"/\" + str(cls_features[f1])] < evidence[f2 + \"/\" + str(cls_features[f2])]:\n",
    "            evidence.pop(f1 + \"/\" + str(cls_features[f1]))\n",
    "        else:\n",
    "            evidence.pop(f2 + \"/\" + str(cls_features[f2]))\n",
    "        \n",
    "        for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True):\n",
    "            output[z] = output[z] + [f, strength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred5.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_cls2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_features)):\n",
    "        writer.writerow([Z_test_features[z_num]] + [output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][3], \\\n",
    "                        output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][5]] + output[Z_test_features[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:\\\\filters_pred3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "for _,row in df.iterrows():\n",
    "    if row[\"pred_cls1\"] in row[\"feature_1\"]:\n",
    "        agree += 1\n",
    "print(agree/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
