{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"THEANO_FLAGS\"] = \"mode=FAST_RUN,device=cuda,floatX=float32\"\n",
    "import pymc3 as pm\n",
    "from scipy import optimize\n",
    "\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "import keras.models\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from keras import backend as K\n",
    "\n",
    "import argparse\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import csv\n",
    "import niftiutils.helper_fxns as hf\n",
    "import importlib\n",
    "import inference_methods_squash as im\n",
    "import itertools\n",
    "from math import sqrt, log, pi, exp\n",
    "import matplotlib.pyplot as plt\n",
    "from numba import jit, njit, prange, vectorize, guvectorize\n",
    "from numpy import matmul, diag\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "from sklearn.cross_validation import train_test_split\n",
    "%matplotlib inline\n",
    "import theano\n",
    "floatX = theano.config.floatX\n",
    "import pymc3 as pm\n",
    "import theano.tensor as T\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from warnings import filterwarnings\n",
    "filterwarnings('ignore')\n",
    "sns.set_style('white')\n",
    "from sklearn import datasets\n",
    "from sklearn.preprocessing import scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 100)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[144, 148, 157, 164, 188, 202, 206, 209],\n",
       "       [  1,  33,  75,  76, 155, 167, 168, 196],\n",
       "       [ 16,  34,  38,  63,  79,  81,  86,  91],\n",
       "       [197, 198, 207, 208, 262, 265, 266, 287],\n",
       "       [156, 158, 162, 163, 168, 182, 185, 191],\n",
       "       [216, 281, 308, 338, 345, 359, 384, 429],\n",
       "       [532, 533, 546, 555, 562, 577, 585, 635],\n",
       "       [439, 445, 455, 456, 475, 483, 485, 490],\n",
       "       [357, 361, 372, 374, 391, 450, 454, 493],\n",
       "       [ 14,  64,  67,  88, 238, 283, 295, 306],\n",
       "       [440, 441, 464, 472, 489, 497, 500, 518],\n",
       "       [347, 364, 371, 384, 420, 429, 430, 636],\n",
       "       [  3,  27,  37,  83, 403, 533, 534, 535],\n",
       "       [432, 445, 446, 467, 480, 490, 491, 494]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fixed_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Wm0 = np.empty((num_features, num_units))\n",
    "Ws0 = np.empty((num_features, num_units))\n",
    "for f_ix in range(num_features):\n",
    "    Wm0[f_ix] = feature_filter_means[all_features[f_ix]]\n",
    "    Ws0[f_ix] = feature_filter_stds[all_features[f_ix]]*10/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_samples = 1000\n",
    "t = np.random.uniform(0,2,(num_samples, num_features)).astype(int)\n",
    "t[:,0] = np.random.uniform(0,1.5,num_samples).astype(int)\n",
    "t[:,1] = np.random.uniform(0,1.9,num_samples).astype(int)\n",
    "t[:,2] = np.random.uniform(.8,1.9,num_samples).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 100)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "init_1 = np.random.uniform(.4,.6,num_features)\n",
    "init_2 = np.random.normal(size=(num_features, num_units))\n",
    "\n",
    "basic_model = pm.Model()\n",
    "with basic_model:\n",
    "    a = pm.Uniform('a', 1,10, testval=3.)\n",
    "    b = pm.Uniform('b', 2,10, testval=3.)\n",
    "    #W_m = pm.Normal('W_m', mu=0, sd=100, shape=(num_features, num_units), testval=init_2)\n",
    "    #W_s = pm.InverseGamma('W_s')\n",
    "    W = pm.Normal('W', mu=Wm0, sd=Ws0, shape=(num_features, num_units), testval=init_2)\n",
    "    #m = pm.Normal('m', mu=10, 100, size=(…))\n",
    "    #sigma = pm.InverseGamma('sigma')\n",
    "    #z = pm.Bernoulli('z', theta_ij, size=(…))\n",
    "    theta = pm.Beta('theta', alpha=1, beta=1, shape=num_features, testval=init_1)\n",
    "    z = pm.Bernoulli('z', p=theta, shape=num_features)#, observed=t)\n",
    "    u = 1/(1+pm.math.exp(-a*pm.math.dot(z,W)+b))\n",
    "    #activ = pm.Normal('activ', mu=u+m, std=sigma, observed=activations)\n",
    "    activ = pm.Normal('activ', mu=u, sd=5, shape=num_units, observed=filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y = make_moons(noise=0.2, random_state=0, n_samples=1000)\n",
    "X = scale(X)\n",
    "X = X.astype(floatX)\n",
    "Y = Y.astype(floatX)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500,)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann_output = theano.shared(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ann_output.set_value(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "logp = -1.6912e+05, ||grad|| = 0.97452: 100%|██████████████████████████████████████| 156/156 [00:00<00:00, 1266.13it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'W': array([[-1.4025581 , -0.08474326, -0.48939875, ..., -1.0920697 ,\n",
       "         -1.2514087 , -0.70374095],\n",
       "        [-0.9080944 , -0.34132725, -0.2510863 , ...,  0.5594402 ,\n",
       "          1.4512801 ,  0.8067736 ],\n",
       "        [ 0.24105658,  0.02376037, -0.18026425, ...,  0.42091006,\n",
       "         -0.4602084 , -1.1821582 ],\n",
       "        ...,\n",
       "        [ 0.7193063 ,  1.3287077 , -1.645354  , ...,  0.6649547 ,\n",
       "         -1.8511689 ,  0.90907335],\n",
       "        [-1.2599311 ,  0.50950235, -0.269478  , ...,  1.064559  ,\n",
       "          0.5588847 , -1.827742  ],\n",
       "        [ 0.6819176 , -0.04619898,  0.8224402 , ...,  0.49505815,\n",
       "          0.49243146,  0.8916288 ]], dtype=float32),\n",
       " 'theta': array([0.00748609, 0.0065336 , 0.9942835 , 0.99425757, 0.00723091,\n",
       "        0.00747007, 0.99424815, 0.9928663 , 0.00698773, 0.99261093,\n",
       "        0.9941459 , 0.00718646, 0.00690237, 0.99299455], dtype=float32),\n",
       " 'theta_logodds__': array([-4.887194 , -5.024243 ,  5.1586633,  5.15411  , -4.922133 ,\n",
       "        -4.889353 ,  5.1524677,  4.9357605, -4.9565873,  4.900341 ,\n",
       "         5.134741 , -4.9283442, -4.9689646,  4.954033 ], dtype=float32),\n",
       " 'z': array([0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1], dtype=int16)}"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_estimate = pm.find_MAP(model=basic_model)#, fmin=optimize.fmin_powell)\n",
    "map_estimate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config()\n",
    "T = config.Hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_length(vector):\n",
    "    return sqrt(np.sum(vector**2))\n",
    "\n",
    "def squash_per_dim(vector, eps = 10**-10):\n",
    "    for i,x in enumerate(vector):\n",
    "        vector[i] *= x / (1 + x**2)\n",
    "    return vector\n",
    "\n",
    "def squash(vector, eps = 10**-10):\n",
    "    s_squared_norm = np.sum(vector**2)\n",
    "    scale = s_squared_norm / (1 + s_squared_norm) / sqrt(s_squared_norm + eps)\n",
    "    return scale * vector\n",
    "\n",
    "def vec_distance(u, v):\n",
    "    return sqrt(np.sum((u - v)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305\n",
    "model_dense_outputs = cbuild.build_pretrain_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_final_outputs = cbuild.build_pretrain_model(model, last_layer=\"pre-softmax\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[-3].get_weights()[0]\n",
    "bias = model.layers[-3].get_weights()[1]\n",
    "\n",
    "#np.dot(filter_results[0], W) * eff_mult + eff_bias\n",
    "\n",
    "gamma, beta, mu, var = model.layers[-2].get_weights()\n",
    "\n",
    "eff_bias = (np.zeros(6) + bias - mu) / var**.5 * gamma + beta\n",
    "eff_mult = (np.ones(6) + bias - mu) / var**.5 * gamma + beta - eff_bias\n",
    "\n",
    "W_eff = W * eff_mult# + eff_bias\n",
    "\n",
    "#(np.dot(filter_results[0], W) + bias - mu) / var**.5 * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify_vector(vector, feature_vectors):\n",
    "    best_feature = -1\n",
    "    for f_ix, feature_vec in enumerate(feature_vectors):\n",
    "        if np.dot(vector, feature_vec) < threshold:\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scaled_filter_results = (filter_results - filter_avgs) / filter_stds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "np.amin(filter_results, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_vectors = np.empty((num_features, num_units))\n",
    "feature_relevance = np.empty(num_features)\n",
    "for f_ix in range(num_features):\n",
    "    feature_vectors[f_ix, :] = np.mean(feature_filters[all_features[f_ix]], axis=0)\n",
    "    feature_evidence[f_ix] = np.dot(feature_vectors[f_ix], W_eff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "squashed_filter_results = (filter_results / filter_stds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- feature_vectors * unit_relevance should be maximized\n",
    "\n",
    "- np.dot(feature_vectors[i], feature_vectors[j]) should be minimized, OR\n",
    "- vec_distance(feature_vectors[i], feature_vectors[j]) should be maximized for all pairs i,j\n",
    "\n",
    "- features \"turn on / off\" specific units; try to minimize the number of units impacted by a given feature\n",
    "\n",
    "- p(z|x) > .75 for all x manually annotated by z\n",
    "\n",
    "===\n",
    "- show % of evidence explained (fraction of sum of contributing units that are captured by features that turn those units on)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_relevance = np.empty(num_units)\n",
    "for u_ix in range(num_units):\n",
    "    unit_relevance[u_ix] = np.amax(W_eff[u_ix]) - np.amin(W_eff[u_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_results = filter_results*unit_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_length = np.mean(np.apply_along_axis(get_length, 1, filter_results*unit_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239091267665856"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(squash(filter_results[2] * unit_relevance * 2/avg_length)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "filters_by_cls = {cls: model_dense_outputs.predict(orig_data_dict[cls][0], verbose=False) for cls in C.classes_to_include}\n",
    "filter_results = np.concatenate([filters_by_cls[cls] for cls in C.classes_to_include], axis=0)\n",
    "\n",
    "filter_avgs = np.mean(filter_results, axis=0)\n",
    "filter_stds = np.std(filter_results, axis=0)\n",
    "\n",
    "#filter_cls_avg_unscaled = {cls: np.mean(filter_results[cls], axis=0) for cls in C.classes_to_include}\n",
    "#filter_cls_avg_scaled = {cls: np.mean(filter_results[cls], axis=0) / filter_avgs for cls in C.classes_to_include}\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "feat_count.pop(\"central scar\")\n",
    "all_features = list(feat_count.keys())\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "z = dict(zip(*np.unique(features_by_cls['colorectal'], return_counts=True)))\n",
    "for k in z:\n",
    "    print(k, \" (\",z[k],\")\", sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "feat_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_features = cnna.get_annotated_files(features_by_cls)\n",
    "Z_features.pop(\"central scar\")\n",
    "\n",
    "feature_filter_means = {}\n",
    "feature_filter_stds = {}\n",
    "feature_filters_scaled = {}#{cls: {} for cls in features}\n",
    "feature_filters = {f:np.empty([0,100]) for f in all_features}\n",
    "\n",
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs\n",
    "        \n",
    "    feature_filters[f] = (feature_filters[f] - filter_avgs) / filter_stds\n",
    "    \n",
    "    feature_filter_means[f] = np.mean(feature_filters[f], axis=0)# / filter_avgs\n",
    "    feature_filter_stds[f] = np.std(feature_filters[f], axis=0)# / filter_avgs\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > np.mean(ff), ff, 0)\n",
    "\n",
    "    #ff = feature_filters[cls][f]\n",
    "    #feature_filters[cls][f] = np.where(ff > 1*filter_cls_avg_unscaled[cls], ff, 0)\n",
    "\n",
    "    #ff = feature_filters[f]\n",
    "    #feature_filters[f] = np.where(ff > 1.3*filter_avgs, ff, 0)\n",
    "\n",
    "    feature_filters_scaled[f] = feature_filter_means[f] / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in all_features:\n",
    "    for cls in C.classes_to_include:\n",
    "        x_features = orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_features[f]))]\n",
    "        if x_features.size > 0:\n",
    "            feature_filters[f] = np.concatenate([feature_filters[f], model_dense_outputs.predict(x_features, verbose=False)], axis=0)# / filter_avgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_features = len(all_features) # number of features\n",
    "num_units = 100 # number of dense units\n",
    "\n",
    "num_annotations = 8\n",
    "Z_test = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "indices_f = [orig_data_dict[cls][1] for cls in C.classes_to_include]\n",
    "indices_f = hf.flatten(indices_f)\n",
    "\n",
    "fixed_indices = np.empty([num_features, num_annotations])\n",
    "for f_ix,f in enumerate(all_features):\n",
    "    fixed_indices[f_ix, :] = np.where(np.isin(indices_f, random.sample(set(Z_features[f]), num_annotations)))[0]\n",
    "fixed_indices = fixed_indices.astype(int)\n",
    "\n",
    "test_indices = np.where(np.isin(indices_f, Z_test))[0]\n",
    "\n",
    "z_states = np.array([z for z in itertools.product([0,1], repeat=num_features) if sum(z) <= 4 and sum(z) >= 2])\n",
    "num_states = len(z_states)\n",
    "num_imgs = filter_results.shape[0]\n",
    "z_states_bool = [tuple([bool(x) for x in z]) for z in z_states]\n",
    "\n",
    "X = filter_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def EM_loop(k, alpha=.8, beta=.3, **kwargs):\n",
    "    if len(kwargs) == 0:\n",
    "        W = np.ones((num_features, num_units))\n",
    "        for f_ix in range(num_features):\n",
    "            W[f_ix] = feature_filter_means[all_features[f_ix]]\n",
    "        mu = np.ones(num_units)\n",
    "        sigma = 5\n",
    "        a = 5\n",
    "        b = 3\n",
    "        theta = np.zeros((num_features, num_features))#scipy.random.normal(size=(num_features, num_features))\n",
    "    else:\n",
    "        mu = kwargs['mu']\n",
    "        sigma = kwargs['sigma']\n",
    "        W = kwargs['W']\n",
    "        a = kwargs['a']\n",
    "        b = kwargs['b']\n",
    "        theta = kwargs['theta']\n",
    "    \n",
    "    print(\"Initializing probabilities...\")\n",
    "    U = im.get_squashed_X(X, a, b)\n",
    "    p_z = im.get_p_z(z_states, theta)\n",
    "    s_states = im.get_s_states(z_states, W, p_z)\n",
    "    p_x_z = im.get_all_p_x_z(mu, sigma, s_states, U, fixed_indices, z_states)\n",
    "    p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "    \n",
    "    print(\"Running EM:\")\n",
    "    for jj in range(k):\n",
    "        print(\"   Iteration\", jj, end=\"...\")\n",
    "        print(\"Updating W, theta...\", end=\"\")\n",
    "        W = im.update_W(mu, W, z_states, U, p_z_x, fixed_indices, alpha)\n",
    "        theta = im.update_thetas(np.sum(p_z_x, axis=0), z_states_bool, theta, alpha)\n",
    "\n",
    "        p_z = im.get_p_z(z_states, theta)\n",
    "        s_states = im.get_s_states(z_states, W, p_z)\n",
    "        p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "        \n",
    "        print(\"Updating mu, sigma, a, b...\", end=\"\")\n",
    "        mu = im.update_mus(mu, s_states, U, p_z_x, beta)\n",
    "        sigma = im.update_sigma(mu, sigma, s_states, U, p_z_x)\n",
    "        a, b = im.update_ab(mu, a, b, sigma, s_states, X, p_z_x, beta)\n",
    "\n",
    "        if jj < k-1:\n",
    "            print(\"Updating probabilities...\")\n",
    "            U = im.get_squashed_X(X, a, b)\n",
    "            s_states = im.get_s_states(z_states, W, p_z)\n",
    "            p_x_z = im.get_all_p_x_z(mu, sigma, s_states, U, fixed_indices, z_states)\n",
    "            p_z_x = im.get_all_p_z_x(p_x_z, p_z)\n",
    "            \n",
    "    return W, theta, mu, sigma, a, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.6068820855405788 0.4020698107354496\n",
      "-1.2357424596567832 0.8557082098766997\n",
      "-0.0009163150682455879 0.011434793765638829\n",
      "1.9101354005255533\n",
      "2.0048 1.0032\n"
     ]
    }
   ],
   "source": [
    "params = {'mu': mu,\n",
    "'sigma': sigma,\n",
    "'a': a,\n",
    "'b': b,\n",
    "'W': W,\n",
    "'theta': theta}\n",
    "\n",
    "print(np.amin(mu), np.amax(mu))\n",
    "print(np.amin(W), np.amax(W))\n",
    "print(np.amin(theta), np.amax(theta))\n",
    "print(sigma)\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'inference_methods' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\python\\\\inference_methods.py'>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(im)\n",
    "im.W_opt_func(np.ravel(W), a_i, b_i, c_i, z_states, W, fixed_indices, U)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "theta = np.zeros((num_features, num_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing probabilities...\n",
      "Running EM:\n",
      "   Iteration 0...Updating W, theta...Updating mu, sigma, a, b...Updating probabilities...\n",
      "   Iteration 1...Updating W, theta...Updating mu, sigma, a, b..."
     ]
    }
   ],
   "source": [
    "importlib.reload(im)\n",
    "#W = im.update_W(mu, W, z_states, U, p_z_x, fixed_indices, .5, view_weights=True)\n",
    "W, theta, mu, sigma, a, b = EM_loop(2, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "p_z = np.ones(p_z.shape)\n",
    "p_z_x = im.get_all_p_z_x(p_x_z, p_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2., -2., -2., ..., -2., -2., -2.],\n",
       "       [-2., -2., -2., ..., -2., -2., -2.],\n",
       "       [-2., -2., -2., ..., -2., -2., -2.],\n",
       "       ...,\n",
       "       [-2., -2., -2., ..., -2., -2., -2.],\n",
       "       [-2., -2., -2., ..., -2., -2., -2.],\n",
       "       [-2., -2., -2., ..., -2., -2., -2.]])"
      ]
     },
     "execution_count": 195,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "U = im.get_squashed_X(X, a, b)\n",
    "p_z = im.get_p_z(z_states, theta)\n",
    "s_states = im.get_s_states(z_states, W, p_z)\n",
    "p_x_z = im.get_all_p_x_z(mu, sigma, s_states, U, fixed_indices, z_states)\n",
    "p_z_x = im.get_all_p_z_x(p_x_z, p_z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for _ in range(num_states*10):\n",
    "    np.nan_to_num(0)\n",
    "    #pass\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "progressive centripetal filling 25%\n",
      "hyperintense mass on delayed phase 25%\n",
      "venous washout 25%\n",
      "infiltrative 25%\n",
      "lobulated margins 25%\n",
      "heterogeneous 25%\n",
      "regular spherical hypointense mass 25%\n",
      "progressive or concentric enhancement 25%\n",
      "continuous enhancing rim 25%\n",
      "thin well-defined walls 25%\n",
      "arterial enhancement 25%\n",
      "delayed isointensity 25%\n",
      "nodular or discontinuous enhancement 25%\n",
      "hypointense without enhancement 25%\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAkMAAADPCAYAAAD21NURAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\ngg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1e501e4c3c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_ix = 0\n",
    "#for test_ix in test_indices[5:]:\n",
    "p_zi_x = np.zeros([num_features])\n",
    "for f_ix in range(num_features):\n",
    "    state_ixs = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    p_zi_x[f_ix] = sum(p_z_x[test_ix, state_ixs])\n",
    "    #for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "    #    output[z] = output[z] + [f, strength]\n",
    "#break\n",
    "\n",
    "for f,strength in sorted(enumerate(p_zi_x), key=lambda x:x[1], reverse=True):\n",
    "    if strength<0.2:\n",
    "        break\n",
    "    print(\"%s %d%%\" % (all_features[f], strength*100))\n",
    "\n",
    "for cls in C.classes_to_include:\n",
    "    x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == indices_f[test_ix])]\n",
    "    if len(x_test_quick) > 0:\n",
    "        break\n",
    "hf.draw_slices(x_test_quick[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "@njit\n",
    "def update_stdevs(mu, m, sigma, s, z_states, filter_results, p_z_x):\n",
    "    sigma_est = np.empty([num_features, num_units])\n",
    "    s_est = np.empty([num_units])\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "\n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in range(num_states):\n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - \\\n",
    "                       np.dot(mu[:, u_ix], z_states_bool[state_ix]) - m[u_ix])**2 for img_ix in range(num_imgs)]) / 2\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "        temp = scipy.optimize.minimize(\\\n",
    "                    lambda Var: sum([a_i[state_ix] / (sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) + c_i[state_ix] * log(sqrt(sum(np.where(z_states_bool[state_ix],\n",
    "                       Var[:-1], 0))+Var[-1]) * sqrt(2*pi)) for state_ix in range(num_states)]),\n",
    "                      np.concatenate([sigma[:, u_ix]**2, [s[u_ix]**2]]), \n",
    "                      bounds=tuple(itertools.repeat((.001, 1000),num_features+1)))\n",
    "\n",
    "        sigma_est[:, u_ix] = [sqrt(i) for i in temp['x'][:-1]]\n",
    "        s_est[u_ix] = sqrt(temp['x'][-1])\n",
    "    \n",
    "    return sigma_est, s_est"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([a_i[state_ix]/(x+var_adj[state_ix])**2 - \\\n",
    "                                         c_i[state_ix]/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "#state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "#p_z_x does not need to be rescaled because the factor cancels out\n",
    "var_adj = np.zeros([num_states])\n",
    "a_i = np.zeros([num_states])\n",
    "c_i = np.zeros([num_states])\n",
    "\n",
    "for u_ix in range(num_units):\n",
    "    for state_ix in range(num_states):\n",
    "        mean = np.dot(mu[:, u_ix], z_states[state_ix]) + m[u_ix]\n",
    "        a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "        c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "\n",
    "    temp = scipy.optimize.linearmixing(\\\n",
    "                lambda Var: sum([a_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2)**2 - \\\n",
    "                 c_i[state_ix]/(sum(np.where(z_states[state_ix]==1, Var[:-1]**2, 0))+Var[-1]**2) for state_ix in range(num_states)]), \\\n",
    "                  np.concatenate([sigma[:, u_ix], [s[u_ix]]]), verbose=True, maxiter=10000)\n",
    "    \n",
    "    #sigma_est[:, u_ix]\n",
    "    if u_ix % 20 == 0:\n",
    "        print(u_ix, time.time()-t)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "t=time.time()\n",
    "for f_ix in range(num_features):\n",
    "    state_indices = [state_ix for state_ix in range(num_states) if z_states_bool[state_ix][f_ix]]\n",
    "    #p_z_x does not need to be rescaled because the factor cancels out\n",
    "    var_adj = np.zeros([num_states])\n",
    "    a_i = np.zeros([num_states])\n",
    "    c_i = np.zeros([num_states])\n",
    "    \n",
    "    for u_ix in range(num_units):\n",
    "        for state_ix in state_indices:\n",
    "            z = z_states[state_ix]\n",
    "\n",
    "            mean = np.dot(mu[:, u_ix], z) + m[u_ix]\n",
    "            var_adj[state_ix] = sum(np.where(z==1, sigma[:, u_ix]**2, 0)) + s[u_ix]**2 - sigma[f_ix, u_ix]**2\n",
    "            \n",
    "            a_i[state_ix] = sum([p_z_x[img_ix, state_ix] * (filter_results[img_ix, u_ix] - mean)**2 for img_ix in range(num_imgs)])\n",
    "            c_i[state_ix] = sum(p_z_x[:, state_ix])\n",
    "        \n",
    "        sigma_est[f_ix, u_ix] = sqrt(scipy.optimize.fsolve(lambda x: sum([(a_i[state_ix]/(x+var_adj[state_ix]) - \\\n",
    "                                         c_i[state_ix])/(x+var_adj[state_ix]) for state_ix in state_indices]), \\\n",
    "                                          sigma[f_ix, u_ix]))\n",
    "        \n",
    "        if u_ix % 20 == 0:\n",
    "            print(f_ix, u_ix, time.time()-t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Z_test_features = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Least squares approach\n",
    "Theta = np.array([feature_filter_means[f] for f in all_features])\n",
    "Theta = np.transpose(Theta, (1,0))\n",
    "\n",
    "filters_test = {}\n",
    "features_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)\n",
    "    filters_test[cls] = (filters_test[cls] - filter_avgs) / filter_stds\n",
    "    features_test[cls] = np.linalg.lstsq(Theta, np.transpose(filters_test[cls], (1,0)))[0]\n",
    "    #filters_test[cls] = np.apply_along_axis(lambda x: x / filter_avgs, 1, filters_test[cls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fa = FactorAnalysis(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 100)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_results.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def covar_to_corr(covar):\n",
    "    A = np.diag(np.diag(covar)**(-0.5))\n",
    "    return np.matmul(np.matmul(A, covar), A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "corr = covar_to_corr(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(corr - np.linalg.pinv(np.diag(np.diag(np.linalg.pinv(corr)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_cutoff_eigenval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=None,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy = np.random.normal(size=filter_results.shape)\n",
    "dummy_fa = FactorAnalysis()\n",
    "dummy_fa.fit(dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals(dummy_fa.get_covariance())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.linalg.eigvals(fa.get_covariance()) > np.mean(np.linalg.eigvals(dummy_fa.get_covariance())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FactorAnalysis(copy=True, iterated_power=3, max_iter=1000, n_components=10,\n",
       "        noise_variance_init=None, random_state=0, svd_method='randomized',\n",
       "        tol=0.01)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa.fit(filter_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "np.linalg.eigvals()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.89081373261106078"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=3\n",
    "fa.get_covariance()[x,x] - fa.noise_variance_[x]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = cnna.get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))\n",
    "\n",
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "output = {}\n",
    "for cls in C.classes_to_include:\n",
    "    for img_num in range(len(filters_test[cls])):\n",
    "        z = Z_test[cls][img_num]\n",
    "        x = np.expand_dims(x_test[cls][img_num], axis=0)\n",
    "        evidence = {}\n",
    "        \n",
    "        output[z] = [cls]\n",
    "        \n",
    "        preds = model.predict(x, verbose=False)[0]\n",
    "        for pred_cls, pred_conf in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:2]:\n",
    "            output[z] = output[z] + [pred_cls]\n",
    "        \n",
    "        #for f in all_features:\n",
    "        #    evidence[f + \"/\" + str(cls_features[f])] = get_evidence_strength(feature_filters[f], filters_test[cls][img_num])\n",
    "        \n",
    "        for i in range(len(all_features)):\n",
    "            evidence[all_features[i] + \"/\" + str(cls_features[all_features[i]])] = features_test[cls][i, img_num]\n",
    "        \n",
    "        f1='infiltrative'\n",
    "        f2='lobulated margins'\n",
    "        if evidence[f1 + \"/\" + str(cls_features[f1])] < evidence[f2 + \"/\" + str(cls_features[f2])]:\n",
    "            evidence.pop(f1 + \"/\" + str(cls_features[f1]))\n",
    "        else:\n",
    "            evidence.pop(f2 + \"/\" + str(cls_features[f2]))\n",
    "        \n",
    "        for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True):\n",
    "            output[z] = output[z] + [f, strength]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred5.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_cls2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_features)):\n",
    "        writer.writerow([Z_test_features[z_num]] + [output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][3], \\\n",
    "                        output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][5]] + output[Z_test_features[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:\\\\filters_pred3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "for _,row in df.iterrows():\n",
    "    if row[\"pred_cls1\"] in row[\"feature_1\"]:\n",
    "        agree += 1\n",
    "print(agree/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
