{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "https://arxiv.org/pdf/1703.01365.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "import pylab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.models import Model\n",
    "import keras.models\n",
    "import keras.layers as layers\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import confusion_matrix, f1_score, accuracy_score\n",
    "from keras import backend as K\n",
    "\n",
    "import sklearn.decomposition\n",
    "import argparse\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import csv\n",
    "import niftiutils.helper_fxns as hf\n",
    "import niftiutils.private as prv\n",
    "import importlib\n",
    "import inference_methods_squash as im\n",
    "import itertools\n",
    "from math import sqrt, log, pi, exp, e\n",
    "import matplotlib.pyplot as plt\n",
    "from numpy import matmul, diag\n",
    "import numpy as np\n",
    "import operator\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import scipy\n",
    "import time\n",
    "import dr_methods as drm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(crun)\n",
    "C = config.Config()\n",
    "T = config.Hyperparams()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def kl_div(m1, sig1, m2, sig2, one_sided=\"none\"):\n",
    "    #returns kl(p,q) where p~N(m1,s1), q~N(m2,s2)\n",
    "    ret = np.log(sig2/sig1) + (sig1**2+(m1-m2)**2)/(2*sig2**2) - .5\n",
    "    if one_sided==\"less\":\n",
    "        return ret * (m1 < m2)\n",
    "    elif one_sided==\"greater\":\n",
    "        return ret * (m1 > m2)\n",
    "    else:\n",
    "        return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(os.path.join(C.model_dir, \"models_305.hdf5\")) #models_305\n",
    "\n",
    "model_conv1 = cbuild.build_pretrain_model(model, last_layer=-6)\n",
    "model_conv2 = cbuild.build_pretrain_model(model, last_layer=-5)\n",
    "model_conv3 = cbuild.build_pretrain_model(model, last_layer=-4)\n",
    "model_pre_act = cbuild.build_pretrain_model(model, last_layer=-3)\n",
    "model_act = cbuild.build_pretrain_model(model, last_layer=-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_conv1 = cbuild.build_pretrain_model(model, last_layer=-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['12572068_0', '13092836_1', 'E103354630_0', 'E105921537_0', 'E106010098_0']"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "feat_count.pop(\"central scar\")\n",
    "all_features = list(feat_count.keys())\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}\n",
    "\n",
    "Z_features = cnna.get_annotated_files(features_by_cls)\n",
    "Z_features.pop(\"central scar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = np.empty((8,8,4))\n",
    "for x in range(D.shape[0]):\n",
    "    for y in range(D.shape[1]):\n",
    "        for z in range(D.shape[2]):\n",
    "            D[x,y,z] = (D.shape[0]-.5-x)**2 + (D.shape[1]-.5-y)**2 + 4*(D.shape[2]-.5-z)**2\n",
    "\n",
    "voi_df = drm.get_voi_dfs()[0]\n",
    "Z = np.concatenate([orig_data_dict[cls][1] for cls in C.classes_to_include], 0)\n",
    "\n",
    "all_dense = np.empty([0,100])\n",
    "all_conv3_sh = np.empty([0,128*4])\n",
    "all_conv3_ch = np.empty([0,128])\n",
    "all_conv2_ch = np.empty([0,128])\n",
    "all_conv1_ch = np.empty([0,64*3])\n",
    "\n",
    "aug_factor = 10\n",
    "for img_id in range(len(Z)):\n",
    "    voi_row = voi_df.loc[Z[img_id]]\n",
    "    for aug_id in range(aug_factor):\n",
    "        img = np.load(os.path.join(C.aug_dir, voi_row['cls'], \"%s_%d.npy\" % (Z[img_id], aug_id)))\n",
    "        \n",
    "        activ = model_pre_act.predict(np.expand_dims(img, 0))\n",
    "        all_dense = np.concatenate([all_dense, activ], axis=0)\n",
    "        \n",
    "        activ = model_conv3.predict(np.expand_dims(img, 0))\n",
    "        all_conv3_ch = np.concatenate([all_conv3_ch, activ.mean(axis=(1,2,3))], axis=0)\n",
    "        all_conv3_sh = np.concatenate([all_conv3_sh, get_shells(activ, D)], axis=0)\n",
    "        \n",
    "        activ = model_conv2.predict(np.expand_dims(img, 0))\n",
    "        all_conv2_ch = np.concatenate([all_conv2_ch, activ.mean(axis=(1,2,3))], axis=0)\n",
    "        \n",
    "        activ = model_conv1.predict(np.expand_dims(img, 0))\n",
    "        all_conv1_ch = np.concatenate([all_conv1_ch, activ.mean(axis=(1,2,3))], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_neurons = np.concatenate([all_conv3, all_dense], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_shells(activ, D):\n",
    "    shell4 = activ[0, D > 85, :].mean(axis=0)\n",
    "    shell3 = activ[0, (D <= 85) & (D > 62), :].mean(axis=0)\n",
    "    shell2 = activ[0, (D <= 62) & (D > 39), :].mean(axis=0)\n",
    "    shell1 = activ[0, D <= 39, :].mean(axis=0)\n",
    "    return np.expand_dims(np.concatenate([shell1, shell2, shell3, shell4]), 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_neurons = all_dense.shape[1] + all_conv3.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_dense = {f:np.empty([0,100]) for f in all_features}\n",
    "feature_conv3_sh = {f:np.empty([0,128]) for f in all_features}\n",
    "\n",
    "aug_factor = 80\n",
    "for f in all_features:\n",
    "    Z = Z_features[f]\n",
    "    for img_id in range(len(Z)):\n",
    "        voi_row = voi_df.loc[Z[img_id]]\n",
    "        for aug_id in range(aug_factor):\n",
    "            img = np.load(os.path.join(C.aug_dir, voi_row['cls'], \"%s_%d.npy\" % (Z[img_id], aug_id)))\n",
    "            activ = model_pre_act.predict(np.expand_dims(img, 0))\n",
    "            feature_dense[f] = np.concatenate([feature_dense[f], activ], axis=0)\n",
    "        \n",
    "            activ = model_conv3.predict(np.expand_dims(img, 0))\n",
    "            feature_conv3_sh[f] = np.concatenate([feature_conv3_sh[f], get_shells(activ, D)], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_neurons = {f:np.concatenate([feature_conv3[f], feature_dense[f]], axis=1) for f in all_features}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# best model\n",
    "Z_test = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reader\n",
    "Z_test = ['E103312835_1','12823036_0','12569915_0','E102093118_0','E102782525_0','12799652_0','E100894274_0','12874178_3','E100314676_0','12842070_0','13092836_2','12239783_0','12783467_0','13092966_0','E100962970_0','E100183257_1','E102634440_0','E106182827_0','12582632_0','E100121654_0','E100407633_0','E105310461_0','12788616_0','E101225606_0','12678910_1','E101083458_1','12324408_0','13031955_0','E101415263_0','E103192914_0','12888679_2','E106096969_0','E100192709_1','13112385_1','E100718398_0','12207268_0','E105244287_0','E102095465_0','E102613189_0','12961059_0','11907521_0','E105311123_0','12552705_0','E100610622_0','12975280_0','E105918926_0','E103020139_1','E101069048_1','E105427046_0','13028374_0','E100262351_0','12302576_0','12451831_0','E102929168_0','E100383453_0','E105344747_0','12569826_0','E100168661_0','12530153_0','E104697262_0']\n",
    "\n",
    "num_features = len(all_features) # number of features\n",
    "num_units = 100 # number of dense units\n",
    "\n",
    "num_annotations = 8\n",
    "\n",
    "all_imgs = [orig_data_dict[cls][0] for cls in C.classes_to_include]\n",
    "all_imgs = np.array(hf.flatten(all_imgs))\n",
    "\n",
    "all_lesionids = [orig_data_dict[cls][1] for cls in C.classes_to_include]\n",
    "all_lesionids = np.array(hf.flatten(all_lesionids))\n",
    "\n",
    "test_indices = np.where(np.isin(all_lesionids, Z_test))[0]\n",
    "\n",
    "fixed_indices = np.empty([num_features, num_annotations])\n",
    "for f_ix,f in enumerate(all_features):\n",
    "    fixed_indices[f_ix, :] = np.where(np.isin(all_lesionids, random.sample(set(Z_features[f]), num_annotations)))[0]\n",
    "fixed_indices = fixed_indices.astype(int)\n",
    "\n",
    "x_test = all_imgs[test_indices]\n",
    "z_test = all_lesionids[test_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = scipy.stats.norm.pdf(x, loc=m[0],scale=s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = feature_neurons[all_features[0]]\n",
    "y2 = scipy.stats.norm.pdf(x, loc=F.mean(axis=0)[0],scale=F.std(axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5886752939191502"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mean(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = feature_neurons[all_features[2]]\n",
    "y3 = scipy.stats.norm.pdf(x, loc=F.mean(axis=0)[0],scale=F.std(axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = feature_neurons[all_features[10]]\n",
    "y4 = scipy.stats.norm.pdf(x, loc=F.mean(axis=0)[0],scale=F.std(axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = scipy.stats.norm.pdf(x, loc=m_test[0],scale=s_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.plot(x,y,linewidth=2)\n",
    "plt.plot(x,y2,linewidth=2)\n",
    "plt.plot(x,y_test,linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8,7))\n",
    "plt.plot(x,y,linewidth=5, label='population')\n",
    "plt.plot(x,y2,linewidth=2, label=all_features[0])\n",
    "plt.plot(x,y3,linewidth=2, label=all_features[2])\n",
    "plt.plot(x,y4,linewidth=2, label=all_features[10])\n",
    "plt.axis([-3,3,0,1.2])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# information provided by a feature about the possible values of a neuron, higher KL means that neuron should be weighted more\n",
    "KL = np.zeros((num_features, num_neurons))\n",
    "m = all_neurons.mean(axis=0)\n",
    "s = all_neurons.std(axis=0)\n",
    "for f_ix in range(num_features):\n",
    "    F = feature_neurons[all_features[f_ix]]\n",
    "    KL[f_ix, :] = kl_div(m,s, F.mean(axis=0),F.std(axis=0), one_sided=\"less\")\n",
    "    #KL[f_ix, :] = ttest_ind(F, all_pre_act, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = all_neurons.mean(axis=0)\n",
    "all_cov = np.cov(all_neurons.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_dense = np.empty([0,100])\n",
    "test_conv3 = np.empty([0,128*4])\n",
    "\n",
    "p_f = np.empty(num_features)\n",
    "img_id = 37\n",
    "voi_row = voi_df.loc[z_test[img_id]]\n",
    "cls = voi_row['cls']\n",
    "aug_factor = 100\n",
    "for aug_id in range(aug_factor):\n",
    "    img = np.load(os.path.join(C.aug_dir, cls, \"%s_%d.npy\" % (z_test[img_id], aug_id)))\n",
    "    \n",
    "    activ = model_pre_act.predict(np.expand_dims(img, 0))\n",
    "    test_dense = np.concatenate([test_dense, activ], axis=0)\n",
    "    \n",
    "    activ = model_conv3.predict(np.expand_dims(img, 0))\n",
    "    test_conv3 = np.concatenate([test_conv3, get_shells(activ, D)], axis=0)\n",
    "\n",
    "test_neurons = np.concatenate([test_conv3, test_dense], axis=1)\n",
    "    \n",
    "m_test = test_neurons.mean(axis=0)\n",
    "\n",
    "c_ix = C.classes_to_include.index(cls)\n",
    "#w_u = np.empty((num_features, num_neurons))\n",
    "#p_fu = np.empty((num_features, num_neurons))\n",
    "test_cov = np.cov(test_neurons.T)\n",
    "\n",
    "p_f = np.empty(num_features)\n",
    "for f_ix in range(num_features):\n",
    "    f_neurons = feature_neurons[all_features[f_ix]]\n",
    "    f_m = f_neurons.mean(0)\n",
    "    f_cov = np.cov(f_neurons.T)\n",
    "\n",
    "    samp = scipy.random.multivariate_normal(m_test, test_cov, size=10000)\n",
    "    lnphf = scipy.stats.multivariate_normal.logpdf(samp, f_m, f_cov, allow_singular=True)\n",
    "    lnph = scipy.stats.multivariate_normal.logpdf(samp, m, all_cov, allow_singular=True)\n",
    "    \n",
    "    adj = np.max(lnphf - lnph)\n",
    "    p_f[f_ix] = np.log(np.mean(np.exp(lnphf - lnph - adj))) + adj\n",
    "\n",
    "#for f_ix in range(num_features):\n",
    "#    F = feature_neurons[all_features[f_ix]]\n",
    "#    w_u[f_ix] = KL[f_ix, :] #np.exp(KL[f_ix, :]) #+ W_eff[:, c_ix])\n",
    "#    p_fu[f_ix] = np.exp(-kl_div(m_test, s_test, F.mean(axis=0),F.std(axis=0), one_sided=\"less\"))\n",
    "#    p_f[f_ix] = np.sum(w_u * p_fu) / np.sum(w_u)\n",
    "\n",
    "#for u_ix in range(num_neurons):\n",
    "#    w_u[:, u_ix] = np.where(w_u[:, u_ix] * p_fu[:, u_ix] >= np.median(w_u[:, u_ix] * p_fu[:, u_ix]),\n",
    "#                            w_u[:, u_ix], np.zeros(num_features))\n",
    "\n",
    "#for f_ix in range(num_features):\n",
    "#    p_f[f_ix] = np.sum(w_u[f_ix] * p_fu[f_ix]) / np.sum(w_u[f_ix])\n",
    "\n",
    "print(cls)\n",
    "for f,strength in sorted(enumerate(p_f), key=lambda x:x[1], reverse=True):\n",
    "    #if strength<0.2:\n",
    "    #    break\n",
    "    print(\"%s %d\" % (all_features[f], strength))\n",
    "\n",
    "x_test_quick = x_test[img_id]#orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == indices_f[test_ix])]\n",
    "hf.draw_slices(x_test_quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "E100962970_0\n",
      "E103312835_1\n",
      "E101083458_1\n",
      "E100183257_1\n",
      "E105311123_0\n",
      "E104697262_0\n",
      "12961059_0\n",
      "12324408_0\n",
      "12975280_0\n",
      "12888679_2\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_excel(os.path.join(C.base_dir, \"data\", \"annotated_features.xlsx\"), 5)\n",
    "\n",
    "for accnum, row in df.iterrows():\n",
    "    df = df.drop(accnum)\n",
    "    accnum = prv.decode(accnum[:accnum.find('_')]) + accnum[accnum.find('_'):accnum.find(' ')]\n",
    "    df.loc[accnum] = row\n",
    "\n",
    "print('\\n'.join([x for x in df.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_test_features = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "\n",
    "filters_test = {}\n",
    "features_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f_ix in range(num_features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- feature_vectors * unit_relevance should be maximized\n",
    "\n",
    "- np.dot(feature_vectors[i], feature_vectors[j]) should be minimized, OR\n",
    "- vec_distance(feature_vectors[i], feature_vectors[j]) should be maximized for all pairs i,j\n",
    "\n",
    "- features \"turn on / off\" specific units; try to minimize the number of units impacted by a given feature\n",
    "\n",
    "- p(z|x) > .75 for all x manually annotated by z\n",
    "\n",
    "===\n",
    "- show % of evidence explained (fraction of sum of contributing units that are captured by features that turn those units on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = cnna.get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))\n",
    "\n",
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['true_cls', 'pred_cls'] + \\\n",
    "            [s for i in range(1,5) for s in ['feature_%d' % i,'strength_%d' % i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_conv3[f], feature_dense[f]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = all_conv3.mean(axis=0) #all_neurons.mean(axis=0)\n",
    "all_cov = np.cov(all_conv3.T) #np.cov(all_neurons.T)\n",
    "\n",
    "for img_ix in range(len(z_test)):\n",
    "    #test_dense = np.empty([0,100])\n",
    "    test_conv3 = np.empty([0,128])\n",
    "    z = z_test[img_ix]\n",
    "    voi_row = voi_df.loc[z]\n",
    "    cls = voi_row['cls']\n",
    "    row = [cls]\n",
    "\n",
    "    x = np.expand_dims(x_test[img_ix], axis=0)\n",
    "    preds = model.predict(x, verbose=False)[0]\n",
    "    for pred_cls, _ in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:1]:\n",
    "        row.append(pred_cls)\n",
    "\n",
    "    p_f = np.empty(num_features)\n",
    "    aug_factor = 100\n",
    "    for aug_id in range(aug_factor):\n",
    "        img = np.load(os.path.join(C.aug_dir, cls, \"%s_%d.npy\" % (z, aug_id)))\n",
    "\n",
    "        #activ = model_pre_act.predict(np.expand_dims(img, 0))\n",
    "        #test_dense = np.concatenate([test_dense, activ], axis=0)\n",
    "\n",
    "        activ = model_conv3.predict(np.expand_dims(img, 0))\n",
    "        test_conv3 = np.concatenate([test_conv3, get_shells(activ, D)], axis=0)\n",
    "\n",
    "    test_neurons = test_conv3#np.concatenate([test_conv3, test_dense], axis=1) \n",
    "    #m_test = test_neurons.mean(axis=0)\n",
    "    #test_cov = np.cov(test_neurons.T)\n",
    "\n",
    "    p_f = np.empty(num_features)\n",
    "    for f_ix in range(num_features):\n",
    "        f_neurons = feature_conv3[all_features[f_ix]] #feature_neurons[all_features[f_ix]]\n",
    "        f_m = f_neurons.mean(0)\n",
    "        f_cov = np.cov(f_neurons.T)\n",
    "\n",
    "        samp = test_neurons#scipy.random.multivariate_normal(m_test, test_cov, size=10000)\n",
    "        lnphf = scipy.stats.multivariate_normal.logpdf(samp, f_m, f_cov, allow_singular=True)\n",
    "        lnph = scipy.stats.multivariate_normal.logpdf(samp, m, all_cov, allow_singular=True)\n",
    "\n",
    "        adj = np.amax(lnphf - lnph)\n",
    "        p_f[f_ix] = np.log(np.mean(np.exp(lnphf - lnph - adj))) + adj\n",
    "        \n",
    "    f1='infiltrative'\n",
    "    f2='lobulated margins'\n",
    "    evidence = {all_features[f_ix]: p_f[f_ix] for f_ix in range(num_features)}\n",
    "    if evidence[f1] < evidence[f2]:\n",
    "        evidence.pop(f1)\n",
    "    else:\n",
    "        evidence.pop(f2)\n",
    "\n",
    "    for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:4]:\n",
    "        row += [f, strength]\n",
    "        \n",
    "    df.loc[z] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('D:\\\\feature_analysis\\\\features_conv3ch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred6.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_cls2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_features)):\n",
    "        writer.writerow([Z_test_features[z_num]] + [output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][3], \\\n",
    "                        output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][5]] + output[Z_test_features[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:\\\\filters_pred3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "for _,row in df.iterrows():\n",
    "    if row[\"pred_cls1\"] in row[\"feature_1\"]:\n",
    "        agree += 1\n",
    "print(agree/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = sklearn.decomposition.NMF(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[-3].get_weights()[0]\n",
    "bias = model.layers[-3].get_weights()[1]\n",
    "\n",
    "#np.dot(filter_results[0], W) * eff_mult + eff_bias\n",
    "\n",
    "gamma, beta, mu, var = model.layers[-2].get_weights()\n",
    "\n",
    "eff_bias = (np.zeros(6) + bias - mu) / var**.5 * gamma + beta\n",
    "eff_mult = (np.ones(6) + bias - mu) / var**.5 * gamma + beta - eff_bias\n",
    "\n",
    "W_eff = W * eff_mult# + eff_bias\n",
    "\n",
    "#(np.dot(filter_results[0], W) + bias - mu) / var**.5 * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_relevance = np.empty(num_units)\n",
    "discr_power = np.empty((num_units, 6))\n",
    "for u_ix in range(num_units):\n",
    "    unit_relevance[u_ix] = np.amax(W_eff[u_ix]) - np.amin(W_eff[u_ix])\n",
    "    for c_ix in range(6):\n",
    "        if W_eff[u_ix, c_ix] == np.amax(W_eff[u_ix]):\n",
    "            discr_power[u_ix, c_ix] = W_eff[u_ix, c_ix] - sorted(W_eff[u_ix],reverse=True)[1]\n",
    "        else:\n",
    "            discr_power[u_ix, c_ix] = W_eff[u_ix, c_ix] - np.amax(W_eff[u_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_results = filter_results*unit_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_length = np.mean(np.apply_along_axis(get_length, 1, filter_results*unit_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239091267665856"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(squash(filter_results[2] * unit_relevance * 2/avg_length)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
