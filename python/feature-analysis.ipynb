{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://arxiv.org/pdf/1703.01365.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T14:00:03.454701Z",
     "start_time": "2018-04-08T14:00:02.748258Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import csv\n",
    "import importlib\n",
    "import itertools\n",
    "import operator\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from math import e, exp, log, pi, sqrt\n",
    "from os.path import *\n",
    "\n",
    "import keras.layers as layers\n",
    "import keras.models\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pylab\n",
    "import scipy\n",
    "import sklearn.decomposition\n",
    "from keras import backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "from numpy import diag, matmul\n",
    "from scipy.stats import ttest_ind\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n",
    "\n",
    "import cnn_analyzer as cnna\n",
    "import cnn_builder as cbuild\n",
    "import cnn_runner as crun\n",
    "import config\n",
    "import dr_methods as drm\n",
    "import inference_methods_squash as im\n",
    "import niftiutils.helper_fxns as hf\n",
    "import niftiutils.private as prv\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T14:00:05.780312Z",
     "start_time": "2018-04-08T14:00:05.773296Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)\n",
    "importlib.reload(cbuild)\n",
    "importlib.reload(cnna)\n",
    "C = config.Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T03:16:25.108147Z",
     "start_time": "2018-04-05T03:16:25.106140Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target_dir = \"D:\\\\feature_analysis\"\n",
    "\n",
    "importlib.reload(cbuild)\n",
    "model = keras.models.load_model(join(C.model_dir, \"model_reader1.hdf5\")) #models_305\n",
    "\n",
    "#model_conv1 = cbuild.build_pretrain_model(model, padding = ['same','same'], last_layer=-5, add_activ=True)\n",
    "#model_conv2 = cbuild.build_pretrain_model(model, padding = ['same','same'], last_layer=-4, add_activ=True)\n",
    "#model_conv3 = cbuild.build_pretrain_model(model, padding = ['same','same'], last_layer=-3, add_activ=True)\n",
    "model_dense = cbuild.pretrain_cnn(model, padding = ['same','valid'], last_layer=-2, add_activ=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T14:00:33.573630Z",
     "start_time": "2018-04-08T14:00:28.126993Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['E105368141_0', '12362250_6', 'E100215900_4', 'E106182827_0',\n",
       "       '12362250_2', '12362250_4', 'E105918926_0', 'E100215900_6',\n",
       "       'E104490005_0', '12362250_0'], dtype='<U12')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "orig_data_dict, num_samples = cbuild._collect_unaug_data()\n",
    "\n",
    "features_by_cls, feat_count = cnna.collect_features()\n",
    "#feat_count.pop(\"central scar\")\n",
    "feat_count.pop(\"homogeneous texture\")\n",
    "all_features = list(feat_count.keys())\n",
    "cls_features = {f: [c for c in C.classes_to_include if f in features_by_cls[c]] for f in all_features}\n",
    "\n",
    "num_annotations = 10\n",
    "Z_features = cnna.get_annotated_files(features_by_cls, num_annotations)\n",
    "#Z_features.pop(\"central scar\")\n",
    "Z_features.pop(\"homogeneous texture\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T14:03:52.818385Z",
     "start_time": "2018-04-08T14:03:52.814374Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted(all_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-08T14:03:15.440182Z",
     "start_time": "2018-04-08T14:03:14.670713Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import voi_methods as vm\n",
    "for f in Z_features:\n",
    "    save_path = \"D:\\\\labeled_features\\\\\"+f\n",
    "    if not exists(save_path):\n",
    "        os.makedirs(save_path)\n",
    "    vm.save_vois_as_imgs(lesion_ids=Z_features[f], save_dir=save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "feature_mapping = {'arterial phase enhancement',\n",
    " 'central scar': 'central scar',\n",
    " 'enhancing rim/capsule',\n",
    " 'heterogeneous texture',\n",
    " 'hyperintense mass on delayed phase',\n",
    " 'hypointense mass without enhancement',\n",
    " 'infiltrative growth',\n",
    " 'isointense on venous phase',\n",
    " 'nodular growth',\n",
    " 'nodular or discontinuous enhancement',\n",
    " 'progressive centripetal filling',\n",
    " 'progressive uniform enhancement',\n",
    " 'sharp margins',\n",
    " 'spherical hypointense mass',\n",
    " 'washout'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T01:33:38.600660Z",
     "start_time": "2018-04-05T01:33:38.513401Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# best model\n",
    "#Z_test = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "# reader\n",
    "Z_test = ['E103312835_1','12823036_0','12569915_0','E102093118_0','E102782525_0','12799652_0','E100894274_0','12874178_3','E100314676_0','12842070_0','13092836_2','12239783_0','12783467_0','13092966_0','E100962970_0','E100183257_1','E102634440_0','E106182827_0','12582632_0','E100121654_0','E100407633_0','E105310461_0','12788616_0','E101225606_0','12678910_1','E101083458_1','12324408_0','13031955_0','E101415263_0','E103192914_0','12888679_2','E106096969_0','E100192709_1','13112385_1','E100718398_0','12207268_0','E105244287_0','E102095465_0','E102613189_0','12961059_0','11907521_0','E105311123_0','12552705_0','E100610622_0','12975280_0','E105918926_0','E103020139_1','E101069048_1','E105427046_0','13028374_0','E100262351_0','12302576_0','12451831_0','E102929168_0','E100383453_0','E105344747_0','12569826_0','E100168661_0','12530153_0','E104697262_0']\n",
    "\n",
    "num_features = len(all_features) # number of features\n",
    "\n",
    "all_imgs = [orig_data_dict[cls][0] for cls in C.classes_to_include]\n",
    "all_imgs = np.array(hf.flatten(all_imgs))\n",
    "\n",
    "all_lesionids = [orig_data_dict[cls][1] for cls in C.classes_to_include]\n",
    "all_lesionids = np.array(hf.flatten(all_lesionids))\n",
    "\n",
    "test_indices = np.where(np.isin(all_lesionids, Z_test))[0]\n",
    "\n",
    "fixed_indices = np.empty([num_features, num_annotations])\n",
    "for f_ix,f in enumerate(all_features):\n",
    "    if len(np.where(np.isin(all_lesionids, random.sample(set(Z_features[f]), num_annotations)))[0]) < 10:\n",
    "        print(f,Z_features[f])\n",
    "    fixed_indices[f_ix, :] = np.where(np.isin(all_lesionids, random.sample(set(Z_features[f]), num_annotations)))[0]\n",
    "fixed_indices = fixed_indices.astype(int)\n",
    "\n",
    "x_test = all_imgs[test_indices]\n",
    "z_test = all_lesionids[test_indices]\n",
    "len(z_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T01:35:31.096446Z",
     "start_time": "2018-04-05T01:33:42.134854Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "voi_df = drm.get_voi_dfs()[0]\n",
    "Z = np.concatenate([orig_data_dict[cls][1] for cls in C.classes_to_include], 0)\n",
    "aug_factor = 20\n",
    "num_samples = aug_factor*len(Z)\n",
    "\n",
    "all_dense = np.empty([num_samples,100])\n",
    "all_conv3_sh = np.empty([num_samples,128*4])\n",
    "all_conv2_sh = np.empty([num_samples,128*4])\n",
    "all_conv3_ch = np.empty([num_samples,128])\n",
    "all_conv2_ch = np.empty([num_samples,128])\n",
    "all_conv1_sh = np.empty([num_samples,64*12])\n",
    "all_conv1_ch = np.empty([num_samples,64*3])\n",
    "\n",
    "for img_id in range(len(Z)):\n",
    "    voi_row = voi_df.loc[Z[img_id]]\n",
    "    for aug_id in range(aug_factor):\n",
    "        img = np.load(os.path.join(C.aug_dir, voi_row['cls'], \"%s_%d.npy\" % (Z[img_id], aug_id)))\n",
    "        img = np.expand_dims(img, 0)\n",
    "        ix = img_id*aug_factor + aug_id\n",
    "        \n",
    "        activ = model_dense.predict(img)[0]\n",
    "        #activ[activ < 0] = 0\n",
    "        all_dense[ix] = activ\n",
    "        \n",
    "        #activ = model_conv3.predict(img)[0]\n",
    "        #activ[activ < 0] = 0\n",
    "        #all_conv3_ch[ix] = activ.mean((0,1,2))\n",
    "        #all_conv3_sh[ix] = cnna.get_shells(activ, D)\n",
    "        \n",
    "        #activ = model_conv2.predict(img)[0]\n",
    "        #activ[activ < 0] = 0\n",
    "        #all_conv2_ch[ix] = activ.mean((0,1,2))\n",
    "        #all_conv2_sh[ix] = cnna.get_shells(activ, D)\n",
    "        \n",
    "        #activ = model_conv1.predict(img)[0]\n",
    "        #activ[activ < 0] = 0\n",
    "        #all_conv1_ch[ix] = activ.mean((0,1,2))\n",
    "        #all_conv1_sh[ix] = cnna.get_shells(activ, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T01:36:58.254142Z",
     "start_time": "2018-04-05T01:35:31.686674Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_dense = {f:np.empty([0,100]) for f in all_features}\n",
    "feature_conv3_ch = {f:np.empty([0,128]) for f in all_features}\n",
    "feature_conv3_sh = {f:np.empty([0,128*4]) for f in all_features}\n",
    "feature_conv2_ch = {f:np.empty([0,128]) for f in all_features}\n",
    "feature_conv1_ch = {f:np.empty([0,64*3]) for f in all_features}\n",
    "\n",
    "aug_factor = 100\n",
    "for f in all_features:\n",
    "    Z = Z_features[f]\n",
    "    for img_id in range(len(Z)):\n",
    "        voi_row = voi_df.loc[Z[img_id]]\n",
    "        for aug_id in range(aug_factor):\n",
    "            img = np.load(os.path.join(C.aug_dir, voi_row['cls'], \"%s_%d.npy\" % (Z[img_id], aug_id)))\n",
    "            \n",
    "            activ = model_dense.predict(np.expand_dims(img, 0))\n",
    "            feature_dense[f] = np.concatenate([feature_dense[f], activ], axis=0)\n",
    "        \n",
    "            #activ = model_conv3.predict(np.expand_dims(img, 0))\n",
    "            #feature_conv3_ch[f] = np.concatenate([feature_conv3_ch[f], activ.mean(axis=(1,2,3))], axis=0)\n",
    "            #feature_conv3_sh[f] = np.concatenate([feature_conv3_sh[f], get_shells(activ, D)], axis=0)\n",
    "        \n",
    "            #activ = model_conv2.predict(np.expand_dims(img, 0))\n",
    "            #feature_conv2_ch[f] = np.concatenate([feature_conv2_ch[f], activ.mean(axis=(1,2,3))], axis=0)\n",
    "\n",
    "            #activ = model_conv1.predict(np.expand_dims(img, 0))\n",
    "            #feature_conv1_ch[f] = np.concatenate([feature_conv1_ch[f], activ.mean(axis=(1,2,3))], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T03:14:33.056403Z",
     "start_time": "2018-04-05T03:14:33.052397Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(columns=['true_cls', 'pred_cls'] + \\\n",
    "            [s for i in range(1,5) for s in ['feature_%d' % i,'strength_%d' % i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T03:14:57.836609Z",
     "start_time": "2018-04-05T03:14:57.723307Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_neurons = all_dense#np.concatenate([all_conv1_ch, all_conv2_ch, all_conv3_ch, all_dense], axis=1)\n",
    "m = all_neurons.mean(axis=0)\n",
    "all_cov = np.cov(all_neurons.T)\n",
    "\n",
    "num_neurons = all_neurons.shape[-1]\n",
    "\n",
    "lnZ = np.empty(num_features)\n",
    "f_m = np.empty((num_features, num_neurons))\n",
    "f_cov = np.empty((num_features, num_neurons, num_neurons))\n",
    "for f_ix in range(num_features):\n",
    "    #f_neurons = np.concatenate([feature_conv1_ch[all_features[f_ix]],\n",
    "    #                            feature_conv2_ch[all_features[f_ix]],\n",
    "    #                            feature_conv3_ch[all_features[f_ix]],\n",
    "    #                            feature_dense[all_features[f_ix]]], axis=1)\n",
    "    f_neurons = feature_dense[all_features[f_ix]]\n",
    "    f_m[f_ix] = f_neurons.mean(0)\n",
    "    f_cov[f_ix] = np.cov(f_neurons.T)\n",
    "\n",
    "    lnZ_f = -scipy.stats.multivariate_normal.logpdf(f_neurons, m, all_cov, allow_singular=True)\n",
    "    adj = np.amax(lnZ_f)\n",
    "    lnZ[f_ix] = np.log(np.mean(np.exp(lnZ_f - adj))) + adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T04:07:14.230576Z",
     "start_time": "2018-04-05T04:06:45.863800Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for img_ix in range(len(z_test)):\n",
    "    test_dense = np.empty([0,100])\n",
    "    test_conv3_ch = np.empty([0,128])\n",
    "    test_conv3_sh = np.empty([0,128*4])\n",
    "    test_conv2_ch = np.empty([0,128])\n",
    "    test_conv1_ch = np.empty([0,64*3])\n",
    "    z = z_test[img_ix]\n",
    "    voi_row = voi_df.loc[z]\n",
    "    cls = voi_row['cls']\n",
    "    row = [cls]\n",
    "\n",
    "    x = np.expand_dims(x_test[img_ix], axis=0)\n",
    "    preds = model.predict(x, verbose=False)[0]\n",
    "    for pred_cls, _ in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:1]:\n",
    "        row.append(pred_cls)\n",
    "\n",
    "    p_f = np.empty(num_features)\n",
    "    aug_factor = 100\n",
    "    for aug_id in range(aug_factor):\n",
    "        img = np.load(os.path.join(C.aug_dir, cls, \"%s_%d.npy\" % (z, aug_id)))\n",
    "\n",
    "        activ = model_dense.predict(np.expand_dims(img, 0))\n",
    "        test_dense = np.concatenate([test_dense, activ], axis=0)\n",
    "\n",
    "        #activ = model_conv3.predict(np.expand_dims(img, 0))\n",
    "        #test_conv3_ch = np.concatenate([test_conv3_ch, activ.mean(axis=(1,2,3))], axis=0)\n",
    "        #test_conv3_sh = np.concatenate([test_conv3_sh, get_shells(activ, D)], axis=0)\n",
    "        \n",
    "        #activ = model_conv2.predict(np.expand_dims(img, 0))\n",
    "        #test_conv2_ch = np.concatenate([test_conv2_ch, activ.mean(axis=(1,2,3))], axis=0)\n",
    "        \n",
    "        #activ = model_conv1.predict(np.expand_dims(img, 0))\n",
    "        #test_conv1_ch = np.concatenate([test_conv1_ch, activ.mean(axis=(1,2,3))], axis=0)\n",
    "\n",
    "    test_neurons = test_dense#np.concatenate([test_conv3_ch, test_dense], axis=1) \n",
    "    #m_test = test_neurons.mean(axis=0)\n",
    "    #test_cov = np.cov(test_neurons.T)\n",
    "\n",
    "    p_f = np.empty(num_features)\n",
    "    for f_ix in range(num_features):  \n",
    "        #indices = np.random.randint(0,test_neurons.shape[0], 1000)\n",
    "        samp = test_neurons#[indices] #scipy.random.multivariate_normal(m_test, test_cov, size=10000)\n",
    "        lnphf = scipy.stats.multivariate_normal.logpdf(samp, f_m[f_ix], f_cov[f_ix], allow_singular=True)\n",
    "        lnph = scipy.stats.multivariate_normal.logpdf(samp, m, all_cov, allow_singular=True)\n",
    "\n",
    "        adj = np.amax(lnphf - lnph)\n",
    "        p_f[f_ix] = np.log(np.mean(np.exp(lnphf - lnph - adj))) + adj# + lnZ[f_ix]\n",
    "        \n",
    "    evidence = {all_features[f_ix]: p_f[f_ix] for f_ix in range(num_features)}\n",
    "    \n",
    "    f1='infiltrative growth'\n",
    "    f2='nodular growth'\n",
    "    if evidence[f1] < evidence[f2]:\n",
    "        evidence[f1] -= 10\n",
    "    else:\n",
    "        evidence[f2] -= 10\n",
    "        \n",
    "    f3='central scar'\n",
    "    evidence[f3] -= 50\n",
    "    \n",
    "    f4='isointense on venous phase'\n",
    "    f5='washout'\n",
    "    if evidence[f4] < evidence[f5]:\n",
    "        evidence.pop(f4)\n",
    "    else:\n",
    "        evidence.pop(f5)\n",
    "    #top3 = np.array(sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:3])[:,0]\n",
    "    #if f4 not in top3:\n",
    "    #    evidence.pop(f4)\n",
    "    #if f5 not in top3:\n",
    "    #    evidence.pop(f5)\n",
    "\n",
    "    for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:4]:\n",
    "        row += [f, strength]\n",
    "        \n",
    "    if np.mean([row[-7], row[-5], row[-3]]) / 3 > row[-1]:\n",
    "        row[-2] = ''\n",
    "        \n",
    "    if np.mean([row[-7], row[-5]]) / 3 > row[-3]:\n",
    "        row[-4] = ''\n",
    "        \n",
    "    df.loc[z] = row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T04:07:26.573968Z",
     "start_time": "2018-04-05T04:07:26.547868Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_excel(join(target_dir,'features_dense_final2.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## univariate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "all_neurons = all_conv3_ch #np.concatenate([all_conv1_ch, all_conv2_ch, all_conv3_ch, all_dense], axis=1)\n",
    "m = all_neurons.mean(0)\n",
    "all_std = all_neurons.std(0)\n",
    "\n",
    "num_neurons = all_neurons.shape[-1]\n",
    "\n",
    "Z_uni = np.empty(num_features)\n",
    "f_m = np.empty((num_features, num_neurons))\n",
    "f_std = np.empty((num_features, num_neurons))\n",
    "for f_ix in range(num_features):\n",
    "    f_neurons = feature_conv3_ch[all_features[f_ix]]\n",
    "    f_m[f_ix] = f_neurons.mean(0)\n",
    "    f_std[f_ix] = f_neurons.std(0)\n",
    "\n",
    "    Z_f = scipy.stats.norm.pdf(f_neurons, m, all_std)\n",
    "    Z_uni[f_ix] = np.mean(1/Z_f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "for img_ix in range(len(z_test)):\n",
    "    test_dense = np.empty([0,100])\n",
    "    test_conv3_ch = np.empty([0,128])\n",
    "    test_conv3_sh = np.empty([0,128*4])\n",
    "    test_conv2_ch = np.empty([0,128])\n",
    "    test_conv1_ch = np.empty([0,64*3])\n",
    "    z = z_test[img_ix]\n",
    "    voi_row = voi_df.loc[z]\n",
    "    cls = voi_row['cls']\n",
    "    row = [cls]\n",
    "\n",
    "    x = np.expand_dims(x_test[img_ix], axis=0)\n",
    "    preds = model.predict(x, verbose=False)[0]\n",
    "    for pred_cls, _ in sorted(zip(C.classes_to_include, preds), key=lambda x:x[1], reverse=True)[:1]:\n",
    "        row.append(pred_cls)\n",
    "\n",
    "    p_f = np.empty(num_features)\n",
    "    aug_factor = 100\n",
    "    for aug_id in range(aug_factor):\n",
    "        img = np.load(os.path.join(C.aug_dir, cls, \"%s_%d.npy\" % (z, aug_id)))\n",
    "\n",
    "        #activ = model_dense.predict(np.expand_dims(img, 0))\n",
    "        #test_dense = np.concatenate([test_dense, activ], axis=0)\n",
    "\n",
    "        activ = model_conv3.predict(np.expand_dims(img, 0))\n",
    "        test_conv3_ch = np.concatenate([test_conv3_ch, activ.mean(axis=(1,2,3))], axis=0)\n",
    "        #test_conv3_sh = np.concatenate([test_conv3_sh, get_shells(activ, D)], axis=0)\n",
    "        \n",
    "        #activ = model_conv2.predict(np.expand_dims(img, 0))\n",
    "        #test_conv2_ch = np.concatenate([test_conv2_ch, activ.mean(axis=(1,2,3))], axis=0)\n",
    "        \n",
    "        #activ = model_conv1.predict(np.expand_dims(img, 0))\n",
    "        #test_conv1_ch = np.concatenate([test_conv1_ch, activ.mean(axis=(1,2,3))], axis=0)\n",
    "\n",
    "    test_neurons = test_conv3_ch #np.concatenate([test_conv1_ch, test_conv2_ch, test_conv3_ch, test_dense], axis=1) \n",
    "    #m_test = test_neurons.mean(axis=0)\n",
    "    #test_cov = np.cov(test_neurons.T)\n",
    "\n",
    "    p_f = np.empty(num_features)\n",
    "    for f_ix in range(num_features):        \n",
    "        samp = test_neurons#scipy.random.multivariate_normal(m_test, test_cov, size=10000)\n",
    "        phf = scipy.stats.norm.pdf(samp, f_m[f_ix], f_std[f_ix])\n",
    "        ph = scipy.stats.norm.pdf(samp, m, all_std)\n",
    "\n",
    "        p_f[f_ix] = np.mean(phf/ph)# * Z_uni[f_ix]\n",
    "        \n",
    "    evidence = {all_features[f_ix]: p_f[f_ix] for f_ix in range(num_features)}\n",
    "    \n",
    "    f1='infiltrative'\n",
    "    f2='lobulated margins'\n",
    "    if evidence[f1] < evidence[f2]:\n",
    "        evidence.pop(f1)\n",
    "    else:\n",
    "        evidence.pop(f2)\n",
    "        \n",
    "    f1='homogeneous'\n",
    "    f2='heterogeneous'\n",
    "    if evidence[f1] < evidence[f2]:\n",
    "        evidence.pop(f1)\n",
    "    else:\n",
    "        evidence.pop(f2)\n",
    "        \n",
    "    f3='central scar'\n",
    "    f4='delayed isointensity'\n",
    "    f5='arterial enhancement'\n",
    "    top2 = np.array(sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:2])[:,0]\n",
    "    if f4 not in top2 and f5 not in top2:\n",
    "        evidence.pop(f3)\n",
    "\n",
    "    for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:4]:\n",
    "        row += [f, strength]\n",
    "        \n",
    "    df.loc[z] = row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "df.to_csv(join(target_dir,'features_dense_uni1.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "m = all_neurons.mean(axis=0)\n",
    "all_cov = np.cov(all_neurons.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "test_dense = np.empty([0,100])\n",
    "test_conv3 = np.empty([0,128*4])\n",
    "\n",
    "p_f = np.empty(num_features)\n",
    "img_id = 37\n",
    "voi_row = voi_df.loc[z_test[img_id]]\n",
    "cls = voi_row['cls']\n",
    "aug_factor = 100\n",
    "for aug_id in range(aug_factor):\n",
    "    img = np.load(os.path.join(C.aug_dir, cls, \"%s_%d.npy\" % (z_test[img_id], aug_id)))\n",
    "    \n",
    "    activ = model_pre_act.predict(np.expand_dims(img, 0))\n",
    "    test_dense = np.concatenate([test_dense, activ], axis=0)\n",
    "    \n",
    "    activ = model_conv3.predict(np.expand_dims(img, 0))\n",
    "    test_conv3 = np.concatenate([test_conv3, get_shells(activ, D)], axis=0)\n",
    "\n",
    "test_neurons = np.concatenate([test_conv3, test_dense], axis=1)\n",
    "    \n",
    "m_test = test_neurons.mean(axis=0)\n",
    "\n",
    "c_ix = C.classes_to_include.index(cls)\n",
    "test_cov = np.cov(test_neurons.T)\n",
    "\n",
    "p_f = np.empty(num_features)\n",
    "for f_ix in range(num_features):\n",
    "    f_neurons = feature_neurons[all_features[f_ix]]\n",
    "    f_m = f_neurons.mean(0)\n",
    "    f_cov = np.cov(f_neurons.T)\n",
    "\n",
    "    samp = scipy.random.multivariate_normal(m_test, test_cov, size=10000)\n",
    "    lnphf = scipy.stats.multivariate_normal.logpdf(samp, f_m, f_cov, allow_singular=True)\n",
    "    lnph = scipy.stats.multivariate_normal.logpdf(samp, m, all_cov, allow_singular=True)\n",
    "    \n",
    "    adj = np.max(lnphf - lnph)\n",
    "    p_f[f_ix] = np.log(np.mean(np.exp(lnphf - lnph - adj))) + adj\n",
    "\n",
    "print(cls)\n",
    "for f,strength in sorted(enumerate(p_f), key=lambda x:x[1], reverse=True):\n",
    "    #if strength<0.2:\n",
    "    #    break\n",
    "    print(\"%s %d\" % (all_features[f], strength))\n",
    "\n",
    "x_test_quick = x_test[img_id]#orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == indices_f[test_ix])]\n",
    "hf.draw_slices(x_test_quick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from os.path import *\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xls = \"F:\\ReArranged-ITS.xlsx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df =pd.read_excel(xls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df[\"MRN-short\"] = df[\"MRN-short\"].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f in glob.glob(join(\"F:\\\\FileCopies\",\"*\")):\n",
    "    #if len(basename(f)) > 8:\n",
    "    g = basename(f)\n",
    "    os.rename(f, f+\"-\"+str(df[df[\"MRN-short\"] == int(g)][\"ID\"].values[0]))\n",
    "    #print(df[df[\"MRN-short\"] == int(g)][\"ID\"].values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T04:31:42.682911Z",
     "start_time": "2018-04-05T04:31:42.668874Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_dir = r\"C:\\Users\\Clinton\\Documents\\voi-classifier\\data\"\n",
    "answer_key = join(data_dir, \"ground_truth.xlsx\")\n",
    "answer_key = pd.read_excel(answer_key, index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T04:33:30.380460Z",
     "start_time": "2018-04-05T04:33:28.555683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "arterial phase enhancement,17,18,19\n",
      "central scar,1,5,1\n",
      "enhancing rim/capsule,5,8,15\n",
      "heterogeneous texture,13,19,17\n",
      "hyperintense mass on delayed phase,8,9,8\n",
      "hypointense mass without enhancement,9,9,10\n",
      "infiltrative growth,2,6,4\n",
      "isointense on venous phase,8,15,9\n",
      "nodular growth,5,14,6\n",
      "nodular or discontinuous enhancement,8,11,10\n",
      "progressive centripetal filling,9,14,9\n",
      "progressive uniform enhancement,5,7,19\n",
      "sharp margins,11,19,11\n",
      "spherical hypointense mass,13,15,20\n",
      "washout,8,15,9\n"
     ]
    }
   ],
   "source": [
    "target_dir = \"D:\\\\feature_analysis\"\n",
    "xls_path = join(target_dir, \"features_dense_final2.xlsx\")\n",
    "df = pd.read_excel(xls_path, index_col=0)\n",
    "#csv_path = join(target_dir, \"features_dense_final1.csv\")\n",
    "#df = pd.read_csv(csv_path, index_col=0)\n",
    "\n",
    "for f in sorted(all_features):\n",
    "    num, prec_den, rec_den = 0,0,0\n",
    "    for key, row in answer_key.iterrows():\n",
    "        answer_features = row[['feature_1', 'feature_2', 'feature_3', 'feature_4']].dropna().values\n",
    "        ignore_features = row[['ignore_1', 'ignore_2']].dropna().values\n",
    "        pred_features = df.loc[key][['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n",
    "        pred_str = df.loc[key][['strength_1', 'strength_2', 'strength_3', 'strength_4']].values\n",
    "        if f in pred_features and f in answer_features:\n",
    "            num += 1\n",
    "        if f in pred_features and f not in ignore_features:\n",
    "            prec_den += 1\n",
    "        if f in answer_features:\n",
    "            rec_den += 1\n",
    "    print(\"%s,%d,%d,%d\" % (f, num, prec_den, rec_den))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 48 211 193\n"
     ]
    }
   ],
   "source": [
    "num, prec_den, rec_den = 0,0,0\n",
    "n=0\n",
    "for key, row in answer_key.iterrows():\n",
    "    answer_features = row[['feature_1', 'feature_2', 'feature_3', 'feature_4']].dropna().values\n",
    "    for f in answer_features:\n",
    "        if f not in all_features:\n",
    "            print(f)\n",
    "    pred_features = df.loc[key][['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n",
    "    pred_str = df.loc[key][['strength_1', 'strength_2', 'strength_3', 'strength_4']].values\n",
    "    if df.loc[key]['true_cls'] == df.loc[key]['pred_cls']:\n",
    "        continue\n",
    "    if np.mean(pred_str) - pred_str[-1] > 20:\n",
    "        pred_features = pred_features[:-1]\n",
    "    num += len([f for f in answer_features if f in pred_features])\n",
    "    prec_den += len([f for f in pred_features if f not in ignore_features])\n",
    "    rec_den += len(answer_features)\n",
    "    n+=1\n",
    "print(n, num, prec_den, rec_den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num, prec_den, rec_den = 0,0,0\n",
    "n=0\n",
    "for key, row in answer_key.iterrows():\n",
    "    answer_features = row[['feature_1', 'feature_2', 'feature_3', 'feature_4']].dropna().values\n",
    "    for f in answer_features:\n",
    "        if f not in all_features:\n",
    "            print(f)\n",
    "    pred_features = df.loc[key][['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n",
    "    pred_str = df.loc[key][['strength_1', 'strength_2', 'strength_3', 'strength_4']].values\n",
    "    if np.mean(pred_str) - pred_str[-1] > 20:\n",
    "        pred_features = pred_features[:-1]\n",
    "    if pred_features[0] in answer_features:\n",
    "        num += 1\n",
    "    prec_den += len([f for f in pred_features if f not in ignore_features])\n",
    "    rec_den += len(answer_features)\n",
    "    n+=1\n",
    "print(n, num, prec_den, rec_den)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-05T04:36:23.842049Z",
     "start_time": "2018-04-05T04:36:23.697637Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cholangio 18 33 30\n",
      "colorectal 11 35 27\n",
      "cyst 24 30 27\n",
      "fnh 17 36 20\n",
      "hcc 23 35 32\n",
      "hemangioma 28 26 31\n"
     ]
    }
   ],
   "source": [
    "corr = 0\n",
    "for cls in sorted(C.classes_to_include):\n",
    "    num, prec_den, rec_den = 0,0,0\n",
    "    for key, row in answer_key.iterrows():\n",
    "        if row['true_cls'] != cls:\n",
    "            continue\n",
    "        answer_features = row[['feature_1', 'feature_2', 'feature_3', 'feature_4']].dropna().values\n",
    "        for f in answer_features:\n",
    "            if f not in all_features:\n",
    "                print(f)\n",
    "        pred_features = df.loc[key][['feature_1', 'feature_2', 'feature_3', 'feature_4']].values\n",
    "        pred_str = df.loc[key][['strength_1', 'strength_2', 'strength_3', 'strength_4']].values\n",
    "        if np.mean(pred_str) - pred_str[-1] > 20:\n",
    "            pred_features = pred_features[:-1]\n",
    "        num += len([f for f in answer_features if f in pred_features])\n",
    "        prec_den += len([f for f in pred_features if f not in ignore_features])\n",
    "        rec_den += len(answer_features)\n",
    "        #corr += len([f for f in answer_features if f in pred_features]) / len(pred_features)\n",
    "    #corr/60\n",
    "    print(cls, num, prec_den, rec_den)#, round(num/prec_den*100), round(num/rec_den*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.689119170984456"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "133/193"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "answer_features = hf.flatten(answer_key[['feature_1', 'feature_2', 'feature_3', 'feature_4']].values)\n",
    "unique, counts = np.unique(answer_features, return_counts=True)\n",
    "base_freq = dict(zip(unique, counts))\n",
    "base_freq.pop('nan')\n",
    "\n",
    "features = hf.flatten(df[['feature_1', 'feature_2', 'feature_3', 'feature_4']].values)\n",
    "unique, counts = np.unique(features, return_counts=True)\n",
    "freq = dict(zip(unique, counts))\n",
    "for f in base_freq.keys():\n",
    "    if f in freq:\n",
    "        print(f, freq[f])\n",
    "    else:\n",
    "        print(f, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = np.linspace(-3,3, num=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = scipy.stats.norm.pdf(x, loc=m[0],scale=s[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = feature_neurons[all_features[0]]\n",
    "y2 = scipy.stats.norm.pdf(x, loc=F.mean(axis=0)[0],scale=F.std(axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.5886752939191502"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.mean(axis=0)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = feature_neurons[all_features[2]]\n",
    "y3 = scipy.stats.norm.pdf(x, loc=F.mean(axis=0)[0],scale=F.std(axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "F = feature_neurons[all_features[10]]\n",
    "y4 = scipy.stats.norm.pdf(x, loc=F.mean(axis=0)[0],scale=F.std(axis=0)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_test = scipy.stats.norm.pdf(x, loc=m_test[0],scale=s_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(3,2))\n",
    "plt.plot(x,y,linewidth=2)\n",
    "plt.plot(x,y2,linewidth=2)\n",
    "plt.plot(x,y_test,linewidth=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(8,7))\n",
    "plt.plot(x,y,linewidth=5, label='population')\n",
    "plt.plot(x,y2,linewidth=2, label=all_features[0])\n",
    "plt.plot(x,y3,linewidth=2, label=all_features[2])\n",
    "plt.plot(x,y4,linewidth=2, label=all_features[10])\n",
    "plt.axis([-3,3,0,1.2])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#information provided by a feature about the possible values of a neuron, higher KL means that neuron should be weighted more\n",
    "KL = np.zeros((num_features, num_neurons))\n",
    "m = all_neurons.mean(axis=0)\n",
    "s = all_neurons.std(axis=0)\n",
    "for f_ix in range(num_features):\n",
    "    F = feature_neurons[all_features[f_ix]]\n",
    "    KL[f_ix, :] = kl_div(m,s, F.mean(axis=0),F.std(axis=0), one_sided=\"less\")\n",
    "    #KL[f_ix, :] = ttest_ind(F, all_pre_act, equal_var=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(os.path.join(C.base_dir, \"data\", \"annotated_features.xlsx\"), 5)\n",
    "\n",
    "for accnum, row in df.iterrows():\n",
    "    df = df.drop(accnum)\n",
    "    accnum = prv.decode(accnum[:accnum.find('_')]) + accnum[accnum.find('_'):accnum.find(' ')]\n",
    "    df.loc[accnum] = row\n",
    "\n",
    "print('\\n'.join([x for x in df.index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Z_test_features = ['E106097391_0', 'E104978772_1', '12900535_0', 'E100150242_0', 'E105490014_0', 'E103147618_0', 'E103510187_0', 'E104657225_0', 'E100551966_0', 'E101388602_0', 'E100215900_8', 'E100215900_7', 'E104045692_0', '13104521_0', 'E100383453_0', '12943286_0', '12271995_0', 'E102315724_0', 'E104949189_0', 'E100511083_1', 'E101579471_0', '13018986_1', '13203550_8', '13112385_0', '12712463_0', '12361082_0', '13028374_0', 'E103985934_1', 'E100529980_0', '12042703_3', '12961059_0', 'E105724706_2', 'E100592424_2', 'E103104254_0', 'E104546069_0', 'E101665217_1', '12090000_0', 'E100592424_1', '12961059_1', 'E105474285_0', '12502068_1', 'E100814791_0', 'E102613189_0', 'E105427046_0', 'E102881031_1', 'E102929168_0', 'E102310482_0', 'E102095465_0', 'E101811299_0', 'E104737273_0', '12890053_0', 'E100168661_1', '12637865_0', 'E100168661_2', '12239783_0', '12707781_0', '12706568_1', '12823036_0', '12404081_0', '12365693_1']\n",
    "\n",
    "x_test = {cls: orig_data_dict[cls][0][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "Z_test = {cls: orig_data_dict[cls][1][np.where(np.isin(orig_data_dict[cls][1], Z_test_features))] for cls in C.classes_to_include}\n",
    "\n",
    "filters_test = {}\n",
    "features_test = {}\n",
    "for cls in C.classes_to_include:\n",
    "    filters_test[cls] = model_dense_outputs.predict(x_test[cls], verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for f_ix in range(num_features):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements\n",
    "- feature_vectors * unit_relevance should be maximized\n",
    "\n",
    "- np.dot(feature_vectors[i], feature_vectors[j]) should be minimized, OR\n",
    "- vec_distance(feature_vectors[i], feature_vectors[j]) should be maximized for all pairs i,j\n",
    "\n",
    "- features \"turn on / off\" specific units; try to minimize the number of units impacted by a given feature\n",
    "\n",
    "- p(z|x) > .75 for all x manually annotated by z\n",
    "\n",
    "===\n",
    "- show % of evidence explained (fraction of sum of contributing units that are captured by features that turn those units on)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "header = ['filter_num']\n",
    "for cls in C.classes_to_include:\n",
    "    header += [f+\"_\"+cls for f in features_by_cls[cls]]\n",
    "\n",
    "with open('E:\\\\feature_filters.csv', 'w', newline='') as csvfile:\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for f_num in range(100):\n",
    "        writer.writerow([f_num] + [feature_filters[f][f_num] for cls in features for f in features_by_cls[cls]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "cls = \"colorectal\"\n",
    "x_test_quick = orig_data_dict[cls][0][np.where(orig_data_dict[cls][1] == \"E105724706_2.npy\")]\n",
    "x_test_quick = orig_data_dict[\"fnh\"][0][np.where(orig_data_dict[\"fnh\"][1] == \"E104189184_0.npy\")]\n",
    "filters_quick = model_dense_outputs.predict(x_test_quick, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_num = 0\n",
    "evidence = {}\n",
    "\n",
    "for f in all_features:\n",
    "    evidence[f + \"/\" + str(cls_features[f])] = cnna.get_evidence_strength(feature_filters[f], filters_quick[0])#filters_test[true_cls][img_num])\n",
    "    #max_strength = max(max_strength, evidence[f + \"/\" + str(cls_features[f])])\n",
    "\n",
    "#for f in evidence:\n",
    "#    evidence[f] /= max_strength\n",
    "print(\"Detected features:\")\n",
    "for f,strength in sorted(evidence.items(), key=lambda x:x[1], reverse=True)[:5]:\n",
    "    #if strength > 1:\n",
    "    print(\"- \" + f, \"- %d%%\" % (strength*100))\n",
    "\n",
    "hf.plot_section_auto(x_test_quick[0])#[true_cls][img_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('E:\\\\filters_pred6.csv', 'w', newline='') as csvfile:\n",
    "    header = ['img_fn', 'agreement1', 'agreement2', 'true_cls', 'pred_cls1', 'pred_cls2'] + \\\n",
    "            [s for i in range(len(all_features)) for s in ['feature_%d' % i,'strength_%d' % i]]\n",
    "    writer = csv.writer(csvfile)\n",
    "    writer.writerow(header)\n",
    "    for z_num in range(len(Z_test_features)):\n",
    "        writer.writerow([Z_test_features[z_num]] + [output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][3], \\\n",
    "                        output[Z_test_features[z_num]][0] in output[Z_test_features[z_num]][5]] + output[Z_test_features[z_num]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_excel(\"E:\\\\filters_pred3.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "agree = 0\n",
    "for _,row in df.iterrows():\n",
    "    if row[\"pred_cls1\"] in row[\"feature_1\"]:\n",
    "        agree += 1\n",
    "print(agree/60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf = sklearn.decomposition.NMF(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nmf.fit_transform()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance calculations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "W = model.layers[-3].get_weights()[0]\n",
    "bias = model.layers[-3].get_weights()[1]\n",
    "\n",
    "#np.dot(filter_results[0], W) * eff_mult + eff_bias\n",
    "\n",
    "gamma, beta, mu, var = model.layers[-2].get_weights()\n",
    "\n",
    "eff_bias = (np.zeros(6) + bias - mu) / var**.5 * gamma + beta\n",
    "eff_mult = (np.ones(6) + bias - mu) / var**.5 * gamma + beta - eff_bias\n",
    "\n",
    "W_eff = W * eff_mult# + eff_bias\n",
    "\n",
    "#(np.dot(filter_results[0], W) + bias - mu) / var**.5 * gamma + beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "unit_relevance = np.empty(num_units)\n",
    "discr_power = np.empty((num_units, 6))\n",
    "for u_ix in range(num_units):\n",
    "    unit_relevance[u_ix] = np.amax(W_eff[u_ix]) - np.amin(W_eff[u_ix])\n",
    "    for c_ix in range(6):\n",
    "        if W_eff[u_ix, c_ix] == np.amax(W_eff[u_ix]):\n",
    "            discr_power[u_ix, c_ix] = W_eff[u_ix, c_ix] - sorted(W_eff[u_ix],reverse=True)[1]\n",
    "        else:\n",
    "            discr_power[u_ix, c_ix] = W_eff[u_ix, c_ix] - np.amax(W_eff[u_ix])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filter_results = filter_results*unit_relevance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "avg_length = np.mean(np.apply_along_axis(get_length, 1, filter_results*unit_relevance))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6239091267665856"
      ]
     },
     "execution_count": 219,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(squash(filter_results[2] * unit_relevance * 2/avg_length)**2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
