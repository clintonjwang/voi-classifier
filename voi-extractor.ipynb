{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieves images from \"full_imgs\" and stores each VOI separately in \"train_imgs\". Retrieves spreadsheet listing VOIs and stores them in text file. Requires data-retrieval to be run first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import config\n",
    "import helper_fxns as hf\n",
    "import transforms as tr\n",
    "import importlib\n",
    "import numpy as np\n",
    "import os\n",
    "import requests\n",
    "import sys\n",
    "import copy\n",
    "import math\n",
    "import pandas as pd\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "with open(\"train_list.txt\", \"r\") as f:\n",
    "    vois = [x.split(',') for x in f.read().split(\"\\n\")]\n",
    "\n",
    "voi_df = pd.DataFrame(vois, columns = [\"Filename\", \"x1\", \"x2\", \"y1\", \"y2\", \"z1\", \"z2\", \"cls\"]).dropna()\n",
    "voi_df = voi_df.astype({\"x1\": int, \"x2\": int, \"y1\": int, \"y2\": int, \"z1\": int, \"z2\": int})\n",
    "\n",
    "voi_df['dx'] = voi_df.apply(lambda row: row['x2'] - row['x1'], axis=1)\n",
    "voi_df['dy'] = voi_df.apply(lambda row: row['y2'] - row['y1'], axis=1)\n",
    "voi_df['dz'] = voi_df.apply(lambda row: row['z2'] - row['z1'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'helper_fxns' from 'C:\\\\Users\\\\Clinton\\\\Documents\\\\voi-classifier\\\\helper_fxns.py'>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "importlib.reload(config)\n",
    "importlib.reload(hf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "C = config.Config()\n",
    "final_size = C.dims\n",
    "\n",
    "voi_df_art = pd.read_csv(C.art_voi_path)\n",
    "voi_df_ven = pd.read_csv(C.ven_voi_path)\n",
    "voi_df_eq = pd.read_csv(C.eq_voi_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#voi_df = voi_df[~voi_df['cls'].isin(['colorectal', 'adenoma', 'fnh'])]\n",
    "#set(voi_df['cls'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract_voi(img, voi, min_dims, ven_voi=[], eq_voi=[]):\n",
    "    \"\"\"Input: image, a voi to center on, and the min dims of the unaugmented img.\n",
    "    Outputs voi-centered image and classes.\n",
    "    \"\"\"\n",
    "    \n",
    "    voi_imgs = []\n",
    "    classes = []\n",
    "    temp_img = copy.deepcopy(img)\n",
    "    \n",
    "    x1 = voi['x1']\n",
    "    x2 = voi['x2']\n",
    "    y1 = voi['y1']\n",
    "    y2 = voi['y2']\n",
    "    z1 = voi['z1']\n",
    "    z2 = voi['z2']\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    dz = z2 - z1\n",
    "    \n",
    "    # align all phases\n",
    "    if len(ven_voi) > 0:\n",
    "        ven_voi = ven_voi.iloc[0]\n",
    "        temp_img = hf.align(temp_img, voi, ven_voi, 1)\n",
    "        \n",
    "    if len(eq_voi) > 0:\n",
    "        eq_voi = eq_voi.iloc[0]\n",
    "        temp_img = hf.align(temp_img, voi, eq_voi, 2)\n",
    "\n",
    "    #pad if voi is too close to edge\n",
    "    xpad = max(min_dims[0] - dx, 0) * math.sqrt(2)\n",
    "    ypad = max(min_dims[1] - dy, 0) * math.sqrt(2)\n",
    "    zpad = max(min_dims[2] - dz, 0) * math.sqrt(2)\n",
    "    side_padding = math.ceil(max(xpad, ypad, zpad) / 2)\n",
    "    pad_img = []\n",
    "    for ch in range(temp_img.shape[-1]):\n",
    "        pad_img.append(np.pad(temp_img[:,:,:,ch], side_padding, 'constant'))\n",
    "    pad_img = np.stack(pad_img, axis=3)\n",
    "    \n",
    "    #choice of ceil/floor needed to make total padding amount correct\n",
    "    x1 += side_padding - math.ceil(xpad/2)\n",
    "    x2 += side_padding + math.floor(xpad/2)\n",
    "    y1 += side_padding - math.ceil(ypad/2)\n",
    "    y2 += side_padding + math.floor(ypad/2)\n",
    "    z1 += side_padding - math.ceil(zpad/2)\n",
    "    z2 += side_padding + math.floor(zpad/2)\n",
    "    \n",
    "    new_voi = [voi['x1'] - x1, voi['x2'] - x1,\n",
    "               voi['y1'] - y1, voi['y2'] - y1,\n",
    "               voi['z1'] - z1, voi['z2'] - z1]\n",
    "\n",
    "    pad_img = pad_img[x1:x2, y1:y2, z1:z2, :]\n",
    "        \n",
    "    return pad_img, voi['cls'], new_voi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 7, 8)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4, 3)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[crop[0]//2:-crop[0]//2, crop[1]//2:-math.ceil(crop[1]/2), crop[2]//2:-crop[2]//2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, -2)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crop[0]//2,-crop[0]//2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "crop = [3,3,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def resize_img(img, final_dims, voi):\n",
    "    \"\"\"For rescaling an img to final_dims while scaling to make sure the image contains the voi.\"\"\"\n",
    "    x1 = voi[0]\n",
    "    x2 = voi[1]\n",
    "    y1 = voi[2]\n",
    "    y2 = voi[3]\n",
    "    z1 = voi[4]\n",
    "    z2 = voi[5]\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    dz = z2 - z1\n",
    "    \n",
    "    scale_ratio = max(final_dims[0]/dx, final_dims[1]/dy, final_dims[2]/dz)*0.9\n",
    "    \n",
    "    if scale_ratio < 0.9: #need to shrink original image to fit\n",
    "        img = tr.scale3d(img, [scale_ratio]*3)\n",
    "    elif scale_ratio > 1.4: #need to enlarge original image\n",
    "        img = tr.scale3d(img, [scale_ratio]*3)\n",
    "    \n",
    "    crop = [img.shape[i] - final_dims[i] for i in range(3)]\n",
    "    \n",
    "    return img[crop[0]//2:-crop[0]//2, crop[1]//2:-crop[1]//2, crop[2]//2:-crop[2]//2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def augment_img(img, final_dims, voi, num_samples, translate=None):\n",
    "    \"\"\"For rescaling an img to final_dims while scaling to make sure the image contains the voi.\"\"\"\n",
    "    x1 = voi[0]\n",
    "    x2 = voi[1]\n",
    "    y1 = voi[2]\n",
    "    y2 = voi[3]\n",
    "    z1 = voi[4]\n",
    "    z2 = voi[5]\n",
    "    dx = x2 - x1\n",
    "    dy = y2 - y1\n",
    "    dz = z2 - z1\n",
    "    \n",
    "    scale_ratio = max(final_dims[0]/dx, final_dims[1]/dy, final_dims[2]/dz)*0.9\n",
    "    scales = [scale_ratio*0.8, scale_ratio]\n",
    "\n",
    "    aug_imgs = []\n",
    "    \n",
    "    for _ in range(num_samples):\n",
    "        angle = random.randint(0, 359)\n",
    "        temp_img = tr.rotate(img, angle)\n",
    "        \n",
    "        if translate is not None:\n",
    "            trans = [random.randint(-translate[0], translate[0]),\n",
    "                     random.randint(-translate[1], translate[1]),\n",
    "                     random.randint(-translate[2], translate[2])]\n",
    "        else:\n",
    "            trans = [0,0,0]\n",
    "        \n",
    "        flip = [random.choice([-1, 1]), random.choice([-1, 1]), random.choice([-1, 1])]\n",
    "\n",
    "        scale = [random.uniform(scales[0],scales[1]), random.uniform(scales[0],scales[1]), random.uniform(scales[0],scales[1])]\n",
    "\n",
    "        crops = [img.shape[i] - final_dims[i] for i in range(3)]\n",
    "\n",
    "        #temp_img = add_noise(temp_img)\n",
    "\n",
    "        temp_img = temp_img[crops[0]//2 *flip[0] + trans[0] : -crops[0]//2 *flip[0] + trans[0] : flip[0],\n",
    "                            crops[1]//2 *flip[1] + trans[1] : -crops[1]//2 *flip[1] + trans[1] : flip[1],\n",
    "                            crops[2]//2 *flip[2] + trans[2] : -crops[2]//2 *flip[2] + trans[2] : flip[2], :]\n",
    "        \n",
    "        aug_imgs.append(temp_img)\n",
    "    \n",
    "    return aug_imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classes_to_include = ['cyst', 'hcc', 'hemangioma', 'cholangio']#, 'fnh', 'colorectal']\n",
    "\n",
    "voi_df_ven[\"id\"] = str(voi_df_ven[\"id\"])\n",
    "voi_df_eq[\"id\"] = str(voi_df_eq[\"id\"])\n",
    "\n",
    "\"\"\"if os.path.exists(C.aug_dir):\n",
    "    print(\"Warning: path\", C.aug_dir, \"already exists.\")\n",
    "else:\n",
    "    os.makedirs(C.aug_dir)\"\"\"\n",
    "if not os.path.exists(C.orig_dir):\n",
    "    os.makedirs(C.orig_dir)\n",
    "if not os.path.exists(C.aug_dir):\n",
    "    os.makedirs(C.aug_dir)\n",
    "if not os.path.exists(C.crops_dir):\n",
    "        os.makedirs(C.crops_dir)\n",
    "    \n",
    "for cls in classes_to_include:\n",
    "    if not os.path.exists(C.orig_dir + cls):\n",
    "        os.makedirs(C.orig_dir + cls)\n",
    "    if not os.path.exists(C.aug_dir + cls):\n",
    "        os.makedirs(C.aug_dir + cls)\n",
    "    if not os.path.exists(C.crops_dir + cls):\n",
    "        os.makedirs(C.crops_dir + cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...........................................................................................................................................\n",
      "102.36110711097717\n"
     ]
    }
   ],
   "source": [
    "t = time.time()\n",
    "\n",
    "small_vois = {}\n",
    "\n",
    "# iterate over image series\n",
    "for img_fn in os.listdir(C.full_img_dir):\n",
    "    img = np.load(C.full_img_dir+\"\\\\\"+img_fn)\n",
    "    art_vois = voi_df_art[(voi_df_art[\"Filename\"] == img_fn) & (voi_df_art[\"cls\"].isin(classes_to_include))]\n",
    "    \n",
    "    # iterate over each voi in that image\n",
    "    for voi_num, voi in enumerate(art_vois.iterrows()):\n",
    "        ven_voi = voi_df_ven[voi_df_ven[\"id\"] == voi[1][\"id\"]]\n",
    "        eq_voi = voi_df_eq[voi_df_eq[\"id\"] == voi[1][\"id\"]]\n",
    "        \n",
    "        cropped_img, cls, small_voi = extract_voi(img, copy.deepcopy(voi[1]), final_size, ven_voi=ven_voi, eq_voi=eq_voi)\n",
    "        fn = img_fn[:-4] + \"_\" + str(voi_num)\n",
    "        np.save(C.crops_dir + cls + \"\\\\\" + fn, cropped_img)\n",
    "        small_vois[fn] = small_voi\n",
    "        \n",
    "        \"\"\"        try:\n",
    "            cropped_imgs, classes = extract_voi(img, copy.deepcopy(voi[1]), final_size, aug=True, ven_voi=ven_voi, eq_voi=eq_voi)\n",
    "        except:\n",
    "            print(\"Exception\", img_fn)\n",
    "            break\n",
    "\n",
    "        for i in range(len(cropped_imgs)):\n",
    "            np.save(C.aug_dir + classes[i] + \"\\\\\" + img_fn[:-4] + \"_\" + str(voi_num) + \"_\" + str(i), cropped_imgs[i])\n",
    "\n",
    "        cropped_imgs, classes = extract_voi(img, copy.deepcopy(voi[1]), final_size, aug=False, ven_voi=ven_voi, eq_voi=eq_voi)\n",
    "        np.save(C.orig_dir+classes[0]+\"\\\\\"+img_fn[:-4]+\"_\" + str(voi_num), cropped_imgs[0])\"\"\"\n",
    "\n",
    "        if voi_num % 20 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "print(\"\")\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "for cls in os.listdir(C.crops_dir):\n",
    "    for fn in os.listdir(C.crops_dir + cls):\n",
    "        img = np.load(C.crops_dir + cls + \"\\\\\" + fn)\n",
    "        unaug_img = resize_img(img, C.dims, small_vois[fn[:-4]])\n",
    "        np.save(C.orig_dir + cls + \"\\\\\" + fn, unaug_img)\n",
    "        \n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "t = time.time()\n",
    "\n",
    "for cls in os.listdir(C.crops_dir):\n",
    "    for file_num, fn in enumerate(os.listdir(C.crops_dir + cls)):\n",
    "        img = np.load(C.crops_dir + cls + \"\\\\\" + fn)\n",
    "        aug_img = augment_img(img, C.dims, small_vois[fn[:-4]], num_samples=50, translate=[1,1,0])\n",
    "        for x, sample in enumerate(aug_img):\n",
    "            np.save(C.aug_dir + cls + \"\\\\\" + fn[:-4] + \"_\" + str(x), sample)\n",
    "            \n",
    "        if file_num % 30 == 0:\n",
    "            print(\".\", end=\"\")\n",
    "            \n",
    "print(\"\")\n",
    "print(time.time()-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img = np.load(C.aug_dir + \"cholangio\\\\E100814791_0_0.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "img_fn = \"E100529980.npy\"\n",
    "img = np.load(C.full_img_dir+\"\\\\\"+img_fn)\n",
    "hf.plot_section(img, voi_df[voi_df[\"Filename\"] == img_fn].iloc[0], pad=0)\n",
    "#art_vois = voi_df_art[(voi_df_art[\"Filename\"] == img_fn) & (voi_df_art[\"cls\"].isin(classes_to_include))]\n",
    "\n",
    "#img = np.load(\"orig_imgs\\\\cyst\\\\E100529980_1.npy\")\n",
    "#plt.imshow(img[:,:,5,0], cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hf.augment(pad_img, C.dims, translate=[1,1,0], exceed_ratio=0.9, num_samples=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
